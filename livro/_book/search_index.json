[
["index.html", "Como faz no R Cap√≠tulo 1 Introdu√ß√£o ", " Como faz no R Curso-R 28 de fevereiro de 2019 Cap√≠tulo 1 Introdu√ß√£o "],
["1-1-objetivos.html", "1.1 Por qu√™ ler esse livro", " 1.1 Por qu√™ ler esse livro "],
["1-2-organizacao.html", "1.2 Organiza√ß√£o", " 1.2 Organiza√ß√£o "],
["2-analises.html", "Cap√≠tulo 2 An√°lises", " Cap√≠tulo 2 An√°lises "],
["3-tutoriais.html", "Cap√≠tulo 3 Tutoriais ", " Cap√≠tulo 3 Tutoriais "],
["3-1-por-que-usar-o.html", "3.1 Por que usar o %&gt;%", " 3.1 Por que usar o %&gt;% Provavelmente voc√™ j√° ouviu falar do operador pipe (%&gt;%). Muita gente acha que ele √© uma sequ√™ncia m√°gica de s√≠mbolos que muda completamente o visual do seu c√≥digo, mas na verdade ele n√£o passa de uma fun√ß√£o como outra qualquer. Vamos explorar um pouco da hist√≥ria do pipe, como ele funciona e por que utiliz√°-lo. 3.1.1 Origem O conceito de pipe existe pelo menos desde os anos 1970. De acordo com seu criador, o operador foi concebido em ‚Äúuma noite febril‚Äù e tinha o objetivo de simplificar comandos cujos resultados deveriam ser passados para outros comandos. ls | cat #&gt; Desktop #&gt; Documents #&gt; Downloads #&gt; Music #&gt; Pictures #&gt; Public #&gt; Templates #&gt; Videos Por essa descri√ß√£o j√° conseguimos ter uma ideia de onde vem o seu nome: pipe em ingl√™s significa ‚Äúcano‚Äù, referindo-se ao transporte das sa√≠das dos comandos. Em portugu√™s o termo √© traduzido como ‚Äúcanaliza√ß√£o‚Äù ou ‚Äúencadeamento‚Äù, mas no dia-a-dia √© mais comum usar o termo em ingl√™s. A partir da√≠ o pipe tem aparecido nas mais diversas aplica√ß√µes, desde HTML at√© o nosso t√£o querido R. Ele pode ter m√∫ltiplos disfarces, mas o seu objetivo √© sempre o mesmo: transportar resultados. 3.1.2 Como funciona Em R o pipe tem uma cara meio estranha (%&gt;%), mas no fundo ele n√£o passa de uma fun√ß√£o infixa, ou seja, uma fun√ß√£o que aparece entre os seus argumentos (como a + b ou a %in% b). Na verdade √© por isso mesmo que ele tem porcentagens antes e depois: porque no R uma fun√ß√£o infixa s√≥ pode ser declarada assim. Vamos come√ßar demonstrando sua funcionalidade b√°sica. Carregue o pacote magrittr e declare o pipe usando Ctrl + Shift + M. library(magrittr) `%&gt;%`(&quot;oi&quot;, print) #&gt; [1] &quot;oi&quot; N√£o ligue para os acentos graves em volta do pipe, o comando acima s√≥ serve para demonstrar que ele n√£o √© nada mais que uma fun√ß√£o; perceba que o seu primeiro argumento (&quot;oi&quot;) virou a entrada do seu segundo argumento (print). &quot;oi&quot; %&gt;% print() #&gt; [1] &quot;oi&quot; Observe agora o comando abaixo. Queremos primeiro somar 3 a uma sequ√™ncia de n√∫meros e depois divid√≠-los por 2: mais_tres &lt;- function(x) { x + 3 } sobre_dois &lt;- function(x) { x / 2 } x &lt;- 1:3 sobre_dois(mais_tres(x)) #&gt; [1] 2.0 2.5 3.0 Perceba como fica dif√≠cil de entender o que est√° acontecendo primeiro? A linha relevante come√ßa com a divis√£o por 2, depois vem a soma com 3 e, por fim, os valores de entrada. Nesse tipo de situa√ß√£o √© mais leg√≠vel usar a nota√ß√£o de composi√ß√£o de fun√ß√µes, com as fun√ß√µes sendo exibidas na ordem em que ser√£o aplicadas: \\(f \\circ g\\). Isso pode ser realizado se tivermos uma fun√ß√£o que passa o resultado do que est√° √† sua esquerda para a fun√ß√£o que est√° √† sua direita‚Ä¶ x %&gt;% mais_tres() %&gt;% sobre_dois() #&gt; [1] 2.0 2.5 3.0 No comando acima fica evidente que pegamos o objeto x, somamos 3 e dividimos por 2. Voc√™ pode j√° ter notado isso, mas a entrada (esquerda) de um pipe sempre √© passada como o primeiro argumento agumento da sua sa√≠da (direita). Isso n√£o impede que as fun√ß√µes utilizadas em uma sequ√™ncia de pipes tenham outros argumentos. mais_n &lt;- function(x, n) { x + n } x %&gt;% mais_n(4) %&gt;% sobre_dois() #&gt; [1] 2.5 3.0 3.5 3.1.3 Vantagens A grande vantagem do pipe n√£o √© s√≥ enxergar quais fun√ß√µes s√£o aplicadas primeiro, mas sim nos ajudar a programar pipelines (‚Äúencanamento‚Äù em ingl√™s) de tratamentos de dados. library(dplyr) starwars %&gt;% mutate(bmi = mass/((height/100)^2)) %&gt;% select(name, bmi, species) %&gt;% group_by(species) %&gt;% summarise(bmi = mean(bmi)) #&gt; # A tibble: 38 x 2 #&gt; species bmi #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aleena 24.0 #&gt; 2 Besalisk 26.0 #&gt; 3 Cerean 20.9 #&gt; 4 Chagrian NA #&gt; 5 Clawdite 19.5 #&gt; 6 Droid NA #&gt; 7 Dug 31.9 #&gt; 8 Ewok 25.8 #&gt; 9 Geonosian 23.9 #&gt; 10 Gungan NA #&gt; # ... with 28 more rows Acima fica extremamente claro o que est√° acontecendo em cada passo da pipeline. Partindo da base starwars, primeiro transformamos, depois selecionamos, agrupamos e resumimos; em cada linha temos uma opera√ß√£o e elas s√£o executadas em sequ√™ncia. Isso n√£o melhora s√≥ a legibilidade do c√≥digo, mas tamb√©m a sua debugabilidade! Se tivermos encontrado um bug na pipeline, basta executar linha a linha do encadeamento at√© que encontremos a linha problem√°tica. Com o pipe podemos programar de forma mais compacta, leg√≠vel e correta. Todos os exemplos acima envolvem passar a entrada do pipe como o primeiro argumento da fun√ß√£o √† direita, mas n√£o √© uma obrigatoriedade. Com um operador placeholder . podemos indicar exatamente onde deve ser colocado o valor que chega no pipe: y_menos_x &lt;- function(x, y) { y - x } x %&gt;% mais_tres() %&gt;% purrr::map2(4:6, ., y_menos_x) # [[1]] # [1] 0 # # [[2]] # [1] 0 # # [[3]] # [1] 0 3.1.4 B√¥nus Agora que voc√™ j√° sabe dos usos mais comuns do pipe, aqui est√° uma outra funcionalidade interessante: fun√ß√µes un√°rias. Se voc√™ estiver familiarizado com o pacote purrr, esse √© um jeito bastante simples de criar fun√ß√µes descart√°veis. m3_s2 &lt;- . %&gt;% mais_tres() %&gt;% sobre_dois() m3_s2(x) #&gt; [1] 2.0 2.5 3.0 Usando novamente o . definimos uma fun√ß√£o que recebe apenas um argumento com uma sequ√™ncia de aplica√ß√µes de outras fun√ß√µes. 3.1.5 Conclus√£o O pipe n√£o √© apenas algo que deve ser usado pelos f√£s do tidyverse. Ele √© uma fun√ß√£o extremamente √∫til que ajuda na legibilidade e programa√ß√£o de c√≥digo, independentemente de quais pacotes utilizamos. Se quiser saber mais sobre o mundo do pipe, leia este post do Daniel sobre o Manifesto Tidy e o nosso tutorial mais aprofundado sobre o pr√≥prio pipe. "],
["3-2-o-que-e-um-grafico-estatistico.html", "3.2 O que √© um gr√°fico estat√≠stico?", " 3.2 O que √© um gr√°fico estat√≠stico? Os gr√°ficos s√£o t√©cnicas de visualiza√ß√£o de dados amplamente utilizadas em todas as √°reas da pesquisa. A sua popularidade se deve √† maneira como elucidam informa√ß√µes que estavam escondidas nas colunas do banco de dados, sendo que muitos deles podem ser compreendidos at√© mesmo por leigos no assunto que est√° sendo discutido. Mas ser√° que podemos definir formalmente o que √© um gr√°fico estat√≠stico? Gra√ßas ao estat√≠stico norte-americano Leland Wilkinson, a resposta √© sim. Em 2005, Leland publicou o livro The Grammar of Graphics, uma fonte de princ√≠pios fundamentais para a constru√ß√£o de gr√°ficos estat√≠sticos. No livro, ele defende que um gr√°fico √© o mapeamento dos dados a partir de atributos est√©ticos (posi√ß√£o, cor, forma, tamanho) e de objetos geom√©tricos (pontos, linhas, barras, caixas). Simples assim. Al√©m de responder a pergunta levantada nesse post, os conceitos de Leland tiveram outra grande import√¢ncia para a visualiza√ß√£o de dados. Alguns anos mais tarde, o seu trabalho inspirou Hadley Wickham a criar o pacote ggplot2, que enterrou com muitas p√°s de terra as fun√ß√µes gr√°ficas do R base. Em A Layered Grammar of Graphics, Hadley sugeriu que os principais aspectos de um gr√°fico (dados, sistema de coordenadas, r√≥tulos e anota√ß√µes) podiam ser divididos em camadas, constru√≠das uma a uma na elabora√ß√£o do gr√°fico. Essa √© a ess√™ncia do ggplot2. No gr√°fico abaixo, temos informa√ß√£o de 32 carros com respeito a 4 vari√°veis: milhas por gal√£o, tonelagem, transmiss√£o e n√∫mero de cilindros. O objeto geom√©trico escolhido para representar os dados foi o ponto. As posi√ß√µes dos pontos no eixo xy mapeia a associa√ß√£o entre a tonelagem e a quantidade de milhas por gal√£o. A cor dos pontos mapeia o n√∫mero de cilindros de cada carro, enquanto a forma dos pontos mapeia o tipo de transmiss√£o. Observando o c√≥digo, fica claro como cada linha/camada representa um aspecto diferente do gr√°fico. Os conceitos criados por Leland e Hadley defendem que essa estrutura pode ser utilizada para construir e entender qualquer tipo de gr√°fico, dando a eles, dessa maneira, a sua defini√ß√£o formal. ggplot(mtcars) + geom_point(aes(x = disp, y = mpg, shape = as.factor(am), color = cyl)) + labs(x = &quot;Tonelagem&quot;, y = &quot;Milhas por gal√£o&quot;, shape = &quot;Transmiss√£o&quot;, color = &quot;Cilindros&quot;) + scale_shape_discrete(labels = c(&quot;Autom√°tica&quot;,&quot;Manual&quot;)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) Por fim, √© preciso frisar que, apesar de a gram√°tica prover uma forte funda√ß√£o para a constru√ß√£o de gr√°ficos, ela n√£o indica qual gr√°fico deve ser usado ou como ele deve parecer. Essas escolhas, fundamentadas na pergunta a ser respondida, nem sempre s√£o triviais, e negligenci√°-las pode gerar gr√°ficos mal constru√≠dos e conclus√µes equivocadas. Cabe a n√≥s, pesquisadores, desenvolver, aprimorar e divulgar as t√©cnicas de visualiza√ß√£o adequadas para cada tipo de vari√°vel, assim como apontar ou denunciar os usos incorretos e mal-intencionados. Mas, em um mundo cuja veracidade das not√≠cias √© cada vez menos importante, √© papel de todos ter senso cr√≠tico para entender e julgar as informa√ß√µes trazidas por um gr√°fico. "],
["3-3-colando-textos-no-r.html", "3.3 Colando textos no R", " 3.3 Colando textos no R Uma tarefa muito comum no R √© a de colar textos. As fun√ß√µes mais importantes para isso s√£o paste() e sprintf(), que v√™m com o pacote base. Nesse texto, vamos falar dessas duas fun√ß√µes e de um novo pacote do tidyverse, o glue. 3.3.1 paste() A fun√ß√£o paste() recebe um conjunto indeterminado de objetos como argumento atrav√©s do ... e vai colando os objetos passados elemento a elemento. Isso significa que se voc√™ passar dois vetores de tamanho n, a fun√ß√£o paste() retornar√° um vetor de tamanho n sendo cada posi√ß√£o a colagem dos dois vetores nessa posi√ß√£o. Por padr√£o, a colagem √© feita com um separador de espa√ßo simples (o &quot; &quot;). Exemplo: paste(c(1, 2, 3), c(4, 5, 6)) FALSE [1] &quot;1 4&quot; &quot;2 5&quot; &quot;3 6&quot; √â poss√≠vel alterar o separador pelo argumento sep =. Um atalho √∫til para o separador vazio (&quot;&quot;) √© a fun√ß√£o paste0: paste0(c(1, 2, 3), c(4, 5, 6)) FALSE [1] &quot;14&quot; &quot;25&quot; &quot;36&quot; Algumas vezes nosso interesse n√£o √© juntar vetores elemento a elemento, mas sim passar um vetor e colar todos seus elementos. Isso √© feito com o par√¢metro collapse =: paste(c(1, 2, 3, 4, 5, 6), collapse = &#39;@&#39;) FALSE [1] &quot;1@2@3@4@5@6&quot; Se voc√™ passar mais de um vetor e mandar colapsar os elementos, o paste() vai primeiro colar e depois colapsar: paste(c(1, 2, 3), c(4, 5, 6), collapse = &#39;@&#39;) FALSE [1] &quot;1 4@2 5@3 6&quot; 3.3.1.1 Cuidado Tenha muito cuidado ao passar vetores com comprimentos diferentes no paste()! Assim como muitas fun√ß√µes do R, o paste() faz reciclagem, ou seja, ele copia os elementos do menor vetor at√© ele ficar com o comprimento do maior vetor1. O problema √© que o paste() faz isso silenciosamente e n√£o avisa se voc√™ inserir um vetor com comprimento que n√£o √© m√∫ltiplo dos demais. Veja que resultado bizarro: paste(5:9, 1:3, 4:5) FALSE [1] &quot;5 1 4&quot; &quot;6 2 5&quot; &quot;7 3 4&quot; &quot;8 1 5&quot; &quot;9 2 4&quot; Por essas e outras que dizemos que √†s vezes o R funciona bem demais‚Ä¶ 3.3.2 sprintf() O sprintf() √© similar ao printf do C. Primeiro escrevemos um texto com %s no lugar das coisas que queremos substituir. Depois colocamos esses objetos nos outros argumentos da fun√ß√£o, na ordem em que eles aparecem no texto. sprintf(&#39;Aba%ste&#39;, &#39;ca&#39;) FALSE [1] &quot;Abacate&quot; Quando o argumento √© um vetor, a fun√ß√£o retorna um vetor com as substitui√ß√µes ponto a ponto. sprintf(&#39;Aba%ste&#39;, c(&#39;ca&#39;, &#39;ixas&#39;)) FALSE [1] &quot;Abacate&quot; &quot;Abaixaste&quot; Se o texto cont√©m mais de um %s e os objetos correspondentes s√£o vetores, o sprintf() tenta reciclar os vetores para ficarem do mesmo tamanho. Isso s√≥ funciona quando todos os objetos t√™m comprimentos que s√£o m√∫ltiplos do comprimento do maior objeto. Por exemplo, isso funciona: sprintf(&#39;Aba%s%s&#39;, c(&#39;ca&#39;), c(&#39;xi&#39;, &#39;te&#39;)) # ca foi reciclado FALSE [1] &quot;Abacaxi&quot; &quot;Abacate&quot; Isso n√£o funciona: sprintf(&#39;Aba%s%s&#39;, c(&#39;ca&#39;, &#39;ixaste&#39;), c(&#39;xi&#39;, &#39;te&#39;, &#39;.&#39;)) FALSE Error in sprintf(&quot;Aba%s%s&quot;, c(&quot;ca&quot;, &quot;ixaste&quot;), c(&quot;xi&quot;, &quot;te&quot;, &quot;.&quot;)): arguments cannot be recycled to the same length Nem sempre queremos substituir peda√ßos do nosso texto por outros textos. No lugar do %s, √© poss√≠vel colocar padr√µes para n√∫meros, por exemplo. Eu uso bastante o %d, que recebe inteiros. Uma funcionalidade legal do %d √© a possibilidade de adicionar zeros √† esquerda quando um n√∫mero n√£o atinge certa quantidade de d√≠gitos. Assim, quando ordenamos um vetor de textos que come√ßa com n√∫meros, a ordena√ß√£o √© a mesma da vers√£o num√©rica do vetor. Exemplo: nums &lt;- 1:11 sort(as.character(nums)) # ordenado pela string: 10 vem antes de 2 FALSE [1] &quot;1&quot; &quot;10&quot; &quot;11&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; sort(sprintf(&#39;%02d&#39;, nums)) # ordenado pela string: 02 vem antes de 10 FALSE [1] &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;04&quot; &quot;05&quot; &quot;06&quot; &quot;07&quot; &quot;08&quot; &quot;09&quot; &quot;10&quot; &quot;11&quot; 3.3.3 glue O glue √© um pacote recente. Sua primeira apari√ß√£o no GitHub foi em 23/12/2016. Isso significa que √© prov√°vel que algumas estejam em constante desenvolvimento, mas isso n√£o nos impede de aproveitar o que a ferramenta tem de bom. A fun√ß√£o glue() √© uma generaliza√ß√£o do sprintf() que permite chamar objetos do R diretamente ao inv√©s de utilizar o %s. Os objetos podem estar no global environment ou descritos por meio de objetos nomeados nos argumentos do glue(). Basta inserir os objetos entre chaves {}: library(glue) planeta &lt;- &#39;mundo&#39; glue(&#39;Ol√° {planeta} pela {y}a vez&#39;, y = 1) FALSE Ol√° mundo pela 1a vez Temb√©m √© poss√≠vel adicionar express√µes dentro das chaves: p &lt;- 1.123123123 glue(&#39;{p * 100}% das pessoas adoram R.&#39;) FALSE 112.3123123% das pessoas adoram R. glue(&#39;{scales::percent(p)} das pessoas adoram R.&#39;) FALSE 112% das pessoas adoram R. A fun√ß√£o glue_collapse() √© parecida com o paste() quando collapse = '', mas s√≥ aceita um objeto como entrada: x &lt;- glue_collapse(1:10) x FALSE 12345678910 x == paste(1:10, collapse = &#39;&#39;) FALSE [1] TRUE Se quiser colar os objetos elemento a elemento e depois colapsar, fa√ßa isso explicitamente em duas opera√ß√µes: library(magrittr) glue(&#39;{letters}/{LETTERS}&#39;) %&gt;% glue_collapse(&#39;, &#39;) FALSE a/A, b/B, c/C, d/D, e/E, f/F, g/G, h/H, i/I, j/J, k/K, l/L, m/M, n/N, o/O, p/P, q/Q, r/R, s/S, t/T, u/U, v/V, w/W, x/X, y/Y, z/Z O glue tamb√©m tem uma fun√ß√£o extra para trabalhar melhor com o %&gt;%, o glue_data(). O primeiro argumento dessa fun√ß√£o √© uma lista ou data.frame, e seus nomes s√£o utilizados como vari√°veis para alimentar as chaves das strings. Use o . para fazer opera√ß√µes com toda a base de dados: mtcars %&gt;% head() %&gt;% glue_data(&#39;O carro {row.names(.)} rende {mpg} milhas por gal√£o.&#39;) FALSE O carro Mazda RX4 rende 21 milhas por gal√£o. FALSE O carro Mazda RX4 Wag rende 21 milhas por gal√£o. FALSE O carro Datsun 710 rende 22.8 milhas por gal√£o. FALSE O carro Hornet 4 Drive rende 21.4 milhas por gal√£o. FALSE O carro Hornet Sportabout rende 18.7 milhas por gal√£o. FALSE O carro Valiant rende 18.1 milhas por gal√£o. 3.3.4 Resumo Use paste() para colar ou colapsar elementos usando um separador fixado. Use sprintf() quando quiser colocar objetos dentro de um texto complexo. Em todos os casos existe uma solu√ß√£o usando glue. Atualmente sempre que tenho um problema desse tipo uso o glue. At√© o momento, n√£o encontrei nenhum problema ou dificuldade. A vida do cientista de dados √© mais feliz com o tidyverse! 3.3.5 Extra: O Guilherme Jardim Duarte fez uma √≥tima sugest√£o sobre este artigo. No pacote stringi existe um operador %s+% que cola textos iterativamente, com uma sintaxe similar √† linguagem python, e que permite a colagem de textos usando um simples +. Exemplo: library(stringi) &#39;a&#39; %s+% &#39;ba&#39; %s+% &#39;ca&#39; %s+% &#39;xi&#39; FALSE [1] &quot;abacaxi&quot; Voc√™ pode adicionar esse operador como um atalho no RStudio, an√°logo ao Ctrl+Shift+M que usamos para escrever o %&gt;%. Para isso, veja esse tutorial sobre RStudio Addins. Mais sobre isso no livro R inferno‚Ü© "],
["3-4-leaflet-com-markercluster.html", "3.4 leaflet com markerCluster", " 3.4 leaflet com markerCluster Autor: Julio Dificuldade baixa model O leaflet √© uma biblioteca javascript para cria√ß√£o de mapas interativos. O pacote leaflet do R √© um htmlwidget que permite gerar esses mapas de forma direta no R, para usar em documentos RMarkdown e Shiny. Uma das ferramentas que mais gosto do leaflet √© a fun√ß√£o markerClusterOptions(), que permite agrupar pontos no mapa em conjuntos de v√°rios pontos. Como exemplo, utilizaremos uma base de dados que cont√©m a localiza√ß√£o e informa√ß√µes das varas da Justi√ßa Estadual no Brasil. A Tabela abaixo mostra as primeiras linhas dessa base. A coluna lab j√° foi trabalhada para ser adicionada nos marcadores do mapa como popup. lat long uf municipio nome lab -21.243369 -48.80407 SP Santa Ad√©lia VARA √öNICA VARA √öNICAPRA√áA DR. ADHEMAR DE BARROS 255Santa Ad√©lia - SP, CEP 15950-000Santa Ad√©lia - SPTelefone indispon√≠vel -3.102226 -67.95186 AM Santo Ant√¥nio do I√ß√° VARA DA COMARCA DE SANTO ANT√îNIO DO I√á√Å VARA DA COMARCA DE SANTO ANT√îNIO DO I√á√ÅHUGO RIBEIRO S/NSanto Ant√¥nio do I√ß√° - AM, CEP 69680-000Santo Ant√¥nio do I√ß√° - AM(097) 9791-8763 -3.067617 -59.95668 AM Manaus 2¬∫ VARA DE FAM√çLIA E SUCESS√ïES 2¬∫ VARA DE FAM√çLIA E SUCESS√ïESRUA PARAIBA S/NManaus - AM, CEP 69079-265Manaus - AM(092) 9233-0351 Para utilizar o pacote leaflet, basta instalar o pacote via install.packages(). Na Figura 2.1, experimente passear pelo mapa. Procure tamb√©m algum lugar que tenha v√°rias varas juntas, para ver o que o markerCluster faz nesse caso. library(magrittr) library(leaflet) dados_aj_lab %&gt;% leaflet() %&gt;% addTiles() %&gt;% addMarkers( lng = ~long, lat = ~lat, popup = ~lab, clusterOptions = markerClusterOptions() ) Figura 2.1: Mapa das varas estaduais do Brasil. A fun√ß√£o leaflet() carrega o motor do leaflet, ainda em branco. A fun√ß√£o addTiles() adiciona as camadas de mapas de acordo com o zoom. √â poss√≠vel escolher temas para essas camadas. A fun√ß√£o addMarkers() mapeia as varas da base de dados de acordo com as respectivas latitude e longitude. Note que √© necess√°rio adicionar um ~ antes das vari√°veis para mape√°-las da base de dados. A op√ß√£o popup permite adicionar um bal√£o com informa√ß√µes ao clicar num marcador. A op√ß√£o clusterOptions faz a m√°gica que agrupa os pontos. A regi√£o azul observada ao colocar o mouse sobre um cluster √© a casca convexa dos marcadores agrupados. √â isso! "],
["4-modelagem.html", "Cap√≠tulo 4 Modelagem ", " Cap√≠tulo 4 Modelagem "],
["4-1-monty-hall-e-diagramas-de-influencia.html", "4.1 Monty Hall e diagramas de influ√™ncia", " 4.1 Monty Hall e diagramas de influ√™ncia Autor: Julio Dificuldade alta model Voc√™ est√° num jogo na TV e o apresentador pede para escolher uma entre 3 portas. Atr√°s de uma dessas portas tem uma Ferrari e nas outras duas temos cabras. Voc√™ escolhe uma porta. Depois, o apresentador retira uma porta que tem uma cabra e pergunta: voc√™ quer trocar de porta? A princ√≠pio, voc√™ pode achar que sua probabilidade de ganhar √© 1/2, j√° que uma das portas foi retirada, ent√£o n√£o importa se voc√™ troca ou n√£o. Mas a resposta √© que sim, vale √† pena trocar de porta! A probabilidade de vencer o jogo trocando a porta √© de 2/3. Figura 2.2: Brincadeira do XKCD. O problema de Monty Hall √© talvez o mais eloquente exemplo de como a probabilidade pode confundir a mente humana. Esse problema desafiou a comunidade cient√≠fica no final do s√©culo XX e chegou at√© a ser considerado um paradoxo. Recomendo ler o livro O Andar do B√™bado, de Leonard Mlodinow, que conta essa e muitas outras hist√≥rias interessantes sobre a probabilidade. Existem v√°rias formas de explicar por qu√™ trocar a porta √© a melhor estrat√©gia. A que eu mais gosto √© a do pr√≥prio Andar do B√™bado, que mostra que, quando voc√™ escolhe a primeira porta, voc√™ est√° apostando se acertou ou n√£o a Ferrari. Se voc√™ apostar que acertou a Ferrari, n√£o deve trocar a porta e, se voc√™ apostar que errou a Ferrari, deve trocar. A aposta de errar a Ferrari de primeira tem probabilidade 2/3, logo, vale √† pena trocar. Nesse post, mostramos uma solu√ß√£o alternativa, simples e elegante para o problema usando diagramas de influ√™ncia e o pacote bnlearn. 4.1.1 Redes bayesianas As redes Bayesianas s√£o o resultado da combina√ß√£o de conceitos probabil√≠sticos e conceitos da teoria dos grafos. Segundo Pearl, tal uni√£o tem como consequ√™ncias tr√™s benef√≠cios: i) prover formas convenientes para expressar suposi√ß√µes do modelo; ii) facilitar a representa√ß√£o de fun√ß√µes de probabilidade conjuntas; e iii) facilitar o c√°lculo eficiente de infer√™ncias a partir de observa√ß√µes. Da teoria de probabilidades precisamos apenas de alguns resultados b√°sicos sobre probabilidade condicional. Primeiramente, pela defini√ß√£o de probabilidade condicional, sabemos que \\[ p(x_1, x_2) = p(x_1)p(x_2|x_1). \\] Aplicando essa regra iterativamente para \\(n\\) vari√°veis, temos \\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|x_1,\\dots, x_{j-1}). \\] Agora, imagine que, no seu problema, a vari√°vel aleat√≥ria \\(X_j\\) n√£o dependa probabilisticamente de todas as vari√°veis \\(X_1,\\dots, X_{j-1}\\), e sim apenas de um subconjunto \\(\\Pi_j\\) dessas vari√°veis. Fazendo isso, a equa√ß√£o pode ser escrita como \\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|\\pi_j). \\] Chamamos \\(\\Pi_j\\) de pais de \\(X_j\\). Esse conjunto pode ser pensado como as vari√°veis que s√£o suficientes para determinar as probabilidades de \\(X_j\\). A parte mais legal das redes Bayesianas √© que elas podem ser representadas a partir de DAGs (grafos direcionados ac√≠clicos). No grafo, se \\(X_1\\) aponta para \\(X_2\\), ent√£o \\(X_1\\) √© pai de \\(X_2\\). Por exemplo, esse grafo aqui representa a distribui√ß√£o de probabilidades \\(p(x_1, \\dots, x_5)\\) com \\[ p(x_1, \\dots, x_5) = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_3,x_2)p(x_5|x_4). \\] 4.1.2 Diagrama de influ√™ncia Um diagrama e influ√™ncias √© uma rede Bayesiana com n√≥s de decis√£o e utilidade (ganhos). Ou seja, √© uma jun√ß√£o de tr√™s conceitos: \\[ \\underbrace{\\text{prob. condicional} + \\text{grafos}}_{\\text{rede Bayesiana}} + \\text{teoria da decis√£o} = \\text{diagrama de influ√™ncia} \\] Na teoria da decis√£o, usualmente estamos interessados em maximizar a utilidade esperada. No diagrama, considerando a estrutura de probabilidades dada pela rede Bayesiana e as informa√ß√µes dispon√≠veis, queremos escolher a decis√£o que faz com que, em m√©dia, nosso retorno seja mais alto. Com diagramas de influ√™ncias, √© poss√≠vel organizar sistemas complexos com m√∫ltiplas decis√µes, considerando diferentes conjuntos de informa√ß√µes dispon√≠veis. √â uma ferramenta realmente muito poderosa. 4.1.3 Voltando ao Monty Hall Agora que sabemos um pouquinho de diagramas de influ√™ncia, podemos desenhar o do Monty Hall: O jogador tem duas decis√µes a tomar: \\(D_1\\) (escolha_inicial): A escolha da porta inicial (1, 2, 3). \\(D_2\\) (trocar): Trocar a porta ou n√£o (s, n). Tamb√©m temos duas fontes de incerteza: \\(X_1\\) (ferrari): Em qual porta est√° a Ferrari (1, 2, 3). \\(X_2\\) (porta_retirada): Qual porta foi retirada (1, 2, 3). Essa vari√°vel n√£o √© sempre aleat√≥ria: se eu escolho a porta 1 e a Ferrari est√° em 2, o apresentador √© obrigado a retirar a porta 3. Se o apresentador tiver a op√ß√£o de escolher (que acontece no caso da escolha inicial ser a Ferrari), o apresentador escolhe uma porta para retirar aleatoriamente. Finalmente, temos um n√≥ de utilidade: \\(U_1\\) (result): Ganhei a Ferrari (ganhei, perdi). Em R, podemos construir a rede Bayesiana do problema utilizando o pacote bnlearn: FALSE [,1] [,2] FALSE [1,] &quot;escolha_inicial&quot; &quot;porta_retirada&quot; FALSE [2,] &quot;ferrari&quot; &quot;porta_retirada&quot; FALSE [3,] &quot;porta_retirada&quot; &quot;trocar&quot; FALSE [4,] &quot;trocar&quot; &quot;result&quot; FALSE [5,] &quot;ferrari&quot; &quot;result&quot; FALSE [6,] &quot;escolha_inicial&quot; &quot;result&quot; O output desse conjunto de opera√ß√µes √© um objeto do tipo bn com v√°rias propriedades pr√© calculadas pelo pacote bnlearn: Random/Generated Bayesian network model: [escolha_inicial][ferrari][porta_retirada|escolha_inicial:ferrari][trocar|porta_retirada] [result|escolha_inicial:ferrari:trocar] nodes: 5 arcs: 6 undirected arcs: 0 directed arcs: 6 average markov blanket size: 3.60 average neighbourhood size: 2.40 average branching factor: 1.20 generation algorithm: Empty Com as especifica√ß√£o do problema dada, se gerarmos aleatoriamente todos os cen√°rios, chegamos √† essa combina√ß√£o de casos equiprov√°veis (ver Extra 2) Agora, vamos escrever todas as combina√ß√µes poss√≠veis de cen√°rios e guardar num data.frame chamado dados: |escolha_inicial |ferrari |porta_retirada |trocar |result | |:---------------|:-------|:--------------|:------|:------| |1 |1 |2 |n |ganhei | |1 |1 |2 |s |perdi | |1 |1 |3 |n |ganhei | |1 |1 |3 |s |perdi | |1 |2 |3 |n |perdi | |1 |2 |3 |s |ganhei | |1 |3 |2 |n |perdi | |1 |3 |2 |s |ganhei | |2 |1 |3 |n |perdi | |2 |1 |3 |s |ganhei | |2 |2 |1 |n |ganhei | |2 |2 |1 |s |perdi | |2 |2 |3 |n |ganhei | |2 |2 |3 |s |perdi | |2 |3 |1 |n |perdi | |2 |3 |1 |s |ganhei | |3 |1 |2 |n |perdi | |3 |1 |2 |s |ganhei | |3 |2 |1 |n |perdi | |3 |2 |1 |s |ganhei | |3 |3 |2 |n |ganhei | |3 |3 |2 |s |perdi | |3 |3 |1 |n |ganhei | |3 |3 |1 |s |perdi | Finalmente, ajustamos nossa rede Bayesiana, usando a fun√ß√£o bnlearn::bn.fit(). A fun√ß√£o bnlearn::cpquery() (conditional probability query) serve para realizar uma consulta de probabilidades dada a rede ajustada. No nosso caso, a partir de uma escolha inicial qualquer \\(d_1\\), queremos saber o ganho ao trocar √© maior que o ganho ao n√£o trocar. \\[ \\mathbb E(U_1\\; |\\; D_2 = \\text{s}, D_1 = d_1) &gt; \\mathbb E(U_1\\; |\\; D_2 = \\text{n}, D_1 = d_1). \\] Fazendo contas, isso equivale matematicamente a consultar se \\[ \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s}) &gt; \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{n}) \\] Agora, podemos consultar \\(\\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s})\\) com nosso modelo! [1] 0.6666704 E n√£o √© que d√° 2/3 mesmo? Da mesma forma, temos [1] 0.3333187 Resolvido! 4.1.4 Wrap-up Vale √† pena trocar a porta! Redes Bayesianas juntam grafos e probabilidades condicionais Diagramas de influ√™ncia juntam redes Bayesianas e teoria da decis√£o Essas ferramentas podem ser utilizadas tanto para resolver Monty Hall quanto para ajudar em sistemas complexos. √â isso pessoal. Happy coding ;) 4.1.5 Extra Se voc√™ ficou interessado(a) em como eu fiz o diagrama, utilizei o pacote DiagrammeR. O c√≥digo est√° aqui: 4.1.6 Extra 2 √â poss√≠vel simular os dados que coloquei no post com uma fun√ß√£o simples, que adicionei abaixo. Na verdade, o fato de eu ter considerado somente as combina√ß√µes √∫nicas de cen√°rios e n√£o os dados simulados abaixo √© um pouco roubado, e s√≥ funciona porque os cen√°rios calham de ser, de fato, equiprov√°veis. Observations: 10,000 Variables: 5 $ escolha_inicial &lt;fct&gt; 3, 1, 2, 1, 1, 1, 3, 1, 2, 3, 3, 1, 3, 1, 2, 2, 2,... $ ferrari &lt;fct&gt; 1, 1, 2, 1, 1, 2, 3, 3, 1, 2, 3, 3, 2, 1, 1, 3, 1,... $ porta_retirada &lt;fct&gt; 2, 3, 1, 3, 2, 3, 2, 2, 3, 1, 1, 2, 1, 2, 3, 1, 3,... $ trocar &lt;fct&gt; n, s, s, n, s, n, n, n, n, s, s, s, s, n, n, s, n,... $ result &lt;fct&gt; perdi, perdi, perdi, ganhei, perdi, perdi, ganhei,... Os dados do post podem ser obtidos fazendo isso aqui: Agradecimentos: Rafael Stern, que me convenceu de que vale √† pena mostrar os dados simulados üòâ "],
["4-2-construindo-autoencoders.html", "4.2 Construindo Autoencoders", " 4.2 Construindo Autoencoders Autoencoders s√£o redes neurais treinadas com o objetivo de copiar o seu input para o seu output. Esse interesse pode parecer meio estranho, mas na pr√°tica o objetivo √© aprender representa√ß√µes (encodings) dos dados, que podem ser usadas para redu√ß√£o de dimensionalidade ou at√© mesmo compress√£o de arquivos. Basicamente, um autoencoder √© dividido em duas partes: um encoder que √© uma fun√ß√£o \\(f(x)\\) que transforma o input para uma representa√ß√£o \\(h\\) um decoder que √© uma fun√ß√£o \\(g(x)\\) que transforma a representa√ß√£o \\(h\\) em sua reconstru√ß√£o \\(r\\) Imagem do blog do Keras 4.2.1 Construindo o seu primeiro autoencoder Nesse pequeno tutorial, vou usar o keras para definir e treinar os nossos autoencoders. Como base de dados vou usar algumas simula√ß√µes e o banco de dados mnist (famoso para todos que j√° mexeram um pouco com deep learning). O mnist √© um banco de dados de imagens de tamanho 28x28 de d√≠gitos escritos √† m√£o. Esse dataset promoveu grandes avan√ßos na √°rea de reconhecimento de imagens. Com esse c√≥digo definimos um modelo da seguinte forma: \\[ X = (X*W_1 + b_1)*W_2 + b_2 \\] Em que: \\(X\\) √© o nosso input com dimens√£o (?, 784) \\(W_1\\) √© uma matriz de pesos com dimens√µes (784, 32) \\(b_1\\) √© uma matriz de forma (?, 32) \\(W_2\\) √© uma matriz de pesos com dimens√µes (32, 784) \\(b_2\\) √© uma matriz de forma (?, 784) Note que ? aqui √© o n√∫mero de observa√ß√£oes da base de dados. Agora vamos estimar \\(W_1\\), \\(W_2\\), \\(b_1\\) e \\(b_2\\) de modo a minimizar alguma fun√ß√£o de perda. Inicialmente vamos usar a binary crossentropy por pixel que √© definida por: \\[-\\sum_{i=1}y_i*log(\\hat{y}_i)\\] Isso √© definido no keras usando: N√£o vou entrar em detalhes do que √© o adadelta, mas √© uma varia√ß√£o do m√©todo de otimiza√ß√£o conhecido como gradient descent. Agora vamos carregar a base de dados e em seguida treinar o nosso autoencoder`. Estimamos os par√¢metros desse modelo no keras fazendo: Depois de rodar todas as itera√ß√µes, voc√™ poder√° usar o seu encoder e o seu decoder para entender o que eles fazem com as imagens. Veja o exemplo a seguir em que vamos obter os encodings para as 10 primeiras imagens da base de teste e depois reconstruir a imagem usando o decoder. FALSE [1] 10 32 FALSE [1] 0.0000000 10.1513205 3.5742311 2.6635208 6.3097358 3.4840517 FALSE [7] 9.1041250 6.6329145 1.6385922 9.8017225 9.5529270 1.6670935 FALSE [13] 5.7208562 4.8035479 3.9149191 0.6408147 1.2716029 3.1215091 FALSE [19] 13.7575903 0.0000000 1.8692881 3.2142215 0.7444992 5.0728440 FALSE [25] 8.2932110 9.9866810 2.7651572 11.1291723 5.2460670 5.6875997 FALSE [31] 10.6097431 3.6338394 O encoder transforma a matriz de (10, 784) para uma matriz com dimensao (10, 2). Podemos reconstruir a imagem, a pardir da imagem que foi comprimida usando o nosso decoder. Compare as reconstru√ß√µes com as imagens originais abaixo: Um ponto interessante √© que esse modelo faz uma aproxima√ß√£o da solu√ß√£o por componentes principais! Na verdade, a defini√ß√£o do quanto s√£o parecidos √© quase-equivalente. Isso quer dizer que os pesos \\(W\\) encontrados pelo PCA e pelo autoencoder ser√£o diferentes, mas o sub-espa√ßo criado pelos mesmos ser√° equivalente. Se s√£o equivalentes, qual a vantagem de usar autoencoders ao inv√©s de PCA? O PCA para por aqui, voc√™ define que ser√£o apenas rela√ß√µes lineares, e voc√™ reduz dimens√£o apenas reduzindo o tamanho da matriz. Em autoencoders voc√™ tem diversas outras sa√≠das para aprimorar o m√©todo. A primeira delas √© simplesmente adicionar uma condi√ß√£o de esparsidade nos pesos. Isso vai reduzir o tamanho do vetor latente (como √© chamada a camada do meio do autoencoder) tamb√©m, pois ele ter√° mais zeros. Isso pode ser feito rapidamente com o keras. Basta adicionar um activity_regularizer em nossa camada de encoding. Isso vai adicionar na fun√ß√£o de perda um termo que toma conta do valor dos outputs da camada intermedi√°ria. Outra forma de melhorar o seu autoencoder √© permitir que o encoder e o decoder sejam redes neurais profundas. Com isso, ao inv√©s de tentar encontrar transforma√ß√µes lineares, voc√™ permitir√° que o autoencoder encontre transforma√ß√µes n√£o lineares. Mais uma vez fazemos isso com o keras: Existem formas ainda mais inteligentes de construir esses autoencoders, mas o post iria ficar muito longo e n√£o ia sobrar asssunto para o pr√≥ximo. Se voc√™ quiser saber mais, recomendo fortemente a leitura deste artigo do blog do Keras e desse cap√≠tulo. Uma fam√≠lia bem moderna de autoencoders s√£o os VAE (Variational Autoencoders). Esses autoencoders aprendem modelos de vari√°veis latentes. Isso √© interessante porque permite que voc√™ gere novos dados, parecidos com os que voc√™ usou para treinar o seu autoencoder. Voc√™ pode encontrar uma implementa√ß√£o desse modelo aqui. "],
["4-3-modelos-beseados-em-arvores-e-a-multicolinearidade.html", "4.3 Modelos beseados em √°rvores e a multicolinearidade", " 4.3 Modelos beseados em √°rvores e a multicolinearidade Modelos baseados em √°rvores como √°rvores de decis√£o, random forest, ligthGBM e xgboost s√£o conhecidos, dentre outras qualidades, pela sua robust√™s diante do problema de multicolinearidade. √â sabido que seu poder preditivo n√£o se abala na presen√ßa de vari√°veis extremamente correlacionadas. Por√©m, quem nunca usou um Random Forest pra fazer sele√ß√£o de vari√°veis? Pegar, por exemplo, as top 10 mais importantes e descartar o resto? Ou at√© mesmo arriscou uma interpreta√ß√£o e concluiu sobre a ordem das vari√°veis mais importantes? Abaixo mostraremos o porqu√™ n√£o devemos ignorar a quest√£o da multicolinearidade completamente! 4.3.1 Um modelo bonitinho Primeiro vamos ajustar um modelo bonitinho, livre de multicolinearidade. Suponha que queiramos prever Petal.Length utilizando as medidas das s√©palas (Sepal.Width e Sepal.Length) da nossa boa e velha base iris. O gr√°fico acima mostra que as vari√°veis explicativas n√£o s√£o fortemente correlacionadas. Ajustando uma random fores, temos a seguinte ordem de import√¢ncia das vari√°veis: Sem surpresas. Agora vamos para o problema! 4.3.2 Um modelo com feinho Vamos forjar uma situa√ß√£o extrema em que muitas vari√°veis sejam multicolineares. Vou fazer isso repetindo a coluna Sepal.Length v√°rias vezes. Agora a coisa t√° feia! Temos 20 vari√°veis perfeitamente colineares. Mesmo assim um random forest nessa nova base n√£o perderia poder preditivo. Mas como ficou a import√¢ncia das vari√°veis? Aqui o jogo j√° se inverteu: concluir√≠amos que Sepal.Width √© mais importante de todas as vari√°veis! 4.3.3 Sele√ß√£o de vari√°veis furado O gr√°fico abaixo mostra que quanto mais vari√°veis correlacionadas tivermos, menor a import√¢ncia de TODAS ELAS SIMULTANEAMENTE! √â como se as vari√°veis colineares repartissem a import√¢ncia entre elas. Na pr√°tica, se estabelecessemos um corte no valor de import√¢ncia pra descartar vari√°veis (como ilustrado pela linha vermelha), ter√≠amos um problema em potencial: poder√≠amos estar jogando fora informa√ß√£o muito importante. 4.3.4 Como tratar multicolinearidade, ent√£o? Algumas maneiras de lidar com multicolinearidade s√£o: Observar a matriz de correla√ß√£o VIF Recursive feature elimination 4.3.5 Conclus√£o Cuidado ao jogar tudo no caldeir√£o! Devemos sempre nos preocupar com multicolinearidade, mesmo ajustando modelos baseados em √°rvores. "],
["4-4-woe-em-r-com-tidywoe.html", "4.4 WoE em R com tidywoe", " 4.4 WoE em R com tidywoe WoE (weight of evidence) √© uma ferramenta bastante usada em aplica√ß√µes de regress√£o log√≠stica, principalmente na √°rea de score de cr√©dito. Simploriamente falando, ele transforma categorias em n√∫meros que refletem a diferen√ßa entre elas pelo crit√©rio de separa√ß√£o do Y = 1 e Y = 0. Se voc√™ ainda n√£o sabe o que √© ou quer ler mais sobre o assunto, um texto que eu gostei de ler: Data Exploration with Weight of Evidence and Information Value in R O autor desse texto √© o Kim Larsen, criador do pacote Information que √© completo e cheio de ferramentas sofisticadas em torno do WoE. Por√©m, no dia a dia do meu trabalho volta e meia eu tinha que construir rotinas pr√≥prias para fazer as vers√µes em WoE das minhas vari√°veis, mesmo com v√°rios pacotes completos dispon√≠veis. A principal motiva√ß√£o era que eles n√£o eram muito pr√°ticos e n√£o se encaixavam na filosofia do tidyverse. Da√≠ acabei juntando essas rotinas num pacote chamado tidywoe e deixando no ar. A ideia √© que ela fa√ßa o analista ganhar em tempo, legibilidade e reprodutibilidade. Abaixo segue como usar. 4.4.1 Instala√ß√£o e dados Para instalar, basta rodar abaixo. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;athospd/tidywoe&quot;) library(tidyverse) library(tidywoe) # install.packages(&quot;FactoMineR&quot;) data(tea, package = &quot;FactoMineR&quot;) tea_mini &lt;- tea %&gt;% dplyr::select(breakfast, how, where, price) 4.4.2 Como usar Tem duas fun√ß√µes que importam: - add_woe() - adiciona os woe‚Äôs num data frame. - woe_dictionary() - cria dicion√°rio que mapeia as categorias com os woe‚Äôs. 4.4.3 add_woe() A fun√ß√£o add_woe() serve para adicionar as vers√µes WoE‚Äôs das vari√°veis em sua amostra de dados. tea_mini %&gt;% add_woe(breakfast) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 -0.2564295 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Voc√™ pode selecionar as vari√°veis que vc quiser selecionando-as como se fosse no dplyr::select(). tea_mini %&gt;% add_woe(breakfast, where:price) 4.4.4 woe_dictionary() A fun√ß√£o woe_dictionary() √© uma das duas partes necess√°rias para fazer o add_woe() funcionar (a outra parte s√£o os dados). Ele constr√≥i o dicion√°rio de categorias e seus respectivos woe‚Äôs. tea_mini %&gt;% woe_dictionary(breakfast) variable explanatory n_tot n_breakfast n_Not.breakfast p_breakfast p_Not.breakfast woe how tea bag 170 80 90 0.5555556 0.5769231 -0.0377403 how tea bag+unpackaged 94 50 44 0.3472222 0.2820513 0.2078761 how unpackaged 36 14 22 0.0972222 0.1410256 -0.3719424 where chain store 192 90 102 0.6250000 0.6538462 -0.0451204 4.4.5 Usando um dicion√°rio customizado Muitas vezes h√° o interesse em ajustar na m√£o alguns valores de woe para consertar a ordem dos efeitos de uma dada vari√°vel ordinal. Esse √© o motivo de o add_woe() poder receber um dicion√°rio passado pelo usu√°rio. Isso se faz por meio do argumento .woe_dictionary. A maneira mais f√°cil de se fazer isso √© montar um dicion√°rio inicial com o woe_dictionary() e depois alterar os valores nele para alcan√ßar os ajustes desejados. Exemplo: # Construa um dicion√°rio inicial tea_mini_woe_dic &lt;- tea_mini %&gt;% woe_dictionary(breakfast) # Mexa um pouquinho nos woes tea_mini_woe_dic_arrumado &lt;- tea_mini_woe_dic %&gt;% mutate(woe = if_else(explanatory == &quot;p_unknown&quot;, 0, woe)) # Passe esse dicion√°rio para o add_woe() tea_mini %&gt;% add_woe(breakfast, .woe_dictionary = tea_mini_woe_dic_arrumado) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 0.0000000 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 4.4.6 Exemplo de explora√ß√£o O woe_dictionary() devolve uma tabela arrumada, bem conveniente para explorar mais. Por exemplo, a tabela est√° pronta para o ggplot. Aqui est√° o github do pacote para contribui√ß√µes. Pretendo colocar bastante coisa nova no pacote ainda. "],
["4-5-regressao-logistica-aspectos-computacionais.html", "4.5 Regress√£o log√≠stica: aspectos computacionais", " 4.5 Regress√£o log√≠stica: aspectos computacionais Neste texto vamos discutir um pouco sobre regress√£o log√≠stica, tensorflow e Modelos Lineares Generalizados (Generalized Linear Models, GLMs). N√£o vou economizar nas matem√°ticas nem nos c√≥digos. Se voc√™ n√£o conhece GLMs, recomendo dar uma lida, pelo menos na introdu√ß√£o, do livro do professor Gilberto A. Paula. Se voc√™ n√£o conhece o Tensorflow, recomendo ver a p√°gina do RStudio sobre Tensorflow. Se voc√™ curte a parte computacional da estat√≠stica, esse livro do LEG-UFPR √© obrigat√≥rio. Eles s√£o os melhores. 4.5.1 Introdu√ß√£o: o tensorglm Um de meus interesses no momento √© implementar GLMs usando Tensorflow. O Tensorflow √© uma biblioteca computacional mantida pela Google que utiliza paraleliza√ß√£o e o poder das GPUs (Graphical Processing Units) para fazer contas. O Tensorflow foi especialmente desenhado para facilitar o ajuste de redes neurais profundas e outros modelos sofisticados. GLMs s√£o casos particulares de redes neurais. Uma rede neural com apenas uma camada e com fun√ß√µes de perda / verossimilhan√ßas baseadas na Diverg√™ncia de Kullback-Leibler s√£o exatamente iguais aos GLMs. Por exemplo, essa diverg√™ncia equivale ao erro quadr√°tico m√©dio para a distribui√ß√£o gaussiana e binary-crossentropy para log√≠stica. Por isso, n√£o √© de se surpreender que j√° existam solu√ß√µes prontas para modelos espec√≠ficos, como regress√£o linear normal, log√≠stica, e at√© Poisson. No entanto, essas solu√ß√µes t√™m duas limita√ß√µes: N√£o s√£o extensivas. Por exemplo, n√£o achei c√≥digos para as distribui√ß√µes normal inversa, gama e binomial negativa. As solu√ß√µes atuais utilizam o algoritmo descida de gradiente para otimiza√ß√£o, que √© muito legal, mas n√£o se aproveita de alguns resultados que temos na √°rea de GLMs, como o IWLS (Iterated Weighted Least Squares), que √© uma deriva√ß√£o do algoritmo Fisher-scoring, que reduz o problema do ajuste ao c√°lculo iterado de inversas e multiplica√ß√µes de matrizes. Meu intuito √©, ent√£o, montar uma solu√ß√£o alternativa que funcione igual √† fun√ß√£o glm() do R, mas usando Tensorflow no backend ao inv√©s do algoritmo atual, que √© em Fortran. Com isso, espero que o ajuste seja mais eficiente quando os dados s√£o grandes e permita trabalhar com dados que n√£o cabem na mem√≥ria. 4.5.2 A regress√£o log√≠stica Meu primeiro experimento com o tensorglm foi implementar a regress√£o log√≠stica usando tensorflow, com descida de gradiente. Considere o problema \\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta x),\\] em que \\(Y\\) √© nossa vari√°vel resposta, \\(x\\) √© nossa vari√°vel explicativa, \\(\\alpha\\) e \\(\\beta\\) s√£o os par√¢metros que queremos estimar e \\(\\sigma(\\cdot)\\) √© a fun√ß√£o sigmoide, cuja inversa √© a fun√ß√£o de liga√ß√£o log√≠stica. \\[\\sigma(\\eta) = \\frac{1}{1 + e^{-\\eta}}\\] Considerando que temos observa√ß√µes \\(Y_1, \\dots, Y_n\\) condicionalmente independentes, j√° temos o suficiente para especificar nosso modelo de regress√£o log√≠stica. O pr√≥ximo passo √© definir, com base nisso, a fun√ß√£o que queremos otimizar. A partir de uma amostra \\(y_1, \\dots, y_n\\) e observando que \\(\\mu_i = \\sigma(\\alpha + \\beta x_i)\\), a verossimilhan√ßa do modelo √© dada por \\[ \\mathcal L((\\alpha, \\beta)|\\mathbf y) = \\prod_{i=1}^n f(y_i|(\\alpha, \\beta), x_i) = \\prod_{i=1}^n\\mu_i^{y_i}(1-\\mu_i)^{1-y_i} \\] O logaritmo da verossimilhan√ßa √© dado por \\[ \\begin{aligned} l((\\alpha, \\beta)|\\mathbf y) &amp;= \\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\\\ &amp;= \\sum_{i=1}^n y_i\\log(\\sigma(\\alpha + \\beta x_i)) + (1-y_i)\\log(1 - \\sigma(\\alpha + \\beta x_i)) \\end{aligned} \\] Nosso objetivo √© maximizar \\(l\\) com rela√ß√£o √† \\(\\alpha\\) e \\(\\beta\\). Detalhe: essa soma, se multiplicada por -1, tamb√©m √© chamada de fun√ß√£o de perda binary cross-entropy. Por isso que tanto faz voc√™ definir GLMs a partir de \\(P(Y|x)\\) ou a partir da fun√ß√£o de perda! OK, problema dado! vamos implementar usando tensorflow! Feito! Agora podemos usar a magia do tensorflow, que √© esperto o suficiente para otimizar essa perda sem a gente se preocupar em calcular derivadas na m√£o. Para quem n√£o conhece o algoritmo de descida de gradiente, ele funciona assim: \\[ (\\alpha, \\beta)_{\\text{novo}} = (\\alpha, \\beta)_{\\text{velho}} + k \\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}}), \\] onde \\(\\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}})\\) √© o gradiente da verossimilhan√ßa em rela√ß√£o ao vetor \\((\\alpha, \\beta)\\), ou seja, s√£o as derivadas parciais de \\(l\\) em rela√ß√£o √† \\(\\alpha\\) e \\(\\beta\\). Isso d√° a dire√ß√£o e intensidade em que os valores devem ser atualizados. \\(k\\) √© chamado de learning rate, √© um fator usado para controlar o tamanho do passo dado pelo gradiente. Esse valor normalmente √© definido √† m√£o. No caso dos GLMs, \\(k\\) √© substitu√≠do pelo inverso da segunda derivada da \\(l\\) em rela√ß√£o aos par√¢metros, gerando assim os algoritmos de Newton-Raphson e Fisher-scoring. Detalhe: se voc√™ procurar esse algoritmo na internet, voc√™ vai encontrar um \\(-\\) e n√£o um \\(+\\). Isso acontece porque estamos usando a verossimilhan√ßa e n√£o a perda. Iter: 01, alpha=2.32, beta=3.593 Iter: 02, alpha=1.56, beta=3.409 Iter: 03, alpha=1.411, beta=2.989 Iter: 04, alpha=1.261, beta=2.665 Iter: 05, alpha=1.153, beta=2.422 Iter: 06, alpha=1.078, beta=2.257 Iter: 07, alpha=1.033, beta=2.154 Iter: 08, alpha=1.006, beta=2.095 Iter: 09, alpha=0.992, beta=2.062 Iter: 10, alpha=0.984, beta=2.045 Parece que funcionou! Agora sabemos ajustar uma regress√£o log√≠stica na m√£o, com o algoritmo de descida de gradiente‚Ä¶ ou ser√° que n√£o? 4.5.3 O Problema Vamos considerar o mesmo problema, mas agora com duas explicativas. temos \\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta_1 x_2+ \\beta_2 x_2),\\] As contas s√£o exatamente as mesmas e vou omitir, mostrando apenas o c√≥digo novo. Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Oops! Explodiu! Por que ser√°??? Uma forma de corrigir esse problema √© considerando uma taxa de aprendizado k um pouco menor. Com os mesmos dados e modelo acima, ao fazer e rodar novamente, j√° conseguimos chegar nos resultados abaixo. Iter: 01, alpha=1.525, beta1=2.492, beta2=3.205 Iter: 02, alpha=1.183, beta1=2.32, beta2=3.36 Iter: 03, alpha=1.122, beta1=2.248, beta2=3.34 Iter: 04, alpha=1.101, beta1=2.208, beta2=3.296 Iter: 05, alpha=1.085, beta1=2.178, beta2=3.254 Iter: 06, alpha=1.073, beta1=2.152, beta2=3.216 Iter: 07, alpha=1.062, beta1=2.13, beta2=3.183 Iter: 08, alpha=1.053, beta1=2.112, beta2=3.154 Iter: 09, alpha=1.044, beta1=2.095, beta2=3.13 Iter: 10, alpha=1.037, beta1=2.082, beta2=3.109 Mais algumas itera√ß√µes e o modelo converge. Mas n√≥s n√£o queremos ficar fazendo um ajuste t√£o fino no valor de k, certo? Afinal, queremos resolver problemas do mundo real, n√£o ficar escolhendo valores de k‚Ä¶ Outra forma de resolver isso √© evitando problemas num√©ricos nas contas. O c√°lculo da fun√ß√£o de perda, por exemplo, pode ser melhorado. Mas como? Bom, problemas num√©ricos n√£o s√£o minha especialidade, ent√£o agora √© hora de seguir os mestres. Vamos olhar como o R e como o Tensorflow implementam as fun√ß√µes de perda para regress√£o log√≠stica. 4.5.3.1 Os objetos de classe family no R No R, os GLMs buscam informa√ß√µes de objetos da classe family() para realizar os ajustes. No caso da log√≠stica, o objeto √© retornado por uma fun√ß√£o chamada binomial(). O resultado disso √© uma lista com v√°rios m√©todos implementados. Por exemplo, a vari√¢ncia da binomial √© dada por: function (mu) mu * (1 - mu) &lt;bytecode: 0x55fc8e220a18&gt; &lt;environment: 0x55fca4eb5040&gt; A fun√ß√£o de perda √© dada pelo m√©todo fam$dev.resids() (res√≠duos deviance), e o c√≥digo fonte √©: function (y, mu, wt) .Call(C_binomial_dev_resids, y, mu, wt) &lt;bytecode: 0x55fc8e2253a0&gt; &lt;environment: 0x55fca4eb5040&gt; Hmm, parece que √© uma fun√ß√£o feita em C. Como as contas da nossa perda (soma, logaritmo, multiplica√ß√£o e divis√£o) j√° s√£o todas implementadas em C, provavelmente a conta foi implementada em C para garantir estabilidade num√©rica. Olhando o c√≥digo-fonte do pacote stats, encontramos a defini√ß√£o da fun√ß√£o. A fun√ß√£o √© um pouco longa, ent√£o eu mantive apenas as partes importantes: static R_INLINE double y_log_y(double y, double mu) { return (y != 0.) ? (y * log(y/mu)) : 0; } SEXP binomial_dev_resids(SEXP y, SEXP mu, SEXP wt) { /* inicializa√ß√£o de vari√°veis e verifica√ß√µes */ /* rmu e ry s√£o os valores de mu e y transformados para reais */ /* rmu e ry s√£o os valores de mu e y transformados para reais */ for (i = 0; i &lt; n; i++) { mui = rmu[i]; yi = ry[i]; rans[i] = 2 * rwt[lwt &gt; 1 ? i : 0] * (y_log_y(yi, mui) + y_log_y(1 - yi, 1 - mui)); } /* outros c√≥digos n√£o muito importantes */ UNPROTECT(nprot); return ans; } Eu n√£o programo muito em C, mas desse c√≥digo d√° para ver duas coisas importantes: i) a fun√ß√£o y_log_y s√≥ faz a conta se o valor de \\(y\\) for diferente de zero, se n√£o, ela j√° retorna zero; ii) a fun√ß√£o y_log_y faz a conta \\(y\\log({y}/{\\mu})\\), ao inv√©s de apenas \\(y\\log({\\mu})\\). Isso acontece pois no R estamos minimizando o Desvio do modelo, dado por \\[ \\begin{aligned} &amp;D(\\mathbf y, \\mu) = 2[l(\\mathbf y|\\mathbf y) - l(\\mathbf y|(\\alpha, \\beta))]\\\\ &amp;=2\\left[\\sum_{i=1}^n y_i\\log(y_i) + (1-y_i)\\log(1-y_i)\\right. - \\\\ &amp;\\left. -\\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\right] \\\\ &amp;=2\\left[\\sum_{i=1}^n y_i\\log\\left(\\frac{y_i}{\\mu_i}\\right) + (1-y_i)\\log\\left(\\frac{1-y_i}{1-\\mu_i}\\right)\\right]. \\end{aligned} \\] Essa √© a formula√ß√£o usual na literatura de GLMs, que apresenta uma s√©rie de propriedades estat√≠sticas. Minimizar o desvio equivale a maximizar a verossimilhan√ßa. Ser√° que isso ajuda nos problemas num√©ricos? Vamos ver: Iter: 01, alpha=NaN, beta1=NaN, beta2=NaN Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Hmm, parece que n√£o. Se olharmos mais atentamente para a fun√ß√£o desvio, como \\(y\\) pode assumir apenas os valores zero ou um, √© poss√≠vel observar que a conta √© equivalente √† perda calculada anteriormente. Possivelmente o problema aqui √© que o tensorflow n√£o trabalha muito bem com essas condi√ß√µes (tf$where) na perda, e isso d√° problemas na hora de calcular o gradiente. Essa fun√ß√£o do R simplesmente n√£o resolve o problema inicial. Melhor olhar o que o tensorflow faz! 4.5.4 A binary cross-entropy no Tensorflow Eu escondi de voc√™s, mas o tensorflow j√° tem a fun√ß√£o de perda implementada: tf$nn$sigmoid_cross_entropy_with_logits. Ela j√° assume que a fun√ß√£o de liga√ß√£o √© log√≠stica, por isso o sigmoid_ no in√≠cio. Traduzindo livremente o help da fun√ß√£o, temos o seguinte (z=\\(y\\) e x=\\(\\eta = \\alpha + \\beta x\\)) z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x)) = (1 - z) * x + log(1 + exp(-x)) = x - x * z + log(1 + exp(-x)) Para \\(\\eta &lt; 0\\) para evitar problemas num√©ricos com \\(\\exp(-\\eta)\\), reformulamos para x - x * z + log(1 + exp(-x)) = log(exp(x)) - x * z + log(1 + exp(-x)) = - x * z + log(1 + exp(x)) Ent√£o, para garantir estabilidade e evitar problemas num√©ricos, a implementa√ß√£o usa essa formula√ß√£o equivalente max(x, 0) - x * z + log(1 + exp(-abs(x))) Beleza, vamos tentar! Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=1.276, beta1=2.495, beta2=3.608 Iter: 03, alpha=1.197, beta1=2.396, beta2=3.562 Iter: 04, alpha=1.164, beta1=2.335, beta2=3.489 Iter: 05, alpha=1.14, beta1=2.287, beta2=3.42 Iter: 06, alpha=1.12, beta1=2.245, beta2=3.358 Iter: 07, alpha=1.102, beta1=2.21, beta2=3.303 Iter: 08, alpha=1.086, beta1=2.178, beta2=3.256 Iter: 09, alpha=1.073, beta1=2.152, beta2=3.215 Iter: 10, alpha=1.061, beta1=2.129, beta2=3.18 Funcionou! :) 4.5.5 Wrap-up Tensorflow √© uma biblioteca interessante a ser explorada. √â poss√≠vel implementar uma regress√£o log√≠stica do zero em poucos passos. Precisamos tomar cuidado com problemas num√©ricos! No futuro, brincaremos tamb√©m com o algoritmo IWLS. Ser√° que ele roda mais r√°pido que a descida de gradiente? "],
["4-6-sao-paulo-e-o-problema-da-mochila.html", "4.6 S√£o Paulo e o problema da mochila", " 4.6 S√£o Paulo e o problema da mochila Autor: Julio Dificuldade m√©dia model S√£o Paulo √© a minha cidade preferida, n√£o s√≥ porque moro aqui, mas tamb√©m porque √© uma cidade cheia de diversidade, boa gastronomia e oportunidades. Para sentir um pouco dessa vibe, recomendo passear na avenida Paulista aos domingos. √â sensacional! Mas a cidade da diversidade s√≥ √© o que √© porque temos muita, muita gente nela. O munic√≠pio tem 12 milh√µes de habitantes. Esse n√∫mero √© t√£o grande que temos um paulistano para cada 17 brasileiros! Se S√£o Paulo fosse um pa√≠s, seria o 77 do mundo, ganhando de pa√≠ses como a B√©lgica, Gr√©cia, Portugal, Bol√≠via e muitas outras. Outro dia eu estava pensando na seguinte problem√°tica: qual √© a √°rea do Brasil ocupada pela popula√ß√£o de S√£o Paulo? Ou seja, se pegarmos os munic√≠pios com grandes √°reas, quanto do pa√≠s conseguir√≠amos preencher com 12 milh√µes de habitantes? O interessante √© que essa quest√£o recai exatamente no problema da mochila, que √© um famoso desafio de programa√ß√£o inteira. Depois de estudar profundamente no wikipedia, vi que o problema n√£o √© t√£o trivial como parece. 4.6.1 O problema da mochila Considere o seguinte contexto: voc√™ tem uma mochila com capacidade de 15kg e precisa carregar a combina√ß√£o de itens com maior valor, com cada item possuindo valores e pesos diferentes. Figura 2.3: Knapsack problem. Retirado do Wikipedia. Outra forma de pensar nesse problema √© com um card√°pio de restaurante: Figura 2.4: XKCD sobre o knapsack problem. Em linguagem matem√°tica, o que temos √© a task: \\[ \\begin{aligned} &amp; \\text{maximizar } \\sum_{i=1}^n v_i x_i \\\\ &amp; \\text{sujeito √† } \\sum_{i=1}^n w_i x_i \\leq W, \\text{ com } x_i \\in\\{0,1\\}\\\\ \\end{aligned} \\] No nosso caso essas letras significam isso aqui: \\(n\\) √© o n√∫mero de munic√≠pios no Brasil (5570). \\(v_i\\) √© a √°rea do munic√≠pio \\(i\\). \\(w_i\\) √© a popula√ß√£o do munic√≠pio \\(i\\). \\(W\\) √© a popula√ß√£o de S√£o Paulo (12 milh√µes). \\(x=(x_1,\\dots,x_n)^\\top\\) √© o vetor que seleciona os munic√≠pios. Se o munic√≠pio \\(i\\) faz parte da solu√ß√£o \\(x_i=1\\) e, caso contr√°rio, \\(x_i=0\\). Ou seja, queremos escolher munic√≠pios para colocar na mochila tentando maximizar a √°rea, mas o m√°ximo de popula√ß√£o que podemos contemplar √© 12 milh√µes. O problema da mochila √© muito interessante pois trata-se de um problema NP-dif√≠cil, ou seja, n√£o existe um algoritmo de polinomial capaz de resolv√™-lo. Se \\(w_i &gt; 0, \\forall i\\in1,\\dots,n\\) ent√£o a solu√ß√£o pode ser encontrada com um algoritmo pseudo-polinomial. 4.6.2 Forma ad-hoc Se \\(x_i\\) pudesse assumir valores entre zero e um (ou seja, se pud√©ssemos selecionar apenas peda√ßos de munic√≠pios), a solu√ß√£o seria trivial. Bastaria colocar os munic√≠pios em ordem decrescente pela raz√£o \\(v_i/w_i\\) e escolher os munic√≠pios ou parte deles at√© obter \\(W\\). Isso indica uma forma sub-√≥tima de resolver o problema. Chamamos essa solu√ß√£o de ad-hoc. A solu√ß√£o √© encontrada assim: Colocar os munic√≠pios em ordem decrescente pela raz√£o \\(v_i/w_i\\), Escolher os munic√≠pios de maior raz√£o at√© que a popula√ß√£o do pr√≥ximo munic√≠pio estoure \\(W\\). Escolher outros munic√≠pios com maior raz√£o na ordem at√© n√£o ser poss√≠vel incluir mais nenhum munic√≠pio. 4.6.3 Solu√ß√£o √≥tima A solu√ß√£o √≥tima pode ser encontrada usando a fun√ß√£o mknapsack() do pacote adagio. Por exemplo, considere os vetores de pesos w, valores p e m√°ximo cap abaixo. O vetor-solu√ß√£o √© dado por FALSE [1] 1 1 1 1 0 1 0 0 4.6.4 Dados As √°reas e estimativas das popula√ß√µes dos munic√≠pios do Brasil em 2016 foram obtidas do IBGE. A leitura √© realizada usando pacotes do tidyverse. Pacotes: Dados: 4.6.5 Resultados A solu√ß√£o ad-hoc e √≥tima s√£o computadas com esse c√≥digo: Agora, vamos melhorar a solu√ß√£o ad-hoc incluindo os melhores munic√≠pios. A Tabela 2.1 mostra os munic√≠pios que foram classificados diferentemente nos dois m√©todos. Note que a solu√ß√£o √≥tima trocou apenas um munic√≠pio da solu√ß√£o adhoc (Coivaras - PI) pelo munic√≠pio de Ang√©lica - MS. Tabela 2.1: Munic√≠pios diferentes nas duas solu√ß√µes. uf nome area pop s_adhoc s_knapsack PIAU√ç COIVARAS 485373828 3953 1 0 MATO GROSSO DO SUL ANG√âLICA 1282110939 10458 0 1 A Tabela 2.2 mostra a diferen√ßa dos resultados dos dois m√©todos. A solu√ß√£o √≥tima fica com apenas 165 pessoas a menos que S√£o Paulo. Tabela 2.2: Diferen√ßa dos resultados. M√©todo √Årea total Popula√ß√£o total Diferen√ßa para sp adhoc 5574729418852.4 12100250 6670 knapsack 5575526155963.55 12106755 165 S√£o Paulo - 12106920 0 4.6.6 Mapa final Visualmente, a solu√ß√£o √≥tima e a solu√ß√£o adhoc s√£o id√™nticas. Por isso vou mostrar apenas como fica o mapa para a solu√ß√£o √≥tima. O resultado aparece na Figura 2.5. √â realmente impressionante ver que aquela regi√£ozinha vermelha tem a mesma popula√ß√£o que toda a regi√£o azul do mapa. Figura 2.5: Mapa do Brasil final. √â isso! Happy coding ;) "],
["4-7-as-cores-da-marvel-vs-dc.html", "4.7 As cores da Marvel vs DC", " 4.7 As cores da Marvel vs DC Autor: William Dificuldade m√©dia model A cor √© uma diferen√ßa not√°vel entre os filmes da Marvel e da DC. Enquanto a Disney/Marvel Studios costuma lan√ßar filmes com tons mais claros e alegres, a Warner tem optado por cen√°rios escuros, com um aspecto mais sombrios. Essas escolhas s√£o um reflexo do clima das hist√≥rias de cada universo: aventuras engra√ßaralhas com um drama superficial vs seja l√° o que passa na cabe√ßa do Zack Snyder. Para estudar melhor a paleta de cores utilizadas nos filmes, vamos aplicar a an√°lise introduzida pelo Dani neste post, com pequenas altera√ß√µes. Como amostra, selecionei 10 imagens de Batman vs Superman e 10 do Capit√£o Am√©rica: guerra civil. Tentando deixar a an√°lise o menos subjetiva poss√≠vel, escolhi imagens de cenas emblem√°ticas e dos principais personagens. Abaixo as imagens que peguei de cada filme. Seguindo a an√°lise do Dani, vamos utilizar as seguintes bibliotecas para a an√°lise. Eu salvei as imagens em arquivos do tipo bvs_n.jpg e cw_n.jpg, com n variando de 1 a 10. Isso facilitou a leitura desses arquivos. O c√≥digo abaixo mostra como criar um vetor com o caminho das 10 imagens de cada filme. Como vamos trabalhar com mais de uma imagem, eu criei a fun√ß√£o ler_imagem() para ler os arquivos. Podemos ent√£o usar a fun√ß√£o map() para aplic√°-la a todos os 10 arquivos. A fun√ß√£o reduce(rbind) transforma as 10 matrizes de pixels em uma matriz s√≥, como se as imagens estivessem coladas uma embaixo da outra. Abaixo est√£o as fun√ß√µes cria_paleta() e exibir() do post do Dani. A √∫nica diferen√ßa aqui √© que a fun√ß√£o cria_paleta() j√° recebe a matriz representando a imagem. Assim, basta aplicar essas fun√ß√µes aos objetos img_bvs e img_cw para obter as paletas. Primeiro para o Batman vs Superman: E agora para o Capit√£o Am√©rica: Observe que o filme da DC tem cores mais escuras e fortes, com v√°rios tons de azul, indicando as cenas noturnas e de chuva. J√° a paleta da Marvel apresenta cores mais claras, com v√°rios tons representando o c√©u p√°lido das cenas externas. Podemos fazer a an√°lise agora para o p√¥ster de cada filme (o que aparece no IMDB): Veja que os diferentes tons de azul se repetem no p√¥ster do Batman vs Superman. J√° o p√¥ster do Capit√£o Am√©rica √© bem cinzento, com metade da paleta representando tons de cinza. Fica ent√£o o desafio de repetir a an√°lise para outros filmes e compartilhar o resultado com a gente. Fa√ßa a sua! "],
["4-8-minimos-quadrados-com-restricoes-lineares.html", "4.8 M√≠nimos quadrados com restri√ß√µes lineares", " 4.8 M√≠nimos quadrados com restri√ß√µes lineares A caracter√≠stica mais importante de um modelo estat√≠stico √© a sua flexibilidade. Esse termo pode ser entendido de v√°rias formas, mas neste texto vamos considerar que um modelo √© flex√≠vel se ele explica de forma coerente uma ampla gama de fen√¥menos reais. Pensando assim, a regress√£o linear pode ser considerada um modelo flex√≠vel, j√° que muitas rela√ß√µes funcionais cotidianas s√£o do tipo \\(y = \\beta x\\). √â justamente por causa dessa flexibilidade que a boa e velha regress√£o de m√≠nimos quadrados √© t√£o usada, at√© mesmo aonde n√£o deveria. O seu uso √© t√£o indiscriminado que uma vez, em aula, um professor extraordinariamente admir√°vel me disse que ‚Äú90% dos problemas do mundo podem ser resolvidos com uma regress√£o linear‚Äù. Sendo bastante honesto, √© prov√°vel que o meu professor esteja certo, mas este texto n√£o √© sobre isso. Este √© um post sobre o que fazer quando a regress√£o linear simples n√£o basta. No que segue, vamos discutir uma pequena (e poderosa) extens√£o do modelo de regress√£o linear simples, mas antes de prosseguir para o problema propriamente dito (e sua implementa√ß√£o em R), vamos discutir da teoria que existe por tr√°s dele. 4.8.1 Regress√£o linear √© programa√ß√£o quadr√°tica Embora seja pouco enfatizado nos bacharelados de estat√≠stica, uma regress√£o linear pode ser formulada como um problema de programa√ß√£o quadr√°tica. Entrando nos detalhes, essa afirma√ß√£o deve-se a dois fatos: Existe uma teoria, que chama-se programa√ß√£o quadr√°tica, que soluciona problemas da forma \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big),\\] onde \\(x \\in \\mathbb{R}^p\\) e \\(Q\\) e \\(c\\) tem dimens√µes que fazem a conta acima ter sentido. A teoria ocupa-se desenvolvendo algoritmos exatos e aproximados para obter solu√ß√µes desses problemas, inclusive com generaliza√ß√µes: \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }Ax \\geq 0.\\] Uma regress√£o linear consiste em resolver \\[\\min_\\beta (Y - \\beta X)&#39;(Y-\\beta X),\\] que, com um pouco de √°lgebra, √© equivalente √† \\[ \\min_\\beta (-2Y&#39;X\\beta + \\beta&#39;X&#39;X\\beta).\\] Logo, tomando \\(Q = 2X&#39;X\\) e \\(c = \\frac{1}{2}X&#39;Y\\) tem-se que esse √© um problema de programa√ß√£o quadr√°tica, que por sua vez √© um problema convexo e que, segundo a teoria, tem uma √∫nica solu√ß√£o no ponto \\(\\beta = (X&#39;X)^{-1}X&#39;Y\\). 4.8.2 Uma regress√£o linear simples mais flex√≠vel Talvez o jeito mais simples de flexibilizar uma regress√£o linear no sentido mencionado no come√ßo desse texto √© restringir os seus par√¢metros. Em muitos contextos, esse √© o √∫nico jeito de colocar conhecimentos pr√©vios na modelagem2. Um caso bastante emblem√°tico aparece nas curvas de cr√©dito divulgadas pela ANBIMA3. L√°, ajusta-se um conjunto de curvas que depende de 6 par√¢metros e cada curva representa uma classifica√ß√£o de risco (que nem aquela em que o Brasil pode tomar downgrade4). Como os n√≠veis de risco est√£o ordenados, √© natural exigir que tamb√©m exista uma ordena√ß√£o entre as curvas. Sem entrar em detalhes, a ideia pode ser expressa assim: \\[\\beta_{AAA} &lt; \\beta_{AA} &lt; \\beta_{A} &lt; \\beta_{BBB} &lt; ...\\] O que √© que isso tem a ver com programa√ß√£o quadr√°tica? A resposta √© que a inequa√ß√£o acima pode ser escrita como \\(A\\beta \\geq 0\\), de tal forma j√° existe uma teoria para resolver uma regress√£o linear simples com restri√ß√µes desse tipo! Basta que ela seja vista como um problema de programa√ß√£o quadr√°tica. 4.8.3 O pacote quadprog Existe um pacote de R para quase tudo, ent√£o, como n√£o poderia deixar de ser, existe um pacote em R para resolver problemas do tipo: \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }Ax \\geq 0.\\] Para ilustrar o seu uso, vamos considerar um exemplo. Vamos simular um conjunto de dados em que \\(\\beta_5 = 0.31, \\beta_4 = 0.43, \\beta_3 = 1.31, \\beta_2 = 2.19, \\beta_1 = 2.29\\) s√£o os valores reais que precisamos estimar, considere que vale \\[Y \\approx \\beta_1X_1 + \\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5\\] e que o erro de regress√£o tem distribui√ß√£o normal. Se soubermos antecipadamente que valem as seguintes afirma√ß√µes \\[ \\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5 &gt; 0 \\text{ e } \\beta_1 &gt; \\beta_2 &gt; \\beta_3 &gt; \\beta_4 &gt; \\beta_5,\\] a minimiza√ß√£o de \\((Y-\\beta X)&#39;(Y-\\beta X)\\) pode ser resolvida usando a fun√ß√£o solve.QP. Tudo que precisamos fazer √© escrever o conjunto de inequa√ß√µes na forma \\(A\\beta \\geq 0\\). Mas isso √© bem f√°cil! Basta notar que as restri√ß√µes s√£o equivalentes √† \\[ \\left(\\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\\\ \\end{array}\\right) \\times \\left(\\begin{array}{c}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\end{array}\\right) \\geq 0.\\] Dessa forma, o problema est√° prontinho pra passar no moedor de carne, com uma √∫ltima ressalva. O problema resolvido no solve.QP √© \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }A&#39;x \\geq 0,\\] ent√£o vamos ter que tomar o cuidado de passar as nossas restri√ß√µes atrav√©s do transposto da matriz que obtivemos acima. Isso resultar√° na matriz \\(A\\). Para checar como valeu a pena todo esse esfor√ßo, d√° uma olhada na diferen√ßa entre as estimativas! Os pontinhos vermelhos s√£o as estimativas do modelo irrestrito, enquanto as barras s√£o as estimativas do modelo com restri√ß√µes. 4.8.4 Conclus√µes Regress√£o linear simples √© um problema de programa√ß√£o quadr√°tica. Algumas restri√ß√µes interessantes podem ser escritas na forma \\(B\\beta \\geq 0\\). Programa√ß√£o quadr√°tica resolve regress√£o linear simples com restri√ß√µes lineares. Se em algum dia voc√™ topar com um bicho desses, o quadprog pode resolver o problema pra voc√™. A menos que voc√™ seja uma pessoa razo√°vel bayesiano.‚Ü© http://www.anbima.com.br/data/files/05/43/3E/84/E12D7510E7FCF875262C16A8/metodologia-curvas_20credito_20131104_v2_1_.pdf‚Ü© http://economia.estadao.com.br/noticias/geral,agravamento-da-crise-politica-eleva-risco-de-rebaixamento-do-brasil-diz-sep,70001824274‚Ü© "],
["4-9-filtros-de-bloom-em-r.html", "4.9 Filtros de Bloom em R", " 4.9 Filtros de Bloom em R Filtro de Bloom √© um algoritmo muito interessante para testar se um elemento pertence a um conjunto. Ele √© considerado uma estrutura de dados probabil√≠stica, ou seja, o resultado pode n√£o estar correto com alguma probabilidade. Especificamente para o filtro de bloom, existe a possibilidade de falsos positivos mas n√£o de falsos negativos: o algoritmo pode dizer que o elemento pertence ao conjunto, mas na verdade n√£o pertencer, mas nunca dir√° que ele n√£o pertence sendo que ele pertence. Bloom Filters s√£o √∫teis em diversas situa√ß√µes, geralmente relacionadas ao ganho de velocidade e de espa√ßo que o seu uso pode trazer. Muitos sistemas de bancos de dados usam bloom filters para reduzir o n√∫mero de buscas no disco (ex. Cassandra). O Medium usa para evitar recomendar uma pa«µina que voc√™ j√° leu. Recentemente, encontraram at√© aplica√ß√µes para bloom filters em machine learning. Nesse post vamos implementar uma vers√£o simplificada, nada otimizada dos filtros de Bloom em R. Mas antes disso, vale a pena ler o verbete da Wikipedia sobre o assunto. Essencialmente, um filtro de bloom √© um vetor de TRUEs e FALSES de tamanho \\(m\\). Inicializamos esse vetor com FALSES. Em seguida para cada elemento do conjunto que voc√™ deseja representar pelo filtro, repetimos o seguinte processo: Hasheamos o elemento usando \\(k\\) fun√ß√µes de hash diferentes. Cada uma dessas fun√ß√µes indicar√° um elemento do vetor que deve ser marcado como TRUE. Armazenamos ent√£o esse vetor de bits. S√£o os valores de \\(m\\) e de \\(k\\) que controlam a probabilidade de falsos positivos. Veja como podemos criar uma fun√ß√£o em R para fazer essas opera√ß√µes. Essa fun√ß√£o inicializa o vetor de bits de tamanho \\(m\\) com FALSES e em seguida, para cada uma das \\(k\\) fun√ß√µes de hash (no caso apenas variamos a semente do hash MurMur32) e para cada elemento de x calculamos o elemento do vetor vec que deve se tornar TRUE. No final, ela retorna o vetor vec, onde armazenamos como atributos os par√¢metros usados na sua constru√ß√£o. library(digest) library(magrittr) criar_vetor_de_bits &lt;- function(x, m = 1000, k = 7){ vec &lt;- rep(FALSE, m) for (i in 1:k) { for (j in 1:length(x)) { hash &lt;- digest(x[j], algo = &quot;murmur32&quot;, serialize = FALSE, seed = i) %&gt;% Rmpfr::mpfr(base = 16) %% m %&gt;% as.integer() vec[hash + 1] &lt;- TRUE } } # armazenamos os par√¢metros usados na constru√ß√£o attributes(vec) &lt;- list(m = m, k= k) return(vec) } Dado um conjunto de strings, podemos criar o vetor de bits que o representa. vect &lt;- criar_vetor_de_bits(c(&quot;eu&quot;, &quot;pertenco&quot;, &quot;ao&quot;, &quot;conjunto&quot;, &quot;de&quot;, &quot;strings&quot;), m = 1000, k = 7) Agora vamos definir uma fun√ß√£o que verifica se uma string pertence ao conjunto, dada apenas a representa√ß√£o dos bits desse conjunto. Hasheamos o elemento que desejamos verificar a presen√ßa no conjunto com a primeira fun√ß√£o de hash. Se ela indicar um elemento do vetor que j√° est√° marcado com TRUE ent√£o continuamos, se n√£o, retorna FALSE indicando que o elemento n√£o pertence ao conjunto. Continuamos at√© acabarem as fun√ß√µes de hash ou at√© 1 FALSE ter sido retornado. verificar_presenca &lt;- function(x, vetor_de_bits){ k &lt;- attr(vetor_de_bits, &quot;k&quot;) m &lt;- attr(vetor_de_bits, &quot;m&quot;) for(i in 1:k){ hash &lt;- digest(x, algo = &quot;murmur32&quot;, serialize = FALSE, seed = i) %&gt;% Rmpfr::mpfr(base = 16) %% m %&gt;% as.integer() if(!vetor_de_bits[hash + 1]) { return(FALSE) } } return(TRUE) } verificar_presenca(&quot;nao&quot;, vect) verificar_presenca(&quot;eu&quot;, vect) verificar_presenca(&quot;abc&quot;, vect) Com m = 1000 e k = 7 n√£o consegui encontrar nenhum falso positivo, mas basta diminuir o tamanho de m e de k que encontraremos. No verbete da Wikipedia a conta est√° bonitinha mas de fato a probabilidade de falsos positivos pode ser estimada em fun√ß√£o dos par√¢metros \\(k\\) e \\(m\\) e \\(n\\) (tamanho do conjunto representado) √© dada por \\[(1 - e^{-kn/m})^k\\] No caso apresentado, a probabilidade de colis√£o √© de 1.991256e-10. "],
["4-10-modelando-a-variancia-da-normal.html", "4.10 Modelando a vari√¢ncia da normal", " 4.10 Modelando a vari√¢ncia da normal Verificar as suposi√ß√µes dos modelos √© muito importante quando fazemos infer√™ncia estat√≠stica. Em particular, a suposi√ß√£o de homocedasticidade5 dos modelos de regress√£o linear √© especialmente importante, pois modifica o c√°lculo de erros padr√£o, intervalos de confian√ßa e valores-p. Neste post, vou mostrar tr√™s pacotes do R que ajustam modelos da forma \\[ Y_i = \\beta_0 + \\sum_{k=1}^p\\beta_kx_{ik} + \\epsilon_i, \\ i = 1,\\ldots,n\\] \\[ \\epsilon_{i} \\sim \\textrm{N}(0,\\sigma_i), \\ i = 1,\\ldots,n \\ \\textrm{independentes, com }\\sigma_i^2 = \\alpha x_i^2. \\] Al√©m de mostrar como se faz, tamb√©m vou ilustrar o desempenho dos pacotes em um exemplo simulado. O modelo que gerar√° os dados do exemplo ter√° a seguinte forma funcional \\[ Y_i = \\beta x_i + \\epsilon_i, \\ i = 1,...n \\] \\[ \\epsilon_i \\sim N(0, \\sigma_i)\\text{ independentes, com }\\sigma_i = \\alpha\\sqrt{|x_i|},\\] e os par√¢metros do modelo ser√£o os valores \\(\\beta = 1\\) e \\(\\alpha = 4\\). A heterocedasticidade faz com que os pontos desenhem um cone ao redor da reta de regress√£o. 4.10.1 Usando o pacote gamlss Quando se ajusta um GAMLSS, voc√™ pode modelar os par√¢metros de loca√ß√£o, escala e curtose ao mesmo tempo em que escolhe a distribui√ß√£o dos dados dentre uma grande gama de op√ß√µes. Escolhendo a distribui√ß√£o normal e modelando apenas os par√¢metros de loca√ß√£o e escala, o GAMLSS ajusta modelos lineares normais com heterocedasticidade. No c√≥digo abaixo, o par√¢metro formula = Y ~ X-1 indica que a fun√ß√£o de regress√£o ser√° constitu√≠da por um preditor linear em X sem intercepto. J√° o par√¢metro sigma.formula = ~X2-1 indica que o desvio padr√£o ser√° modelado por um preditor linear em X2 (ou raiz de X), tamb√©m sem intercepto. FALSE GAMLSS-RS iteration 1: Global Deviance = 17872.29 FALSE GAMLSS-RS iteration 2: Global Deviance = 17870.67 FALSE GAMLSS-RS iteration 3: Global Deviance = 17870.67 Conforme descrito no sum√°rio abaixo, a estimativa de alfa est√° muito abaixo do valor simulado. FALSE ****************************************************************** FALSE Family: c(&quot;NO&quot;, &quot;Normal&quot;) FALSE FALSE Call: gamlss::gamlss(formula = Y ~ X - 1, sigma.formula = ~X2 - FALSE 1, family = NO(), data = dataset) FALSE FALSE Fitting method: RS() FALSE FALSE ------------------------------------------------------------------ FALSE Mu link function: identity FALSE Mu Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X 0.996942 0.005131 194.3 &lt;2e-16 *** FALSE --- FALSE Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 FALSE FALSE ------------------------------------------------------------------ FALSE Sigma link function: log FALSE Sigma Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X2 0.1791449 0.0009606 186.5 &lt;2e-16 *** FALSE --- FALSE Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 FALSE FALSE ------------------------------------------------------------------ FALSE No. of observations in the fit: 1000 FALSE Degrees of Freedom for the fit: 2 FALSE Residual Deg. of Freedom: 998 FALSE at cycle: 3 FALSE FALSE Global Deviance: 17870.67 FALSE AIC: 17874.67 FALSE SBC: 17884.49 FALSE ****************************************************************** 4.10.2 Usando o pacote dglm Quando se ajusta um Modelo Linear Generalizado Duplo (MLGD em portugu√™s e DGLM em ingl√™s), voc√™ tem uma flexibilidade parecida com a de um GAMLSS. Entretanto, voc√™ n√£o pode definir um modelo para a curtose e a classe de distribui√ß√µes dispon√≠vel √© bem menor. O c√≥digo abaixo, similar ao utilizado para ajustar o GAMLSS, ajusta um DGLM aos dados simulados. Novamente, verifica-se que o alfa estimado est√° muito distante do verdadeiro alfa. FALSE FALSE Call: dglm(formula = Y ~ X - 1, dformula = ~X2 - 1, family = gaussian, FALSE data = dataset, method = &quot;reml&quot;) FALSE FALSE Mean Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X 0.9969432 0.008981392 111.001 0 FALSE (Dispersion Parameters for gaussian family estimated as below ) FALSE FALSE Scaled Null Deviance: 27197.48 on 1000 degrees of freedom FALSE Scaled Residual Deviance: 3090.08 on 999 degrees of freedom FALSE FALSE Dispersion Coefficients: FALSE Estimate Std. Error z value Pr(&gt;|z|) FALSE X2 0.3577322 0.001166004 306.8019 0 FALSE (Dispersion parameter for Gamma family taken to be 2 ) FALSE FALSE Scaled Null Deviance: 1628.301 on 1000 degrees of freedom FALSE Scaled Residual Deviance: 6526.59 on 999 degrees of freedom FALSE FALSE Minus Twice the Log-Likelihood: 17870.76 FALSE Number of Alternating Iterations: 18 4.10.3 Usando o pacote rstan Stan √© uma linguagem de programa√ß√£o voltada para descrever e manipular objetos probabil√≠sticos, como por exemplo vari√°veis aleat√≥rias, processos estoc√°sticos, distribui√ß√µes de probabilidades etc. Essa linguagem foi projetada para tornar intuitivo e simples o ajuste de modelos estat√≠sticos. Em particular, a forma de descrever modelos bayesianos √© bem c√¥moda. O stan possui v√°rias interfaces para R. A mais b√°sica √© o rstan, que ser√° utilizada aqui. A principal fun√ß√£o desse pacote √© a fun√ß√£o rstan, que possui dois par√¢metros b√°sicos: um par√¢metro model_code =, que recebe um c√≥digo que descreve o modelo na linguagem stan. um par√¢metro data =, que recebe uma lista contendo os inputs do modelo, tais como dados coletados, par√¢metros de distribui√ß√µes a priori, etc. Embora esse seja o m√≠nimo que a fun√ß√£o precisa, tamb√©m podemos passar outras componentes. O par√¢metro verbose = FALSE faz com que a fun√ß√£o n√£o imprima nada enquanto roda e o par√¢metro control = list(...) passa uma lista de op√ß√µes de controle para o algoritmo de ajuste. O retorno da fun√ß√£o stan() √© um objeto do tipo stanfit, que pode ser sumarizado da mesma forma que outros modelos em R, utilizando a fun√ß√£o summary() e a fun√ß√£o plot(). O c√≥digo abaixo ilustra a aplica√ß√£o da fun√ß√£o stan() ao nosso exemplo. A figura abaixo descreve os intervalos de credibilidade obtidos para cada par√¢metro do modelo. O ponto central de cada intervalo representa as estimativas pontuais dos par√¢metros. Como se nota, as estimativas do modelo utilizando stan est√£o bem pr√≥ximas dos valores verdadeiros. Uma regress√£o linear √© homoced√°stica quando a variabilidade dos erros n√£o depende das covari√°veis do modelo.‚Ü© "],
["5-transformacao.html", "Cap√≠tulo 5 Transforma√ß√£o ", " Cap√≠tulo 5 Transforma√ß√£o "],
["5-1-arrumando-banco-de-dados-o-pacote-janitor.html", "5.1 Arrumando banco de dados: o pacote janitor", " 5.1 Arrumando banco de dados: o pacote janitor No primeiro post sobre arruma√ß√£o de base de dados, a gente viu como usar as fun√ß√µes do stringr para arrumar o nome das vari√°veis. Seguindo a dica do Julio, o quebrador de captchas, vamos falar do pacote janitor, que traz algumas fun√ß√µes para dar aquele trato nas BDs. Antes de mais nada, instale e carregue o pacote: 5.1.1 Arrumando o nome das vari√°veis Assim como no post passado, utilizaremos a base com informa√ß√µes de pacientes com arritmia card√≠aca, cujas vari√°veis selecionadas foram: FALSE [1] &quot;ID&quot; &quot;Sexo&quot; &quot;Nascimento&quot; FALSE [4] &quot;Idade&quot; &quot;Inclus√£o&quot; &quot;Cor&quot; FALSE [7] &quot;Peso&quot; &quot;Altura&quot; &quot;cintura&quot; FALSE [10] &quot;IMC&quot; &quot;Superf√≠cie corporal&quot; &quot;Tabagismo&quot; FALSE [13] &quot;cg.tabag (cig/dia)&quot; &quot;Alcool (dose/semana)&quot; &quot;Drogas il√≠citas&quot; FALSE [16] &quot;Cafe√≠na/dia&quot; &quot;Refrig/dia&quot; &quot;Sedentario&quot; FALSE [19] &quot;ativ. Fisica&quot; Os nomes t√™m letras mai√∫sculas, acentos, par√™nteses, pontos e barras, o que atrapalha na hora da programa√ß√£o. Para resolver esse problema, usamos a fun√ß√£o clean_names(). FALSE [1] &quot;id&quot; &quot;sexo&quot; &quot;nascimento&quot; FALSE [4] &quot;idade&quot; &quot;inclusao&quot; &quot;cor&quot; FALSE [7] &quot;peso&quot; &quot;altura&quot; &quot;cintura&quot; FALSE [10] &quot;imc&quot; &quot;superficie_corporal&quot; &quot;tabagismo&quot; FALSE [13] &quot;cg_tabag_cig_dia&quot; &quot;alcool_dose_semana&quot; &quot;drogas_ilicitas&quot; FALSE [16] &quot;cafeina_dia&quot; &quot;refrig_dia&quot; &quot;sedentario&quot; FALSE [19] &quot;ativ_fisica&quot; Veja que a fun√ß√£o removeu os par√™nteses, pontos e barras e substituiu os espa√ßos por _. No entanto, ela n√£o remove os acentos. Assim, podemos adicionar mais uma linha ao pipeline para chegar onde queremos. FALSE [1] &quot;id&quot; &quot;sexo&quot; &quot;nascimento&quot; FALSE [4] &quot;idade&quot; &quot;inclusao&quot; &quot;cor&quot; FALSE [7] &quot;peso&quot; &quot;altura&quot; &quot;cintura&quot; FALSE [10] &quot;imc&quot; &quot;superficie_corporal&quot; &quot;tabagismo&quot; FALSE [13] &quot;cg_tabag_cig_dia&quot; &quot;alcool_dose_semana&quot; &quot;drogas_ilicitas&quot; FALSE [16] &quot;cafeina_dia&quot; &quot;refrig_dia&quot; &quot;sedentario&quot; FALSE [19] &quot;ativ_fisica&quot; E para substituir na base. 5.1.2 Removendo linhas e colunas vazias Esse banco de dados tamb√©m tinha outro problema: linhas vazias. Na verdade, elas n√£o eram completamente vazias, pois havia algumas informa√ß√µes de identifica√ß√£o do paciente, mas nenhuma outra vari√°vel tinha sido computada. FALSE # A tibble: 1 x 19 FALSE id sexo nascimento idade inclusao cor peso FALSE &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; FALSE 1 3 &lt;NA&gt; NA NA NA &lt;NA&gt; NA FALSE # ‚Ä¶ with 12 more variables: altura &lt;dbl&gt;, cintura &lt;chr&gt;, imc &lt;dbl&gt;, FALSE # superficie_corporal &lt;chr&gt;, tabagismo &lt;chr&gt;, cg_tabag_cig_dia &lt;dbl&gt;, FALSE # alcool_dose_semana &lt;dbl&gt;, drogas_ilicitas &lt;chr&gt;, cafeina_dia &lt;dbl&gt;, FALSE # refrig_dia &lt;dbl&gt;, sedentario &lt;chr&gt;, ativ_fisica &lt;chr&gt; Essa foi a solu√ß√£o que eu pensei para resolver o problema utilizando a fun√ß√£o remove_empty_row(). FALSE # A tibble: 4 x 19 FALSE id sexo nascimento idade inclusao cor peso FALSE &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; FALSE 1 1 F 1964-01-31 00:00:00 41 2006-02-17 00:00:00 bran‚Ä¶ 75 FALSE 2 2 M 1959-01-28 00:00:00 45 2005-11-29 00:00:00 negra 71 FALSE 3 4 M 1957-09-13 00:00:00 50 2008-02-13 00:00:00 NT 80 FALSE 4 5 F 1938-02-06 00:00:00 71 2009-06-25 00:00:00 parda 56 FALSE # ‚Ä¶ with 12 more variables: altura &lt;dbl&gt;, cintura &lt;chr&gt;, imc &lt;dbl&gt;, FALSE # superficie_corporal &lt;chr&gt;, tabagismo &lt;chr&gt;, cg_tabag_cig_dia &lt;dbl&gt;, FALSE # alcool_dose_semana &lt;dbl&gt;, drogas_ilicitas &lt;chr&gt;, cafeina_dia &lt;dbl&gt;, FALSE # refrig_dia &lt;dbl&gt;, sedentario &lt;chr&gt;, ativ_fisica &lt;chr&gt; Eu precisei converter para data.frame primeiro porque n√£o √© poss√≠vel definir os nomes das linhas de uma tibble. Se a linha estivesse completamente vazia, bastaria usar diretamente a fun√ß√£o remove_empty_rows(). Equivalentemente para colunas, existe a fun√ß√£o remove_empty_cols(). 5.1.3 Identificando linhas duplicadas O pacote janitor possui uma fun√ß√£o para identificar entradas duplicadas numa base de dados: get_dupes(). Vamos criar uma base gen√©rica para test√°-la. FALSE # A tibble: 16 x 4 FALSE nome sobrenome dupe_count variavel_importante FALSE &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; FALSE 1 Athos Damiani 2 0.305 FALSE 2 Athos Damiani 2 -0.00796 FALSE 3 Athos Falbel 2 0.198 FALSE 4 Athos Falbel 2 -0.0443 FALSE 5 Athos Trecenti 2 -1.24 FALSE 6 Athos Trecenti 2 -0.729 FALSE 7 Daniel Damiani 2 0.335 FALSE 8 Daniel Damiani 2 3.07 FALSE 9 Fernando Trecenti 2 -0.190 FALSE 10 Fernando Trecenti 2 -0.868 FALSE 11 Julio Trecenti 4 -0.188 FALSE 12 Julio Trecenti 4 -1.39 FALSE 13 Julio Trecenti 4 0.528 FALSE 14 Julio Trecenti 4 1.06 FALSE 15 William Trecenti 2 -0.786 FALSE 16 William Trecenti 2 -0.505 Todas as linhas na tibble resultante representam uma combina√ß√£o de nome-sobrenome repetida. 5.1.4 Outras fun√ß√µes Por fim, o janitor tamb√©m tem fun√ß√µes equivalentes √† table() para produzir tabelas de frequ√™ncia: tabyl() - similar a table(), mas pipe-√°vel e com mais recursos. crosstab() - para tabelas de conting√™ncia. adorn_totals() - acrescenta o total das linhas ou colunas. adorn_crosstab() - deixa tabelas de conting√™ncia mais bonitas. FALSE cyl n percent FALSE 4 11 0.34375 FALSE 6 7 0.21875 FALSE 8 14 0.43750 FALSE cyl n percent FALSE 4 11 0.34375 FALSE 6 7 0.21875 FALSE 8 14 0.43750 FALSE Total 32 1.00000 FALSE cyl 0 1 FALSE 4 3 8 FALSE 6 4 3 FALSE 8 12 2 FALSE cyl 0 1 FALSE 1 4 27.3% (3) 72.7% (8) FALSE 2 6 57.1% (4) 42.9% (3) FALSE 3 8 85.7% (12) 14.3% (2) √â isso! Espero que essas dicas e o pacote janitor ajudem a agilizar as suas an√°lises :) "],
["5-2-skimr-estatisticas-basicas-com.html", "5.2 Skimr: estat√≠sticas b√°sicas com ‚ù§Ô∏è", " 5.2 Skimr: estat√≠sticas b√°sicas com ‚ù§Ô∏è Entre os dias 25 e 27 de maio aconteceu a ROpenSci Unconf 2017. O encontro reuniu v√°rios pop stars da comunidade R como Hadley Wickham, Joe Cheng (criador do shiny), Jeroen Ooms (criador do OpenCPU e autor de v√°rios pacotes bacanas), Jenny Bryan (autora de v√°rios pacotes bacanas como googlesheets), v√°rias pessoas do #R-Ladies e muito mais. Uma coisa muito legal dessa confer√™ncia √© que ela funcionou como uma hackathon. Foi criada uma nova organiza√ß√£o no github chamada ROpenSci Labs, e os presentes simplesmente come√ßaram a subir pacotes fant√°sticos l√° dentro. Recomendo muito dar uma olhada. Dentre os pacotes que olhei, o que mais me chamou aten√ß√£o foi o skimr e por isso estou fazendo esse post! O prop√≥sito do skimr √© simples: fazer algumas estat√≠sticas b√°sicas univariadas de uma base de dados. O skimr ainda n√£o est√° no CRAN, ent√£o para instalar recomendamos utilizar o devtools para instalar direto do GitHub, conforme c√≥digo abaixo. Note que tamb√©m ser√° necess√°rio instalar o pacote colformat do Hadley. A fun√ß√£o skim() calcula estat√≠sticas b√°sicas das vari√°veis e imprime no seu console. Note que a fun√ß√£o separa estat√≠sticas para vari√°veis num√©ricas ou fatores. FALSE Skim summary statistics FALSE n obs: 150 FALSE n variables: 5 FALSE FALSE ‚îÄ‚îÄ Variable type:factor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FALSE variable missing complete n n_unique top_counts FALSE Species 0 150 150 3 set: 50, ver: 50, vir: 50, NA: 0 FALSE ordered FALSE FALSE FALSE FALSE ‚îÄ‚îÄ Variable type:numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FALSE variable missing complete n mean sd p0 p25 p50 p75 p100 FALSE Petal.Length 0 150 150 3.76 1.77 1 1.6 4.35 5.1 6.9 FALSE Petal.Width 0 150 150 1.2 0.76 0.1 0.3 1.3 1.8 2.5 FALSE Sepal.Length 0 150 150 5.84 0.83 4.3 5.1 5.8 6.4 7.9 FALSE Sepal.Width 0 150 150 3.06 0.44 2 2.8 3 3.3 4.4 FALSE hist FALSE ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ FALSE ‚ñá‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ FALSE ‚ñÇ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ FALSE ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ E tem mais! O mais legal do skimr √© que ele usa a fun√ß√£o colformat::spark_bar() para desenhar histogramas direto no seu console! Tabela 2.3: HISTOGRAMA NA TABELA PORQUE SIM! variable type stat level value formatted Sepal.Length numeric hist .all NA ‚ñÇ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ Sepal.Width numeric hist .all NA ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ Petal.Length numeric hist .all NA ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ Petal.Width numeric hist .all NA ‚ñá‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ O skimr tamb√©m possui padr√µes de estat√≠sticas b√°sicas para cada tipo de vari√°vel. Voc√™ pode checar esses tipos com show_skimmers(): Tabela 2.4: Estat√≠sticas b√°sicas para cada tipo de vari√°vel. tipo stats AsIs missing, complete, n, n_unique, min_length, max_length character missing, complete, n, min, max, empty, n_unique complex missing, complete, n date missing, complete, n, min, max, median, n_unique Date missing, complete, n, min, max, median, n_unique difftime missing, complete, n, min, max, median, n_unique factor missing, complete, n, n_unique, top_counts, ordered integer missing, complete, n, mean, sd, p0, p25, p50, p75, p100, hist list missing, complete, n, n_unique, min_length, median_length, max_length logical missing, complete, n, mean, count numeric missing, complete, n, mean, sd, p0, p25, p50, p75, p100, hist POSIXct missing, complete, n, min, max, median, n_unique ts missing, complete, n, start, end, frequency, deltat, mean, sd, min, max, median, line_graph 5.2.1 Criando suas pr√≥prias fun√ß√µes Voc√™ tamb√©m pode usar fun√ß√µes pr√≥prias com o skimr. Por exemplo, digamos que voc√™ queira calcular o coeficiente de varia√ß√£o. Primeiro, adicione sua fun√ß√£o dentro de uma lista: e depois aplique a fun√ß√£o skim_with(): E pronto! Agora voc√™ pode rodar skim() novamente: Tabela 2.5: Histograma e coeficiente de varia√ß√£o. variable type stat level value formatted Sepal.Length numeric hist .all NA ‚ñÇ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ Sepal.Length numeric cv .all 0.1417113 0.14 Sepal.Width numeric hist .all NA ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ Sepal.Width numeric cv .all 0.1425642 0.14 Petal.Length numeric hist .all NA ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ Petal.Length numeric cv .all 0.4697441 0.47 Petal.Width numeric hist .all NA ‚ñá‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ Petal.Width numeric cv .all 0.6355511 0.64 Para retornar ao skim() padr√£o, rode skim_with_defaults(). 5.2.2 Wrap up Instale usando devtools::install_github() Rode a fun√ß√£o skim(). Use dplyr::filter() para filtrar as estat√≠sticas de interesse. Voc√™ pode adicionar suas pr√≥prias estat√≠sticas com skim_with(). Acompanhe a evolu√ß√£o do skimr nesta p√°gina. O pacote ainda vai evoluir muito e n√£o duvido nada que seja um bom candidado a entrar no tidyverse. O que voc√™s acham? Escrevam nos coment√°rios! √â isso. Happy coding ;) "],
["5-3-pacotes-miojo-como-fazer-um-pacote-no-r-em-3-minutos.html", "5.3 Pacotes miojo: como fazer um pacote no R em 3 minutos", " 5.3 Pacotes miojo: como fazer um pacote no R em 3 minutos Autor: Julio Dificuldade m√©dia program Nesse post vou mostrar como fazer um pacote em R muito, muito r√°pido. Tirei v√°rias coisas que costumo fazer nos pacotes, com dor no cora√ß√£o, tudo pela velocidade, mantendo s√≥ o essencial. Duas restri√ß√µes que usei s√£o O pacote precisa ficar dispon√≠vel no GitHub. O pacote precisa ter pelo menos uma fun√ß√£o. Essa √© a solu√ß√£o que eu acho mais segura e r√°pida. Voc√™ tamb√©m pode usar o pr√≥prio RStudio para criar pacotes ou clonar coisas do github, mas isso pode dar alguns bugs. 5.3.1 Passo 1: Crie um pacote no R Rode ‚úî Setting active project to &#39;/home/jtrecenti/Documents/pacote&#39; ‚úî Creating &#39;R/&#39; ‚úî Creating &#39;man/&#39; ‚úî Writing &#39;DESCRIPTION&#39; ‚úî Writing &#39;NAMESPACE&#39; ‚úî Writing &#39;pacote.Rproj&#39; ‚úî Adding &#39;.Rproj.user&#39; to &#39;.gitignore&#39; ‚úî Adding &#39;^pacote\\\\.Rproj$&#39;, &#39;^\\\\.Rproj\\\\.user$&#39; to &#39;.Rbuildignore&#39; ‚úî Opening new project &#39;pacote&#39; in RStudio Pronto! pacote feito. 5.3.2 Passo 2: Adicione git e github no seu pacote FALSE ‚úî Setting active project to &#39;/home/jtrecenti/Documents/livro-blog/livro&#39; ‚úî Setting active project to &#39;/home/jtrecenti/Documents/pacote&#39; Criar reposit√≥rio no GitHub: ‚óè Check title and description Name: pacote Description: What the Package Does (One Line, Title Case) Are title and description ok? 1: No 2: No way 3: Yup 5.3.3 Par√™nteses: GITHUB_PAT Se voc√™ n√£o tiver um GITHUB_PAT, tem um passo adicional: ‚úî Opening URL https://github.com/settings/tokens/new?scopes=repo,gist&amp;description=R:GITHUB_PAT ‚óè Call `edit_r_environ()` to open &#39;.Renviron&#39; and store your PAT with a line like: GITHUB_PAT=xxxyyyzzz ‚óè Make sure &#39;.Renviron&#39; ends with a newline! e depois, rodando Adicione GITHUB_PAT=95d864ed372140f8d72f895d864ed372140f8d72f8 Salve e restarte sua sess√£o. 5.3.4 Passo 3: Adicione coisas de interesse README: &gt; usethis::use_readme_md() ‚úî Writing &#39;README.md&#39; ‚óè Modify &#39;README.md&#39; Pipe (%&gt;%) ‚úî Setting Roxygen field in DESCRIPTION to &#39;list(markdown = TRUE)&#39; ‚úî Setting RoxygenNote field in DESCRIPTION to &#39;6.1.1&#39; ‚óè Run `devtools::document()` ‚úî Adding &#39;magrittr&#39; to Imports field in DESCRIPTION ‚úî Writing &#39;R/utils-pipe.R&#39; ‚óè Run `devtools::document()` 5.3.5 Passo 4: Crie sua fun√ß√£o Exemplo: Crie a fun√ß√£o dentro de um arquivo com extens√£o .R na pasta R As informa√ß√µes que come√ßam com #' acima da fun√ß√£o servem para documentar. Nesse caso, a primeira linha √© o t√≠tulo a segunda linha √© a descri√ß√£o a parte que come√ßa com @param descreve o que √© o par√¢metro de entrada a parte que come√ßa com @export diz para o pacote que essa fun√ß√£o deve estar dispon√≠vel para o usu√°rio quando ele rodar library(nomeDoPacote). 5.3.6 Passo 5: document, commit e push! Rode devtools::document(). Commite suas altera√ß√µes. D√™ um push! Se n√£o saba o que √© commitar e pushar, veja o artigo do Athos sobre o uso do git e do GitHub. 5.3.7 Passo 6: Instalar o pacote em outra m√°quina Mande o nome do seu usu√°rio do GitHub e o nome do seu pacote para sua migue. Pe√ßa para ela rodar: Agora ela poder√° usar sua fun√ß√£o! # [1] 3 4 5 6 7 8 9 10 11 12 Voc√™ tamb√©m pode ver o help da fun√ß√£o com ?soma_2: FIM! 5.3.8 Conclus√µes Agora voc√™ n√£o tem desculpa para n√£o empacotar suas solu√ß√µes em R. Esse tutorial √© incompleto! Para acessar mais detalhes, veja http://r-pkgs.had.co.nz, elaborado por voc√™ sabe quem. 5.3.9 Outras pequenas dicas pr√°ticas Use sempre devtools::check() para checar se seu pacote est√° 100% bem constru√≠do. Use usethis::use_package() para usar fun√ß√µes de outros pacotes. Sempre use os :: para chamar as fun√ß√µes e nunca rode library() ou require() dentro de um pacote. Use usethis::use_mit_license(&quot;seu nome&quot;) para adicionar um arquivo LICENSE ao seu pacote. Use usethis::use_data() para adicionar dados ao seu pacote. Use usethis::use_vignette() para escrever um tutorial sobre seu pacote, igual a esse do dplyr, por exemplo. √â isso. Happy coding ;) "],
["5-4-pvec-o-laco-perfeito.html", "5.4 pvec: O la√ßo perfeito", " 5.4 pvec: O la√ßo perfeito Autor: Julio Dificuldade baixa model Quando usamos la√ßos para rodar algoritmos complexos em uma lista de inputs, podemos pensar em power-ups. Tratam-se de funcionalidades que ajudam na aplica√ß√£o dos la√ßos, tanto do ponto de vista de efici√™ncia do c√≥digo quanto do ponto de vista de efici√™ncia do trabalho do cientista de dados. Aqui na Curso-R n√≥s j√° vimos tr√™s desses power-ups: Como fazer la√ßos em paralelo. Como usar barras de progresso Como fazer tratamento de erros. Mas ser√° que tem um jeito de juntar essas tr√™s funcionalidades em apenas uma opera√ß√£o? Sim, √© claro que tem. E se algo √© poss√≠vel no R, o Caio Lente j√° fez. Trata-se da opera√ß√£o pvec(), do pacote abjutils. Para utiliz√°-la, voc√™ precisar√° instalar a vers√£o de desenvolvimento do abjutils no GitHub: Pode ser que o pvec() n√£o funcione muito bem no Windows. Isso √© algo que vamos trabalhar no futuro. 5.4.1 Como funciona O pvec() recebe duas informa√ß√µes de entrada: uma lista ou vetor de inputs e uma fun√ß√£o a ser aplicada. O pvec() funciona exatamente como um purrr::map(), mas retorna um data.frame com os outputs. Por exemplo, digamos que nosso objetivo seja aplicar a fun√ß√£o em uma lista de entradas, dada por O resultado √© dado por: # A tibble: 4 x 3 id return output &lt;int&gt; &lt;chr&gt; &lt;list&gt; 1 1 result &lt;dbl [1]&gt; 2 2 result &lt;dbl [1]&gt; 3 3 result &lt;dbl [1]&gt; 4 4 error &lt;S3: simpleError&gt; Ou seja, o resultado √© um data.frame, que tem o n√∫mero de linhas exatamente igual ao comprimento do vetor ou lista de entrada, e tr√™s colunas espec√≠ficas. id, que guarda o √≠ndice de entrada. Se a lista de entrada √© nomeada, id guarda esses nomes. return identifica se a aplica√ß√£o retornou num resultado (result) ou erro (error) output √© uma coluna-lista que cont√©m os resultados. Quando o resultado √© um erro, o erro √© capturado e colocado no elemento correspondente. Ou seja, uma caracter√≠stica do pvec() √© que ele nunca ir√° travar. Se essa opera√ß√£o travar, √© porque o computador todo travou. √â importante notar que alguns resultados nesse caso s√£o NaN. Isso ocorre pois log(-1) resulta em NaN, acompanhado de um warning. O pvec() n√£o trabalha com warnings. Outra caracter√≠stica importante do pvec() √© que ele roda em paralelo. Voc√™ pode controlar a quantidade de n√∫cleos de processamento com o par√¢metro .cores. Por padr√£o, ele usar√° o n√∫mero de n√∫cleos da sua m√°quina. Finalmente, o que n√£o poderia faltar no pvec() √© a utiliza√ß√£o de barras de progresso. Por exemplo, considerando como input O resultado √© Progress: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 100% Progress: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 100% # A tibble: 8 x 3 id return output &lt;chr&gt; &lt;chr&gt; &lt;list&gt; 1 a result &lt;dbl [1]&gt; 2 b result &lt;dbl [1]&gt; 3 c result &lt;dbl [1]&gt; 4 d error &lt;S3: simpleError&gt; 5 e result &lt;dbl [1]&gt; 6 f result &lt;dbl [1]&gt; 7 g result &lt;dbl [1]&gt; 8 h error &lt;S3: simpleError&gt; Se voc√™ quiser desligar a barra de progresso, basta adicionar .progress = FALSE. 5.4.2 O par√¢metro .flatten Esse √© o par√¢metro dos pregui√ßosos (eu que pedi para o Caio adicionar). Em muitas opera√ß√µes, o resultado que sai no output √© uma lista de data.frames ou uma lista de vetores. A op√ß√£o .flatten faz tidyr::unnest(), empilhando os resultados e colando tudo num vetor ou data.frame. O √∫nico problema √© que nesse caso n√£o √© poss√≠vel guardar os erros. Por isso, o pvec() retorna um warning: Progress: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 100% # A tibble: 6 x 2 id output &lt;chr&gt; &lt;dbl&gt; 1 a 0 2 b 0.693 3 c NaN 4 e 0.693 5 f 1.10 6 g NaN Warning message: Since &#39;.flatten = TRUE&#39;, a total of 2 errors are being ignored Note que o resultado tem 6 linhas, menor que a entrada, que tem 8 elementos. Por isso, use .flatten somente quando voc√™ tem certeza do que est√° fazendo. 5.4.3 Por tr√°s dos panos: o furrr O pvec() s√≥ funciona por conta de dois excelentes pacotes: o future, que √© um novo paradigma de computa√ß√£o em paralelo no R. o furrr, que faz todo o trabalho sujo e implementa a maioria das opera√ß√µes do purrr usando future. Se quiser estudar esses pacotes e implementar suas pr√≥prias solu√ß√µes, recomendo acessar aqui e aqui. N√£o inclu√≠ detalhes desses pacotes aqui para n√£o sair do foco. Se quiser adicionar op√ß√µes do future no pvec(), basta adicion√°-las na op√ß√£o .options. Por padr√£o, passamos furrr::future_options() nesse argumento. 5.4.3.1 Discuss√£o: o future √© o futuro do purrr? O purrr cont√©m uma s√©rie de discuss√µes no GitHub sobre a possibilidade de rodar fun√ß√µes em paralelo e com barras de progresso. Pode ser que a funcionalidade do pvec() passe a ser parte oficial no futuro. Veremos! 5.4.4 Wrap-up abjutils::pvec() √© um map() que roda em paralelo, tem barras de progresso e trata erros automaticamente. Voc√™ pode brincar com as op√ß√µes .cores, .progress e .flatten para controlar o comportamento do pvec(). Tome muito cuidado com o .flatten, pois ele pode n√£o tratar os erros da forma que voc√™ imagina! Estude future e furrr se quiser estender as funcionalidades do pvec(). √â isso pessoal. Happy coding ;) "],
["6-reflexoes.html", "Cap√≠tulo 6 Reflex√µes", " Cap√≠tulo 6 Reflex√µes "],
["referencias.html", "Refer√™ncias", " Refer√™ncias "]
]
