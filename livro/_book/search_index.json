[
["index.html", "Como faz no R Cap√≠tulo 1 Introdu√ß√£o ", " Como faz no R Curso-R 31 de January de 2019 Cap√≠tulo 1 Introdu√ß√£o "],
["1-1-objetivos.html", "1.1 Por qu√™ ler esse livro", " 1.1 Por qu√™ ler esse livro "],
["1-2-organizacao.html", "1.2 Organiza√ß√£o", " 1.2 Organiza√ß√£o "],
["2-analises.html", "Cap√≠tulo 2 An√°lises", " Cap√≠tulo 2 An√°lises "],
["3-tutoriais.html", "Cap√≠tulo 3 Tutoriais ", " Cap√≠tulo 3 Tutoriais "],
["3-1-por-que-usar-o.html", "3.1 Por que usar o %&gt;%", " 3.1 Por que usar o %&gt;% Provavelmente voc√™ j√° ouviu falar do operador pipe (%&gt;%). Muita gente acha que ele √© uma sequ√™ncia m√°gica de s√≠mbolos que muda completamente o visual do seu c√≥digo, mas na verdade ele n√£o passa de uma fun√ß√£o como outra qualquer. Vamos explorar um pouco da hist√≥ria do pipe, como ele funciona e por que utiliz√°-lo. 3.1.1 Origem O conceito de pipe existe pelo menos desde os anos 1970. De acordo com seu criador, o operador foi concebido em ‚Äúuma noite febril‚Äù e tinha o objetivo de simplificar comandos cujos resultados deveriam ser passados para outros comandos. ls | cat #&gt; Desktop #&gt; Documents #&gt; Downloads #&gt; Music #&gt; Pictures #&gt; Public #&gt; Templates #&gt; Videos Por essa descri√ß√£o j√° conseguimos ter uma ideia de onde vem o seu nome: pipe em ingl√™s significa ‚Äúcano‚Äù, referindo-se ao transporte das sa√≠das dos comandos. Em portugu√™s o termo √© traduzido como ‚Äúcanaliza√ß√£o‚Äù ou ‚Äúencadeamento‚Äù, mas no dia-a-dia √© mais comum usar o termo em ingl√™s. A partir da√≠ o pipe tem aparecido nas mais diversas aplica√ß√µes, desde HTML at√© o nosso t√£o querido R. Ele pode ter m√∫ltiplos disfarces, mas o seu objetivo √© sempre o mesmo: transportar resultados. 3.1.2 Como funciona Em R o pipe tem uma cara meio estranha (%&gt;%), mas no fundo ele n√£o passa de uma fun√ß√£o infixa, ou seja, uma fun√ß√£o que aparece entre os seus argumentos (como a + b ou a %in% b). Na verdade √© por isso mesmo que ele tem porcentagens antes e depois: porque no R uma fun√ß√£o infixa s√≥ pode ser declarada assim. Vamos come√ßar demonstrando sua funcionalidade b√°sica. Carregue o pacote magrittr e declare o pipe usando Ctrl + Shift + M. library(magrittr) `%&gt;%`(&quot;oi&quot;, print) #&gt; [1] &quot;oi&quot; N√£o ligue para os acentos graves em volta do pipe, o comando acima s√≥ serve para demonstrar que ele n√£o √© nada mais que uma fun√ß√£o; perceba que o seu primeiro argumento (&quot;oi&quot;) virou a entrada do seu segundo argumento (print). &quot;oi&quot; %&gt;% print() #&gt; [1] &quot;oi&quot; Observe agora o comando abaixo. Queremos primeiro somar 3 a uma sequ√™ncia de n√∫meros e depois divid√≠-los por 2: mais_tres &lt;- function(x) { x + 3 } sobre_dois &lt;- function(x) { x / 2 } x &lt;- 1:3 sobre_dois(mais_tres(x)) #&gt; [1] 2.0 2.5 3.0 Perceba como fica dif√≠cil de entender o que est√° acontecendo primeiro? A linha relevante come√ßa com a divis√£o por 2, depois vem a soma com 3 e, por fim, os valores de entrada. Nesse tipo de situa√ß√£o √© mais leg√≠vel usar a nota√ß√£o de composi√ß√£o de fun√ß√µes, com as fun√ß√µes sendo exibidas na ordem em que ser√£o aplicadas: \\(f \\circ g\\). Isso pode ser realizado se tivermos uma fun√ß√£o que passa o resultado do que est√° √† sua esquerda para a fun√ß√£o que est√° √† sua direita‚Ä¶ x %&gt;% mais_tres() %&gt;% sobre_dois() #&gt; [1] 2.0 2.5 3.0 No comando acima fica evidente que pegamos o objeto x, somamos 3 e dividimos por 2. Voc√™ pode j√° ter notado isso, mas a entrada (esquerda) de um pipe sempre √© passada como o primeiro argumento agumento da sua sa√≠da (direita). Isso n√£o impede que as fun√ß√µes utilizadas em uma sequ√™ncia de pipes tenham outros argumentos. mais_n &lt;- function(x, n) { x + n } x %&gt;% mais_n(4) %&gt;% sobre_dois() #&gt; [1] 2.5 3.0 3.5 3.1.3 Vantagens A grande vantagem do pipe n√£o √© s√≥ enxergar quais fun√ß√µes s√£o aplicadas primeiro, mas sim nos ajudar a programar pipelines (‚Äúencanamento‚Äù em ingl√™s) de tratamentos de dados. library(dplyr) starwars %&gt;% mutate(bmi = mass/((height/100)^2)) %&gt;% select(name, bmi, species) %&gt;% group_by(species) %&gt;% summarise(bmi = mean(bmi)) #&gt; # A tibble: 38 x 2 #&gt; species bmi #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aleena 24.0 #&gt; 2 Besalisk 26.0 #&gt; 3 Cerean 20.9 #&gt; 4 Chagrian NA #&gt; 5 Clawdite 19.5 #&gt; 6 Droid NA #&gt; 7 Dug 31.9 #&gt; 8 Ewok 25.8 #&gt; 9 Geonosian 23.9 #&gt; 10 Gungan NA #&gt; # ... with 28 more rows Acima fica extremamente claro o que est√° acontecendo em cada passo da pipeline. Partindo da base starwars, primeiro transformamos, depois selecionamos, agrupamos e resumimos; em cada linha temos uma opera√ß√£o e elas s√£o executadas em sequ√™ncia. Isso n√£o melhora s√≥ a legibilidade do c√≥digo, mas tamb√©m a sua debugabilidade! Se tivermos encontrado um bug na pipeline, basta executar linha a linha do encadeamento at√© que encontremos a linha problem√°tica. Com o pipe podemos programar de forma mais compacta, leg√≠vel e correta. Todos os exemplos acima envolvem passar a entrada do pipe como o primeiro argumento da fun√ß√£o √† direita, mas n√£o √© uma obrigatoriedade. Com um operador placeholder . podemos indicar exatamente onde deve ser colocado o valor que chega no pipe: y_menos_x &lt;- function(x, y) { y - x } x %&gt;% mais_tres() %&gt;% purrr::map2(4:6, ., y_menos_x) # [[1]] # [1] 0 # # [[2]] # [1] 0 # # [[3]] # [1] 0 3.1.4 B√¥nus Agora que voc√™ j√° sabe dos usos mais comuns do pipe, aqui est√° uma outra funcionalidade interessante: fun√ß√µes un√°rias. Se voc√™ estiver familiarizado com o pacote purrr, esse √© um jeito bastante simples de criar fun√ß√µes descart√°veis. m3_s2 &lt;- . %&gt;% mais_tres() %&gt;% sobre_dois() m3_s2(x) #&gt; [1] 2.0 2.5 3.0 Usando novamente o . definimos uma fun√ß√£o que recebe apenas um argumento com uma sequ√™ncia de aplica√ß√µes de outras fun√ß√µes. 3.1.5 Conclus√£o O pipe n√£o √© apenas algo que deve ser usado pelos f√£s do tidyverse. Ele √© uma fun√ß√£o extremamente √∫til que ajuda na legibilidade e programa√ß√£o de c√≥digo, independentemente de quais pacotes utilizamos. Se quiser saber mais sobre o mundo do pipe, leia este post do Daniel sobre o Manifesto Tidy e o nosso tutorial mais aprofundado sobre o pr√≥prio pipe. "],
["3-2-o-que-e-um-grafico-estatistico.html", "3.2 O que √© um gr√°fico estat√≠stico?", " 3.2 O que √© um gr√°fico estat√≠stico? Os gr√°ficos s√£o t√©cnicas de visualiza√ß√£o de dados amplamente utilizadas em todas as √°reas da pesquisa. A sua popularidade se deve √† maneira como elucidam informa√ß√µes que estavam escondidas nas colunas do banco de dados, sendo que muitos deles podem ser compreendidos at√© mesmo por leigos no assunto que est√° sendo discutido. Mas ser√° que podemos definir formalmente o que √© um gr√°fico estat√≠stico? Gra√ßas ao estat√≠stico norte-americano Leland Wilkinson, a resposta √© sim. Em 2005, Leland publicou o livro The Grammar of Graphics, uma fonte de princ√≠pios fundamentais para a constru√ß√£o de gr√°ficos estat√≠sticos. No livro, ele defende que um gr√°fico √© o mapeamento dos dados a partir de atributos est√©ticos (posi√ß√£o, cor, forma, tamanho) e de objetos geom√©tricos (pontos, linhas, barras, caixas). Simples assim. Al√©m de responder a pergunta levantada nesse post, os conceitos de Leland tiveram outra grande import√¢ncia para a visualiza√ß√£o de dados. Alguns anos mais tarde, o seu trabalho inspirou Hadley Wickham a criar o pacote ggplot2, que enterrou com muitas p√°s de terra as fun√ß√µes gr√°ficas do R base. Em A Layered Grammar of Graphics, Hadley sugeriu que os principais aspectos de um gr√°fico (dados, sistema de coordenadas, r√≥tulos e anota√ß√µes) podiam ser divididos em camadas, constru√≠das uma a uma na elabora√ß√£o do gr√°fico. Essa √© a ess√™ncia do ggplot2. No gr√°fico abaixo, temos informa√ß√£o de 32 carros com respeito a 4 vari√°veis: milhas por gal√£o, tonelagem, transmiss√£o e n√∫mero de cilindros. O objeto geom√©trico escolhido para representar os dados foi o ponto. As posi√ß√µes dos pontos no eixo xy mapeia a associa√ß√£o entre a tonelagem e a quantidade de milhas por gal√£o. A cor dos pontos mapeia o n√∫mero de cilindros de cada carro, enquanto a forma dos pontos mapeia o tipo de transmiss√£o. Observando o c√≥digo, fica claro como cada linha/camada representa um aspecto diferente do gr√°fico. Os conceitos criados por Leland e Hadley defendem que essa estrutura pode ser utilizada para construir e entender qualquer tipo de gr√°fico, dando a eles, dessa maneira, a sua defini√ß√£o formal. ggplot(mtcars) + geom_point(aes(x = disp, y = mpg, shape = as.factor(am), color = cyl)) + labs(x = &quot;Tonelagem&quot;, y = &quot;Milhas por gal√£o&quot;, shape = &quot;Transmiss√£o&quot;, color = &quot;Cilindros&quot;) + scale_shape_discrete(labels = c(&quot;Autom√°tica&quot;,&quot;Manual&quot;)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) Por fim, √© preciso frisar que, apesar de a gram√°tica prover uma forte funda√ß√£o para a constru√ß√£o de gr√°ficos, ela n√£o indica qual gr√°fico deve ser usado ou como ele deve parecer. Essas escolhas, fundamentadas na pergunta a ser respondida, nem sempre s√£o triviais, e negligenci√°-las pode gerar gr√°ficos mal constru√≠dos e conclus√µes equivocadas. Cabe a n√≥s, pesquisadores, desenvolver, aprimorar e divulgar as t√©cnicas de visualiza√ß√£o adequadas para cada tipo de vari√°vel, assim como apontar ou denunciar os usos incorretos e mal-intencionados. Mas, em um mundo cuja veracidade das not√≠cias √© cada vez menos importante, √© papel de todos ter senso cr√≠tico para entender e julgar as informa√ß√µes trazidas por um gr√°fico. "],
["3-3-colando-textos-no-r.html", "3.3 Colando textos no R", " 3.3 Colando textos no R Uma tarefa muito comum no R √© a de colar textos. As fun√ß√µes mais importantes para isso s√£o paste() e sprintf(), que v√™m com o pacote base. Nesse texto, vamos falar dessas duas fun√ß√µes e de um novo pacote do tidyverse, o glue. 3.3.1 paste() A fun√ß√£o paste() recebe um conjunto indeterminado de objetos como argumento atrav√©s do ... e vai colando os objetos passados elemento a elemento. Isso significa que se voc√™ passar dois vetores de tamanho n, a fun√ß√£o paste() retornar√° um vetor de tamanho n sendo cada posi√ß√£o a colagem dos dois vetores nessa posi√ß√£o. Por padr√£o, a colagem √© feita com um separador de espa√ßo simples (o &quot; &quot;). Exemplo: paste(c(1, 2, 3), c(4, 5, 6)) FALSE [1] &quot;1 4&quot; &quot;2 5&quot; &quot;3 6&quot; √â poss√≠vel alterar o separador pelo argumento sep =. Um atalho √∫til para o separador vazio (&quot;&quot;) √© a fun√ß√£o paste0: paste0(c(1, 2, 3), c(4, 5, 6)) FALSE [1] &quot;14&quot; &quot;25&quot; &quot;36&quot; Algumas vezes nosso interesse n√£o √© juntar vetores elemento a elemento, mas sim passar um vetor e colar todos seus elementos. Isso √© feito com o par√¢metro collapse =: paste(c(1, 2, 3, 4, 5, 6), collapse = &#39;@&#39;) FALSE [1] &quot;1@2@3@4@5@6&quot; Se voc√™ passar mais de um vetor e mandar colapsar os elementos, o paste() vai primeiro colar e depois colapsar: paste(c(1, 2, 3), c(4, 5, 6), collapse = &#39;@&#39;) FALSE [1] &quot;1 4@2 5@3 6&quot; 3.3.1.1 Cuidado Tenha muito cuidado ao passar vetores com comprimentos diferentes no paste()! Assim como muitas fun√ß√µes do R, o paste() faz reciclagem, ou seja, ele copia os elementos do menor vetor at√© ele ficar com o comprimento do maior vetor1. O problema √© que o paste() faz isso silenciosamente e n√£o avisa se voc√™ inserir um vetor com comprimento que n√£o √© m√∫ltiplo dos demais. Veja que resultado bizarro: paste(5:9, 1:3, 4:5) FALSE [1] &quot;5 1 4&quot; &quot;6 2 5&quot; &quot;7 3 4&quot; &quot;8 1 5&quot; &quot;9 2 4&quot; Por essas e outras que dizemos que √†s vezes o R funciona bem demais‚Ä¶ 3.3.2 sprintf() O sprintf() √© similar ao printf do C. Primeiro escrevemos um texto com %s no lugar das coisas que queremos substituir. Depois colocamos esses objetos nos outros argumentos da fun√ß√£o, na ordem em que eles aparecem no texto. sprintf(&#39;Aba%ste&#39;, &#39;ca&#39;) FALSE [1] &quot;Abacate&quot; Quando o argumento √© um vetor, a fun√ß√£o retorna um vetor com as substitui√ß√µes ponto a ponto. sprintf(&#39;Aba%ste&#39;, c(&#39;ca&#39;, &#39;ixas&#39;)) FALSE [1] &quot;Abacate&quot; &quot;Abaixaste&quot; Se o texto cont√©m mais de um %s e os objetos correspondentes s√£o vetores, o sprintf() tenta reciclar os vetores para ficarem do mesmo tamanho. Isso s√≥ funciona quando todos os objetos t√™m comprimentos que s√£o m√∫ltiplos do comprimento do maior objeto. Por exemplo, isso funciona: sprintf(&#39;Aba%s%s&#39;, c(&#39;ca&#39;), c(&#39;xi&#39;, &#39;te&#39;)) # ca foi reciclado FALSE [1] &quot;Abacaxi&quot; &quot;Abacate&quot; Isso n√£o funciona: sprintf(&#39;Aba%s%s&#39;, c(&#39;ca&#39;, &#39;ixaste&#39;), c(&#39;xi&#39;, &#39;te&#39;, &#39;.&#39;)) FALSE Error in sprintf(&quot;Aba%s%s&quot;, c(&quot;ca&quot;, &quot;ixaste&quot;), c(&quot;xi&quot;, &quot;te&quot;, &quot;.&quot;)): arguments cannot be recycled to the same length Nem sempre queremos substituir peda√ßos do nosso texto por outros textos. No lugar do %s, √© poss√≠vel colocar padr√µes para n√∫meros, por exemplo. Eu uso bastante o %d, que recebe inteiros. Uma funcionalidade legal do %d √© a possibilidade de adicionar zeros √† esquerda quando um n√∫mero n√£o atinge certa quantidade de d√≠gitos. Assim, quando ordenamos um vetor de textos que come√ßa com n√∫meros, a ordena√ß√£o √© a mesma da vers√£o num√©rica do vetor. Exemplo: nums &lt;- 1:11 sort(as.character(nums)) # ordenado pela string: 10 vem antes de 2 FALSE [1] &quot;1&quot; &quot;10&quot; &quot;11&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; sort(sprintf(&#39;%02d&#39;, nums)) # ordenado pela string: 02 vem antes de 10 FALSE [1] &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;04&quot; &quot;05&quot; &quot;06&quot; &quot;07&quot; &quot;08&quot; &quot;09&quot; &quot;10&quot; &quot;11&quot; 3.3.3 glue O glue √© um pacote recente. Sua primeira apari√ß√£o no GitHub foi em 23/12/2016. Isso significa que √© prov√°vel que algumas estejam em constante desenvolvimento, mas isso n√£o nos impede de aproveitar o que a ferramenta tem de bom. A fun√ß√£o glue() √© uma generaliza√ß√£o do sprintf() que permite chamar objetos do R diretamente ao inv√©s de utilizar o %s. Os objetos podem estar no global environment ou descritos por meio de objetos nomeados nos argumentos do glue(). Basta inserir os objetos entre chaves {}: library(glue) planeta &lt;- &#39;mundo&#39; glue(&#39;Ol√° {planeta} pela {y}a vez&#39;, y = 1) FALSE Ol√° mundo pela 1a vez Temb√©m √© poss√≠vel adicionar express√µes dentro das chaves: p &lt;- 1.123123123 glue(&#39;{p * 100}% das pessoas adoram R.&#39;) FALSE 112.3123123% das pessoas adoram R. glue(&#39;{scales::percent(p)} das pessoas adoram R.&#39;) FALSE 112% das pessoas adoram R. A fun√ß√£o glue_collapse() √© parecida com o paste() quando collapse = '', mas s√≥ aceita um objeto como entrada: x &lt;- glue_collapse(1:10) x FALSE 12345678910 x == paste(1:10, collapse = &#39;&#39;) FALSE [1] TRUE Se quiser colar os objetos elemento a elemento e depois colapsar, fa√ßa isso explicitamente em duas opera√ß√µes: library(magrittr) glue(&#39;{letters}/{LETTERS}&#39;) %&gt;% glue_collapse(&#39;, &#39;) FALSE a/A, b/B, c/C, d/D, e/E, f/F, g/G, h/H, i/I, j/J, k/K, l/L, m/M, n/N, o/O, p/P, q/Q, r/R, s/S, t/T, u/U, v/V, w/W, x/X, y/Y, z/Z O glue tamb√©m tem uma fun√ß√£o extra para trabalhar melhor com o %&gt;%, o glue_data(). O primeiro argumento dessa fun√ß√£o √© uma lista ou data.frame, e seus nomes s√£o utilizados como vari√°veis para alimentar as chaves das strings. Use o . para fazer opera√ß√µes com toda a base de dados: mtcars %&gt;% head() %&gt;% glue_data(&#39;O carro {row.names(.)} rende {mpg} milhas por gal√£o.&#39;) FALSE O carro Mazda RX4 rende 21 milhas por gal√£o. FALSE O carro Mazda RX4 Wag rende 21 milhas por gal√£o. FALSE O carro Datsun 710 rende 22.8 milhas por gal√£o. FALSE O carro Hornet 4 Drive rende 21.4 milhas por gal√£o. FALSE O carro Hornet Sportabout rende 18.7 milhas por gal√£o. FALSE O carro Valiant rende 18.1 milhas por gal√£o. 3.3.4 Resumo Use paste() para colar ou colapsar elementos usando um separador fixado. Use sprintf() quando quiser colocar objetos dentro de um texto complexo. Em todos os casos existe uma solu√ß√£o usando glue. Atualmente sempre que tenho um problema desse tipo uso o glue. At√© o momento, n√£o encontrei nenhum problema ou dificuldade. A vida do cientista de dados √© mais feliz com o tidyverse! 3.3.5 Extra: O Guilherme Jardim Duarte fez uma √≥tima sugest√£o sobre este artigo. No pacote stringi existe um operador %s+% que cola textos iterativamente, com uma sintaxe similar √† linguagem python, e que permite a colagem de textos usando um simples +. Exemplo: library(stringi) &#39;a&#39; %s+% &#39;ba&#39; %s+% &#39;ca&#39; %s+% &#39;xi&#39; FALSE [1] &quot;abacaxi&quot; Voc√™ pode adicionar esse operador como um atalho no RStudio, an√°logo ao Ctrl+Shift+M que usamos para escrever o %&gt;%. Para isso, veja esse tutorial sobre RStudio Addins. Mais sobre isso no livro R inferno‚Ü© "],
["4-modelagem.html", "Cap√≠tulo 4 Modelagem ", " Cap√≠tulo 4 Modelagem "],
["4-1-monty-hall-e-diagramas-de-influencia.html", "4.1 Monty hall e diagramas de influ√™ncia", " 4.1 Monty hall e diagramas de influ√™ncia Voc√™ est√° num jogo na TV e o apresentador pede para escolher uma entre 3 portas. Atr√°s de uma dessas portas tem uma Ferrari e nas outras duas temos cabras. Voc√™ escolhe uma porta. Depois, o apresentador retira uma porta que tem uma cabra e pergunta: voc√™ quer trocar de porta? A princ√≠pio, voc√™ pode achar que sua probabilidade de ganhar √© 1/2, j√° que uma das portas foi retirada, ent√£o n√£o importa se voc√™ troca ou n√£o. Mas a resposta √© que sim, vale √† pena trocar de porta! A probabilidade de vencer o jogo trocando a porta √© de 2/3. Figura 2.1: Brincadeira do XKCD. O problema de Monty Hall √© talvez o mais eloquente exemplo de como a probabilidade pode confundir a mente humana. Esse problema desafiou a comunidade cient√≠fica no final do s√©culo XX e chegou at√© a ser considerado um paradoxo. Recomendo ler o livro O Andar do B√™bado, de Leonard Mlodinow, que conta essa e muitas outras hist√≥rias interessantes sobre a probabilidade. Existem v√°rias formas de explicar por qu√™ trocar a porta √© a melhor estrat√©gia. A que eu mais gosto √© a do pr√≥prio Andar do B√™bado, que mostra que, quando voc√™ escolhe a primeira porta, voc√™ est√° apostando se acertou ou n√£o a Ferrari. Se voc√™ apostar que acertou a Ferrari, n√£o deve trocar a porta e, se voc√™ apostar que errou a Ferrari, deve trocar. A aposta de errar a Ferrari de primeira tem probabilidade 2/3, logo, vale √† pena trocar. Nesse post, mostramos uma solu√ß√£o alternativa, simples e elegante para o problema usando diagramas de influ√™ncia e o pacote bnlearn. 4.1.1 Redes bayesianas As redes Bayesianas s√£o o resultado da combina√ß√£o de conceitos probabil√≠sticos e conceitos da teoria dos grafos. Segundo Pearl, tal uni√£o tem como consequ√™ncias tr√™s benef√≠cios: i) prover formas convenientes para expressar suposi√ß√µes do modelo; ii) facilitar a representa√ß√£o de fun√ß√µes de probabilidade conjuntas; e iii) facilitar o c√°lculo eficiente de infer√™ncias a partir de observa√ß√µes. Da teoria de probabilidades precisamos apenas de alguns resultados b√°sicos sobre probabilidade condicional. Primeiramente, pela defini√ß√£o de probabilidade condicional, sabemos que \\[ p(x_1, x_2) = p(x_1)p(x_2|x_1). \\] Aplicando essa regra iterativamente para \\(n\\) vari√°veis, temos \\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|x_1,\\dots, x_{j-1}). \\] Agora, imagine que, no seu problema, a vari√°vel aleat√≥ria \\(X_j\\) n√£o dependa probabilisticamente de todas as vari√°veis \\(X_1,\\dots, X_{j-1}\\), e sim apenas de um subconjunto \\(\\Pi_j\\) dessas vari√°veis. Fazendo isso, a equa√ß√£o pode ser escrita como \\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|\\pi_j). \\] Chamamos \\(\\Pi_j\\) de pais de \\(X_j\\). Esse conjunto pode ser pensado como as vari√°veis que s√£o suficientes para determinar as probabilidades de \\(X_j\\). A parte mais legal das redes Bayesianas √© que elas podem ser representadas a partir de DAGs (grafos direcionados ac√≠clicos). No grafo, se \\(X_1\\) aponta para \\(X_2\\), ent√£o \\(X_1\\) √© pai de \\(X_2\\). Por exemplo, esse grafo aqui representa a distribui√ß√£o de probabilidades \\(p(x_1, \\dots, x_5)\\) com \\[ p(x_1, \\dots, x_5) = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_3,x_2)p(x_5|x_4). \\] 4.1.2 Diagrama de influ√™ncias Um diagrama e influ√™ncias √© uma rede Bayesiana com n√≥s de decis√£o e utilidade (ganhos). Ou seja, √© uma jun√ß√£o de tr√™s conceitos: \\[ \\underbrace{\\text{prob. condicional} + \\text{grafos}}_{\\text{rede Bayesiana}} + \\text{teoria da decis√£o} = \\text{diagrama de influ√™ncias} \\] Na teoria da decis√£o, usualmente estamos interessados em maximizar a utilidade esperada. No diagrama, considerando a estrutura de probabilidades dada pela rede Bayesiana e as informa√ß√µes dispon√≠veis, queremos escolher a decis√£o que faz com que, em m√©dia, nosso retorno seja mais alto. Com diagramas de influ√™ncias, √© poss√≠vel organizar sistemas complexos com m√∫ltiplas decis√µes, considerando diferentes conjuntos de informa√ß√µes dispon√≠veis. √â uma ferramenta realmente muito poderosa. 4.1.3 Voltando ao Monty Hall Agora que sabemos um pouquinho de diagramas de influ√™ncia, podemos desenhar o do Monty Hall: O jogador tem duas decis√µes a tomar: \\(D_1\\) (escolha_inicial): A escolha da porta inicial (1, 2, 3). \\(D_2\\) (trocar): Trocar a porta ou n√£o (s, n). Tamb√©m temos duas fontes de incerteza: \\(X_1\\) (ferrari): Em qual porta est√° a Ferrari (1, 2, 3). \\(X_2\\) (porta_retirada): Qual porta foi retirada (1, 2, 3). Essa vari√°vel n√£o √© sempre aleat√≥ria: se eu escolho a porta 1 e a Ferrari est√° em 2, o apresentador √© obrigado a retirar a porta 3. Se o apresentador tiver a op√ß√£o de escolher (que acontece no caso da escolha inicial ser a Ferrari), o apresentador escolhe uma porta para retirar aleatoriamente. Finalmente, temos um n√≥ de utilidade: \\(U_1\\) (result): Ganhei a Ferrari (ganhei, perdi). Em R, podemos construir a rede Bayesiana do problema utilizando o pacote bnlearn: FALSE [,1] [,2] FALSE [1,] &quot;escolha_inicial&quot; &quot;porta_retirada&quot; FALSE [2,] &quot;ferrari&quot; &quot;porta_retirada&quot; FALSE [3,] &quot;porta_retirada&quot; &quot;trocar&quot; FALSE [4,] &quot;trocar&quot; &quot;result&quot; FALSE [5,] &quot;ferrari&quot; &quot;result&quot; FALSE [6,] &quot;escolha_inicial&quot; &quot;result&quot; O output desse conjunto de opera√ß√µes √© um objeto do tipo bn com v√°rias propriedades pr√© calculadas pelo pacote bnlearn: Random/Generated Bayesian network model: [escolha_inicial][ferrari][porta_retirada|escolha_inicial:ferrari][trocar|porta_retirada] [result|escolha_inicial:ferrari:trocar] nodes: 5 arcs: 6 undirected arcs: 0 directed arcs: 6 average markov blanket size: 3.60 average neighbourhood size: 2.40 average branching factor: 1.20 generation algorithm: Empty Com as especifica√ß√£o do problema dada, se gerarmos aleatoriamente todos os cen√°rios, chegamos √† essa combina√ß√£o de casos equiprov√°veis (ver Extra 2) Agora, vamos escrever todas as combina√ß√µes poss√≠veis de cen√°rios e guardar num data.frame chamado dados: escolha_inicial ferrari porta_retirada trocar result 1 1 2 n ganhei 1 1 2 s perdi 1 1 3 n ganhei 1 1 3 s perdi 1 2 3 n perdi 1 2 3 s ganhei 1 3 2 n perdi 1 3 2 s ganhei 2 1 3 n perdi 2 1 3 s ganhei 2 2 1 n ganhei 2 2 1 s perdi 2 2 3 n ganhei 2 2 3 s perdi 2 3 1 n perdi 2 3 1 s ganhei 3 1 2 n perdi 3 1 2 s ganhei 3 2 1 n perdi 3 2 1 s ganhei 3 3 2 n ganhei 3 3 2 s perdi 3 3 1 n ganhei 3 3 1 s perdi Finalmente, ajustamos nossa rede Bayesiana, usando a fun√ß√£o bnlearn::bn.fit(). A fun√ß√£o bnlearn::cpquery() (conditional probability query) serve para realizar uma consulta de probabilidades dada a rede ajustada. No nosso caso, a partir de uma escolha inicial qualquer \\(d_1\\), queremos saber o ganho ao trocar √© maior que o ganho ao n√£o trocar. \\[ \\mathbb E(U_1\\; |\\; D_2 = \\text{s}, D_1 = d_1) &gt; \\mathbb E(U_1\\; |\\; D_2 = \\text{n}, D_1 = d_1). \\] Fazendo contas, isso equivale matematicamente a consultar se \\[ \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s}) &gt; \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{n}) \\] Agora, podemos consultar \\(\\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s})\\) com nosso modelo! [1] 0.6666704 E n√£o √© que d√° 2/3 mesmo? Da mesma forma, temos [1] 0.3333187 Resolvido! 4.1.4 Wrap-up Vale √† pena trocar a porta! Redes Bayesianas juntam grafos e probabilidades condicionais Diagramas de influ√™ncia juntam redes Bayesianas e teoria da decis√£o Essas ferramentas podem ser utilizadas tanto para resolver Monty Hall quanto para ajudar em sistemas complexos. √â isso pessoal. Happy coding ;) 4.1.5 Extra Se voc√™ ficou interessada em como eu fiz o diagrama, utilizei o pacote DiagrammeR. O c√≥digo est√° aqui: 4.1.6 Extra 2 √â poss√≠vel simular os dados que coloquei no post com uma fun√ß√£o simples, que adicionei abaixo. Na verdade, o fato de eu ter considerado somente as combina√ß√µes √∫nicas de cen√°rios e n√£o os dados simulados abaixo √© um pouco roubado, e s√≥ funciona porque os cen√°rios calham de ser, de fato, equiprov√°veis. Observations: 10,000 Variables: 5 $ escolha_inicial &lt;fct&gt; 3, 1, 2, 1, 1, 1, 3, 1, 2, 3, 3, 1, 3, 1, 2, 2, 2,... $ ferrari &lt;fct&gt; 1, 1, 2, 1, 1, 2, 3, 3, 1, 2, 3, 3, 2, 1, 1, 3, 1,... $ porta_retirada &lt;fct&gt; 2, 3, 1, 3, 2, 3, 2, 2, 3, 1, 1, 2, 1, 2, 3, 1, 3,... $ trocar &lt;fct&gt; n, s, s, n, s, n, n, n, n, s, s, s, s, n, n, s, n,... $ result &lt;fct&gt; perdi, perdi, perdi, ganhei, perdi, perdi, ganhei,... Os dados do post podem ser obtidos fazendo isso aqui: Agradecimentos: Rafael Stern, que me convenceu de que vale √† pena mostrar os dados simulados üòâ "],
["4-2-construindo-autoencoders.html", "4.2 Construindo Autoencoders", " 4.2 Construindo Autoencoders Autoencoders s√£o redes neurais treinadas com o objetivo de copiar o seu input para o seu output. Esse interesse pode parecer meio estranho, mas na pr√°tica o objetivo √© aprender representa√ß√µes (encodings) dos dados, que podem ser usadas para redu√ß√£o de dimensionalidade ou at√© mesmo compress√£o de arquivos. Basicamente, um autoencoder √© dividido em duas partes: um encoder que √© uma fun√ß√£o \\(f(x)\\) que transforma o input para uma representa√ß√£o \\(h\\) um decoder que √© uma fun√ß√£o \\(g(x)\\) que transforma a representa√ß√£o \\(h\\) em sua reconstru√ß√£o \\(r\\) Imagem do blog do Keras 4.2.1 Construindo o seu primeiro autoencoder Nesse pequeno tutorial, vou usar o keras para definir e treinar os nossos autoencoders. Como base de dados vou usar algumas simula√ß√µes e o banco de dados mnist (famoso para todos que j√° mexeram um pouco com deep learning). O mnist √© um banco de dados de imagens de tamanho 28x28 de d√≠gitos escritos √† m√£o. Esse dataset promoveu grandes avan√ßos na √°rea de reconhecimento de imagens. Com esse c√≥digo definimos um modelo da seguinte forma: \\[ X = (X*W_1 + b_1)*W_2 + b_2 \\] Em que: \\(X\\) √© o nosso input com dimens√£o (?, 784) \\(W_1\\) √© uma matriz de pesos com dimens√µes (784, 32) \\(b_1\\) √© uma matriz de forma (?, 32) \\(W_2\\) √© uma matriz de pesos com dimens√µes (32, 784) \\(b_2\\) √© uma matriz de forma (?, 784) Note que ? aqui √© o n√∫mero de observa√ß√£oes da base de dados. Agora vamos estimar \\(W_1\\), \\(W_2\\), \\(b_1\\) e \\(b_2\\) de modo a minimizar alguma fun√ß√£o de perda. Inicialmente vamos usar a binary crossentropy por pixel que √© definida por: \\[-\\sum_{i=1}y_i*log(\\hat{y}_i)\\] Isso √© definido no keras usando: N√£o vou entrar em detalhes do que √© o adadelta, mas √© uma varia√ß√£o do m√©todo de otimiza√ß√£o conhecido como gradient descent. Agora vamos carregar a base de dados e em seguida treinar o nosso autoencoder`. Estimamos os par√¢metros desse modelo no keras fazendo: Depois de rodar todas as itera√ß√µes, voc√™ poder√° usar o seu encoder e o seu decoder para entender o que eles fazem com as imagens. Veja o exemplo a seguir em que vamos obter os encodings para as 10 primeiras imagens da base de teste e depois reconstruir a imagem usando o decoder. FALSE [1] 10 32 FALSE [1] 0.0000000 10.1513205 3.5742311 2.6635208 6.3097358 3.4840517 FALSE [7] 9.1041250 6.6329145 1.6385922 9.8017225 9.5529270 1.6670935 FALSE [13] 5.7208562 4.8035479 3.9149191 0.6408147 1.2716029 3.1215091 FALSE [19] 13.7575903 0.0000000 1.8692881 3.2142215 0.7444992 5.0728440 FALSE [25] 8.2932110 9.9866810 2.7651572 11.1291723 5.2460670 5.6875997 FALSE [31] 10.6097431 3.6338394 O encoder transforma a matriz de (10, 784) para uma matriz com dimensao (10, 2). Podemos reconstruir a imagem, a pardir da imagem que foi comprimida usando o nosso decoder. Compare as reconstru√ß√µes com as imagens originais abaixo: Um ponto interessante √© que esse modelo faz uma aproxima√ß√£o da solu√ß√£o por componentes principais! Na verdade, a defini√ß√£o do quanto s√£o parecidos √© quase-equivalente. Isso quer dizer que os pesos \\(W\\) encontrados pelo PCA e pelo autoencoder ser√£o diferentes, mas o sub-espa√ßo criado pelos mesmos ser√° equivalente. Se s√£o equivalentes, qual a vantagem de usar autoencoders ao inv√©s de PCA? O PCA para por aqui, voc√™ define que ser√£o apenas rela√ß√µes lineares, e voc√™ reduz dimens√£o apenas reduzindo o tamanho da matriz. Em autoencoders voc√™ tem diversas outras sa√≠das para aprimorar o m√©todo. A primeira delas √© simplesmente adicionar uma condi√ß√£o de esparsidade nos pesos. Isso vai reduzir o tamanho do vetor latente (como √© chamada a camada do meio do autoencoder) tamb√©m, pois ele ter√° mais zeros. Isso pode ser feito rapidamente com o keras. Basta adicionar um activity_regularizer em nossa camada de encoding. Isso vai adicionar na fun√ß√£o de perda um termo que toma conta do valor dos outputs da camada intermedi√°ria. Outra forma de melhorar o seu autoencoder √© permitir que o encoder e o decoder sejam redes neurais profundas. Com isso, ao inv√©s de tentar encontrar transforma√ß√µes lineares, voc√™ permitir√° que o autoencoder encontre transforma√ß√µes n√£o lineares. Mais uma vez fazemos isso com o keras: Existem formas ainda mais inteligentes de construir esses autoencoders, mas o post iria ficar muito longo e n√£o ia sobrar asssunto para o pr√≥ximo. Se voc√™ quiser saber mais, recomendo fortemente a leitura deste artigo do blog do Keras e desse cap√≠tulo. Uma fam√≠lia bem moderna de autoencoders s√£o os VAE (Variational Autoencoders). Esses autoencoders aprendem modelos de vari√°veis latentes. Isso √© interessante porque permite que voc√™ gere novos dados, parecidos com os que voc√™ usou para treinar o seu autoencoder. Voc√™ pode encontrar uma implementa√ß√£o desse modelo aqui. "],
["4-3-modelos-beseados-em-arvores-e-a-multicolinearidade.html", "4.3 Modelos beseados em √°rvores e a multicolinearidade", " 4.3 Modelos beseados em √°rvores e a multicolinearidade Modelos baseados em √°rvores como √°rvores de decis√£o, random forest, ligthGBM e xgboost s√£o conhecidos, dentre outras qualidades, pela sua robust√™s diante do problema de multicolinearidade. √â sabido que seu poder preditivo n√£o se abala na presen√ßa de vari√°veis extremamente correlacionadas. Por√©m, quem nunca usou um Random Forest pra fazer sele√ß√£o de vari√°veis? Pegar, por exemplo, as top 10 mais importantes e descartar o resto? Ou at√© mesmo arriscou uma interpreta√ß√£o e concluiu sobre a ordem das vari√°veis mais importantes? Abaixo mostraremos o porqu√™ n√£o devemos ignorar a quest√£o da multicolinearidade completamente! 4.3.1 Um modelo bonitinho Primeiro vamos ajustar um modelo bonitinho, livre de multicolinearidade. Suponha que queiramos prever Petal.Length utilizando as medidas das s√©palas (Sepal.Width e Sepal.Length) da nossa boa e velha base iris. O gr√°fico acima mostra que as vari√°veis explicativas n√£o s√£o fortemente correlacionadas. Ajustando uma random fores, temos a seguinte ordem de import√¢ncia das vari√°veis: Sem surpresas. Agora vamos para o problema! 4.3.2 Um modelo com feinho Vamos forjar uma situa√ß√£o extrema em que muitas vari√°veis sejam multicolineares. Vou fazer isso repetindo a coluna Sepal.Length v√°rias vezes. Agora a coisa t√° feia! Temos 20 vari√°veis perfeitamente colineares. Mesmo assim um random forest nessa nova base n√£o perderia poder preditivo. Mas como ficou a import√¢ncia das vari√°veis? Aqui o jogo j√° se inverteu: concluir√≠amos que Sepal.Width √© mais importante de todas as vari√°veis! 4.3.3 Sele√ß√£o de vari√°veis furado O gr√°fico abaixo mostra que quanto mais vari√°veis correlacionadas tivermos, menor a import√¢ncia de TODAS ELAS SIMULTANEAMENTE! √â como se as vari√°veis colineares repartissem a import√¢ncia entre elas. Na pr√°tica, se estabelecessemos um corte no valor de import√¢ncia pra descartar vari√°veis (como ilustrado pela linha vermelha), ter√≠amos um problema em potencial: poder√≠amos estar jogando fora informa√ß√£o muito importante. 4.3.4 Como tratar multicolinearidade, ent√£o? Algumas maneiras de lidar com multicolinearidade s√£o: Observar a matriz de correla√ß√£o VIF Recursive feature elimination 4.3.5 Conclus√£o Cuidado ao jogar tudo no caldeir√£o! Devemos sempre nos preocupar com multicolinearidade, mesmo ajustando modelos baseados em √°rvores. "],
["4-4-woe-em-r-com-tidywoe.html", "4.4 WoE em R com tidywoe", " 4.4 WoE em R com tidywoe WoE (weight of evidence) √© uma ferramenta bastante usada em aplica√ß√µes de regress√£o log√≠stica, principalmente na √°rea de score de cr√©dito. Simploriamente falando, ele transforma categorias em n√∫meros que refletem a diferen√ßa entre elas pelo crit√©rio de separa√ß√£o do Y = 1 e Y = 0. Se voc√™ ainda n√£o sabe o que √© ou quer ler mais sobre o assunto, um texto que eu gostei de ler: Data Exploration with Weight of Evidence and Information Value in R O autor desse texto √© o Kim Larsen, criador do pacote Information que √© completo e cheio de ferramentas sofisticadas em torno do WoE. Por√©m, no dia a dia do meu trabalho volta e meia eu tinha que construir rotinas pr√≥prias para fazer as vers√µes em WoE das minhas vari√°veis, mesmo com v√°rios pacotes completos dispon√≠veis. A principal motiva√ß√£o era que eles n√£o eram muito pr√°ticos e n√£o se encaixavam na filosofia do tidyverse. Da√≠ acabei juntando essas rotinas num pacote chamado tidywoe e deixando no ar. A ideia √© que ela fa√ßa o analista ganhar em tempo, legibilidade e reprodutibilidade. Abaixo segue como usar. 4.4.1 Instala√ß√£o e dados Para instalar, basta rodar abaixo. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;athospd/tidywoe&quot;) library(tidyverse) library(tidywoe) # install.packages(&quot;FactoMineR&quot;) data(tea, package = &quot;FactoMineR&quot;) tea_mini &lt;- tea %&gt;% dplyr::select(breakfast, how, where, price) 4.4.2 Como usar Tem duas fun√ß√µes que importam: - add_woe() - adiciona os woe‚Äôs num data frame. - woe_dictionary() - cria dicion√°rio que mapeia as categorias com os woe‚Äôs. 4.4.3 add_woe() A fun√ß√£o add_woe() serve para adicionar as vers√µes WoE‚Äôs das vari√°veis em sua amostra de dados. tea_mini %&gt;% add_woe(breakfast) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 -0.2564295 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Voc√™ pode selecionar as vari√°veis que vc quiser selecionando-as como se fosse no dplyr::select(). tea_mini %&gt;% add_woe(breakfast, where:price) 4.4.4 woe_dictionary() A fun√ß√£o woe_dictionary() √© uma das duas partes necess√°rias para fazer o add_woe() funcionar (a outra parte s√£o os dados). Ele constr√≥i o dicion√°rio de categorias e seus respectivos woe‚Äôs. tea_mini %&gt;% woe_dictionary(breakfast) variable explanatory n_tot n_breakfast n_Not.breakfast p_breakfast p_Not.breakfast woe how tea bag 170 80 90 0.5555556 0.5769231 -0.0377403 how tea bag+unpackaged 94 50 44 0.3472222 0.2820513 0.2078761 how unpackaged 36 14 22 0.0972222 0.1410256 -0.3719424 where chain store 192 90 102 0.6250000 0.6538462 -0.0451204 4.4.5 Usando um dicion√°rio customizado Muitas vezes h√° o interesse em ajustar na m√£o alguns valores de woe para consertar a ordem dos efeitos de uma dada vari√°vel ordinal. Esse √© o motivo de o add_woe() poder receber um dicion√°rio passado pelo usu√°rio. Isso se faz por meio do argumento .woe_dictionary. A maneira mais f√°cil de se fazer isso √© montar um dicion√°rio inicial com o woe_dictionary() e depois alterar os valores nele para alcan√ßar os ajustes desejados. Exemplo: # Construa um dicion√°rio inicial tea_mini_woe_dic &lt;- tea_mini %&gt;% woe_dictionary(breakfast) # Mexa um pouquinho nos woes tea_mini_woe_dic_arrumado &lt;- tea_mini_woe_dic %&gt;% mutate(woe = if_else(explanatory == &quot;p_unknown&quot;, 0, woe)) # Passe esse dicion√°rio para o add_woe() tea_mini %&gt;% add_woe(breakfast, .woe_dictionary = tea_mini_woe_dic_arrumado) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 0.0000000 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 4.4.6 Exemplo de explora√ß√£o O woe_dictionary() devolve uma tabela arrumada, bem conveniente para explorar mais. Por exemplo, a tabela est√° pronta para o ggplot. Aqui est√° o github do pacote para contribui√ß√µes. Pretendo colocar bastante coisa nova no pacote ainda. "],
["5-regressao-logistica-aspectos-computacionais.html", "Cap√≠tulo 5 Regress√£o log√≠stica: aspectos computacionais", " Cap√≠tulo 5 Regress√£o log√≠stica: aspectos computacionais Neste texto vamos discutir um pouco sobre regress√£o log√≠stica, tensorflow e Modelos Lineares Generalizados (Generalized Linear Models, GLMs). N√£o vou economizar nas matem√°ticas nem nos c√≥digos. Se voc√™ n√£o conhece GLMs, recomendo dar uma lida, pelo menos na introdu√ß√£o, do livro do professor Gilberto A. Paula. Se voc√™ n√£o conhece o Tensorflow, recomendo ver a p√°gina do RStudio sobre Tensorflow. Se voc√™ curte a parte computacional da estat√≠stica, esse livro do LEG-UFPR √© obrigat√≥rio. Eles s√£o os melhores. 5.0.1 Introdu√ß√£o: o tensorglm Um de meus interesses no momento √© implementar GLMs usando Tensorflow. O Tensorflow √© uma biblioteca computacional mantida pela Google que utiliza paraleliza√ß√£o e o poder das GPUs (Graphical Processing Units) para fazer contas. O Tensorflow foi especialmente desenhado para facilitar o ajuste de redes neurais profundas e outros modelos sofisticados. GLMs s√£o casos particulares de redes neurais. Uma rede neural com apenas uma camada e com fun√ß√µes de perda / verossimilhan√ßas baseadas na Diverg√™ncia de Kullback-Leibler s√£o exatamente iguais aos GLMs. Por exemplo, essa diverg√™ncia equivale ao erro quadr√°tico m√©dio para a distribui√ß√£o gaussiana e binary-crossentropy para log√≠stica. Por isso, n√£o √© de se surpreender que j√° existam solu√ß√µes prontas para modelos espec√≠ficos, como regress√£o linear normal, log√≠stica, e at√© Poisson. No entanto, essas solu√ß√µes t√™m duas limita√ß√µes: N√£o s√£o extensivas. Por exemplo, n√£o achei c√≥digos para as distribui√ß√µes normal inversa, gama e binomial negativa. As solu√ß√µes atuais utilizam o algoritmo descida de gradiente para otimiza√ß√£o, que √© muito legal, mas n√£o se aproveita de alguns resultados que temos na √°rea de GLMs, como o IWLS (Iterated Weighted Least Squares), que √© uma deriva√ß√£o do algoritmo Fisher-scoring, que reduz o problema do ajuste ao c√°lculo iterado de inversas e multiplica√ß√µes de matrizes. Meu intuito √©, ent√£o, montar uma solu√ß√£o alternativa que funcione igual √† fun√ß√£o glm() do R, mas usando Tensorflow no backend ao inv√©s do algoritmo atual, que √© em Fortran. Com isso, espero que o ajuste seja mais eficiente quando os dados s√£o grandes e permita trabalhar com dados que n√£o cabem na mem√≥ria. 5.0.2 A regress√£o log√≠stica Meu primeiro experimento com o tensorglm foi implementar a regress√£o log√≠stica usando tensorflow, com descida de gradiente. Considere o problema \\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta x),\\] em que \\(Y\\) √© nossa vari√°vel resposta, \\(x\\) √© nossa vari√°vel explicativa, \\(\\alpha\\) e \\(\\beta\\) s√£o os par√¢metros que queremos estimar e \\(\\sigma(\\cdot)\\) √© a fun√ß√£o sigmoide, cuja inversa √© a fun√ß√£o de liga√ß√£o log√≠stica. \\[\\sigma(\\eta) = \\frac{1}{1 + e^{-\\eta}}\\] Considerando que temos observa√ß√µes \\(Y_1, \\dots, Y_n\\) condicionalmente independentes, j√° temos o suficiente para especificar nosso modelo de regress√£o log√≠stica. O pr√≥ximo passo √© definir, com base nisso, a fun√ß√£o que queremos otimizar. A partir de uma amostra \\(y_1, \\dots, y_n\\) e observando que \\(\\mu_i = \\sigma(\\alpha + \\beta x_i)\\), a verossimilhan√ßa do modelo √© dada por \\[ \\mathcal L((\\alpha, \\beta)|\\mathbf y) = \\prod_{i=1}^n f(y_i|(\\alpha, \\beta), x_i) = \\prod_{i=1}^n\\mu_i^{y_i}(1-\\mu_i)^{1-y_i} \\] O logaritmo da verossimilhan√ßa √© dado por \\[ \\begin{aligned} l((\\alpha, \\beta)|\\mathbf y) &amp;= \\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\\\ &amp;= \\sum_{i=1}^n y_i\\log(\\sigma(\\alpha + \\beta x_i)) + (1-y_i)\\log(1 - \\sigma(\\alpha + \\beta x_i)) \\end{aligned} \\] Nosso objetivo √© maximizar \\(l\\) com rela√ß√£o √† \\(\\alpha\\) e \\(\\beta\\). Detalhe: essa soma, se multiplicada por -1, tamb√©m √© chamada de fun√ß√£o de perda binary cross-entropy. Por isso que tanto faz voc√™ definir GLMs a partir de \\(P(Y|x)\\) ou a partir da fun√ß√£o de perda! OK, problema dado! vamos implementar usando tensorflow! Feito! Agora podemos usar a magia do tensorflow, que √© esperto o suficiente para otimizar essa perda sem a gente se preocupar em calcular derivadas na m√£o. Para quem n√£o conhece o algoritmo de descida de gradiente, ele funciona assim: \\[ (\\alpha, \\beta)_{\\text{novo}} = (\\alpha, \\beta)_{\\text{velho}} + k \\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}}), \\] onde \\(\\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}})\\) √© o gradiente da verossimilhan√ßa em rela√ß√£o ao vetor \\((\\alpha, \\beta)\\), ou seja, s√£o as derivadas parciais de \\(l\\) em rela√ß√£o √† \\(\\alpha\\) e \\(\\beta\\). Isso d√° a dire√ß√£o e intensidade em que os valores devem ser atualizados. \\(k\\) √© chamado de learning rate, √© um fator usado para controlar o tamanho do passo dado pelo gradiente. Esse valor normalmente √© definido √† m√£o. No caso dos GLMs, \\(k\\) √© substitu√≠do pelo inverso da segunda derivada da \\(l\\) em rela√ß√£o aos par√¢metros, gerando assim os algoritmos de Newton-Raphson e Fisher-scoring. Detalhe: se voc√™ procurar esse algoritmo na internet, voc√™ vai encontrar um \\(-\\) e n√£o um \\(+\\). Isso acontece porque estamos usando a verossimilhan√ßa e n√£o a perda. FALSE Iter: 01, alpha=2.32, beta=3.593 FALSE Iter: 02, alpha=1.56, beta=3.409 FALSE Iter: 03, alpha=1.411, beta=2.989 FALSE Iter: 04, alpha=1.261, beta=2.665 FALSE Iter: 05, alpha=1.153, beta=2.422 FALSE Iter: 06, alpha=1.078, beta=2.257 FALSE Iter: 07, alpha=1.033, beta=2.154 FALSE Iter: 08, alpha=1.006, beta=2.095 FALSE Iter: 09, alpha=0.992, beta=2.062 FALSE Iter: 10, alpha=0.984, beta=2.045 Iter: 01, alpha=2.32, beta=3.593 Iter: 02, alpha=1.56, beta=3.409 Iter: 03, alpha=1.411, beta=2.989 Iter: 04, alpha=1.261, beta=2.665 Iter: 05, alpha=1.153, beta=2.422 Iter: 06, alpha=1.078, beta=2.257 Iter: 07, alpha=1.033, beta=2.154 Iter: 08, alpha=1.006, beta=2.095 Iter: 09, alpha=0.992, beta=2.062 Iter: 10, alpha=0.984, beta=2.045 Parece que funcionou! Agora sabemos ajustar uma regress√£o log√≠stica na m√£o, com o algoritmo de descida de gradiente‚Ä¶ ou ser√° que n√£o? 5.0.3 O Problema Vamos considerar o mesmo problema, mas agora com duas explicativas. temos \\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta_1 x_2+ \\beta_2 x_2),\\] As contas s√£o exatamente as mesmas e vou omitir, mostrando apenas o c√≥digo novo. FALSE Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 FALSE Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Oops! Explodiu! Por que ser√°??? Uma forma de corrigir esse problema √© considerando uma taxa de aprendizado k um pouco menor. Com os mesmos dados e modelo acima, ao fazer e rodar novamente, j√° conseguimos chegar nos resultados abaixo. Iter: 01, alpha=1.525, beta1=2.492, beta2=3.205 Iter: 02, alpha=1.183, beta1=2.32, beta2=3.36 Iter: 03, alpha=1.122, beta1=2.248, beta2=3.34 Iter: 04, alpha=1.101, beta1=2.208, beta2=3.296 Iter: 05, alpha=1.085, beta1=2.178, beta2=3.254 Iter: 06, alpha=1.073, beta1=2.152, beta2=3.216 Iter: 07, alpha=1.062, beta1=2.13, beta2=3.183 Iter: 08, alpha=1.053, beta1=2.112, beta2=3.154 Iter: 09, alpha=1.044, beta1=2.095, beta2=3.13 Iter: 10, alpha=1.037, beta1=2.082, beta2=3.109 Mais algumas itera√ß√µes e o modelo converge. Mas n√≥s n√£o queremos ficar fazendo um ajuste t√£o fino no valor de k, certo? Afinal, queremos resolver problemas do mundo real, n√£o ficar escolhendo valores de k‚Ä¶ Outra forma de resolver isso √© evitando problemas num√©ricos nas contas. O c√°lculo da fun√ß√£o de perda, por exemplo, pode ser melhorado. Mas como? Bom, problemas num√©ricos n√£o s√£o minha especialidade, ent√£o agora √© hora de seguir os mestres. Vamos olhar como o R e como o Tensorflow implementam as fun√ß√µes de perda para regress√£o log√≠stica. 5.0.3.1 Os objetos de classe family no R No R, os GLMs buscam informa√ß√µes de objetos da classe family() para realizar os ajustes. No caso da log√≠stica, o objeto √© retornado por uma fun√ß√£o chamada binomial(). O resultado disso √© uma lista com v√°rios m√©todos implementados. Por exemplo, a vari√¢ncia da binomial √© dada por: FALSE function (mu) FALSE mu * (1 - mu) FALSE &lt;bytecode: 0x7fa83fe769e8&gt; FALSE &lt;environment: 0x7fa83fe7dbd8&gt; function (mu) mu * (1 - mu) &lt;bytecode: 0x55fc8e220a18&gt; &lt;environment: 0x55fca4eb5040&gt; A fun√ß√£o de perda √© dada pelo m√©todo fam$dev.resids() (res√≠duos deviance), e o c√≥digo fonte √©: FALSE function (y, mu, wt) FALSE .Call(C_binomial_dev_resids, y, mu, wt) FALSE &lt;bytecode: 0x7fa83fe76278&gt; FALSE &lt;environment: 0x7fa83fe7dbd8&gt; function (y, mu, wt) .Call(C_binomial_dev_resids, y, mu, wt) &lt;bytecode: 0x55fc8e2253a0&gt; &lt;environment: 0x55fca4eb5040&gt; Hmm, parece que √© uma fun√ß√£o feita em C. Como as contas da nossa perda (soma, logaritmo, multiplica√ß√£o e divis√£o) j√° s√£o todas implementadas em C, provavelmente a conta foi implementada em C para garantir estabilidade num√©rica. Olhando o c√≥digo-fonte do pacote stats, encontramos a defini√ß√£o da fun√ß√£o. A fun√ß√£o √© um pouco longa, ent√£o eu mantive apenas as partes importantes: static R_INLINE double y_log_y(double y, double mu) { return (y != 0.) ? (y * log(y/mu)) : 0; } SEXP binomial_dev_resids(SEXP y, SEXP mu, SEXP wt) { /* inicializa√ß√£o de vari√°veis e verifica√ß√µes */ /* rmu e ry s√£o os valores de mu e y transformados para reais */ /* rmu e ry s√£o os valores de mu e y transformados para reais */ for (i = 0; i &lt; n; i++) { mui = rmu[i]; yi = ry[i]; rans[i] = 2 * rwt[lwt &gt; 1 ? i : 0] * (y_log_y(yi, mui) + y_log_y(1 - yi, 1 - mui)); } /* outros c√≥digos n√£o muito importantes */ UNPROTECT(nprot); return ans; } Eu n√£o programo muito em C, mas desse c√≥digo d√° para ver duas coisas importantes: i) a fun√ß√£o y_log_y s√≥ faz a conta se o valor de \\(y\\) for diferente de zero, se n√£o, ela j√° retorna zero; ii) a fun√ß√£o y_log_y faz a conta \\(y\\log({y}/{\\mu})\\), ao inv√©s de apenas \\(y\\log({\\mu})\\). Isso acontece pois no R estamos minimizando o Desvio do modelo, dado por \\[ \\begin{aligned} &amp;D(\\mathbf y, \\mu) = 2[l(\\mathbf y|\\mathbf y) - l(\\mathbf y|(\\alpha, \\beta))]\\\\ &amp;=2\\left[\\sum_{i=1}^n y_i\\log(y_i) + (1-y_i)\\log(1-y_i)\\right. - \\\\ &amp;\\left. -\\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\right] \\\\ &amp;=2\\left[\\sum_{i=1}^n y_i\\log\\left(\\frac{y_i}{\\mu_i}\\right) + (1-y_i)\\log\\left(\\frac{1-y_i}{1-\\mu_i}\\right)\\right]. \\end{aligned} \\] Essa √© a formula√ß√£o usual na literatura de GLMs, que apresenta uma s√©rie de propriedades estat√≠sticas. Minimizar o desvio equivale a maximizar a verossimilhan√ßa. Ser√° que isso ajuda nos problemas num√©ricos? Vamos ver: Iter: 01, alpha=NaN, beta1=NaN, beta2=NaN Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Hmm, parece que n√£o. Se olharmos mais atentamente para a fun√ß√£o desvio, como \\(y\\) pode assumir apenas os valores zero ou um, √© poss√≠vel observar que a conta √© equivalente √† perda calculada anteriormente. Possivelmente o problema aqui √© que o tensorflow n√£o trabalha muito bem com essas condi√ß√µes (tf$where) na perda, e isso d√° problemas na hora de calcular o gradiente. Essa fun√ß√£o do R simplesmente n√£o resolve o problema inicial. Melhor olhar o que o tensorflow faz! 5.0.4 A binary cross-entropy no Tensorflow Eu escondi de voc√™s, mas o tensorflow j√° tem a fun√ß√£o de perda implementada: tf$nn$sigmoid_cross_entropy_with_logits. Ela j√° assume que a fun√ß√£o de liga√ß√£o √© log√≠stica, por isso o sigmoid_ no in√≠cio. Traduzindo livremente o help da fun√ß√£o, temos o seguinte (z=\\(y\\) e x=\\(\\eta = \\alpha + \\beta x\\)) z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x)) = (1 - z) * x + log(1 + exp(-x)) = x - x * z + log(1 + exp(-x)) Para \\(\\eta &lt; 0\\) para evitar problemas num√©ricos com \\(\\exp(-\\eta)\\), reformulamos para x - x * z + log(1 + exp(-x)) = log(exp(x)) - x * z + log(1 + exp(-x)) = - x * z + log(1 + exp(x)) Ent√£o, para garantir estabilidade e evitar problemas num√©ricos, a implementa√ß√£o usa essa formula√ß√£o equivalente max(x, 0) - x * z + log(1 + exp(-abs(x))) Beleza, vamos tentar! Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=1.276, beta1=2.495, beta2=3.608 Iter: 03, alpha=1.197, beta1=2.396, beta2=3.562 Iter: 04, alpha=1.164, beta1=2.335, beta2=3.489 Iter: 05, alpha=1.14, beta1=2.287, beta2=3.42 Iter: 06, alpha=1.12, beta1=2.245, beta2=3.358 Iter: 07, alpha=1.102, beta1=2.21, beta2=3.303 Iter: 08, alpha=1.086, beta1=2.178, beta2=3.256 Iter: 09, alpha=1.073, beta1=2.152, beta2=3.215 Iter: 10, alpha=1.061, beta1=2.129, beta2=3.18 Funcionou! :) 5.0.5 Wrap-up Tensorflow √© uma biblioteca interessante a ser explorada. √â poss√≠vel implementar uma regress√£o log√≠stica do zero em poucos passos. Precisamos tomar cuidado com problemas num√©ricos! No futuro, brincaremos tamb√©m com o algoritmo IWLS. Ser√° que ele roda mais r√°pido que a descida de gradiente? "],
["5-1-minimos-quadrados-com-restricoes-lineares.html", "5.1 M√≠nimos quadrados com restri√ß√µes lineares", " 5.1 M√≠nimos quadrados com restri√ß√µes lineares A caracter√≠stica mais importante de um modelo estat√≠stico √© a sua flexibilidade. Esse termo pode ser entendido de v√°rias formas, mas neste texto vamos considerar que um modelo √© flex√≠vel se ele explica de forma coerente uma ampla gama de fen√¥menos reais. Pensando assim, a regress√£o linear pode ser considerada um modelo flex√≠vel, j√° que muitas rela√ß√µes funcionais cotidianas s√£o do tipo \\(y = \\beta x\\). √â justamente por causa dessa flexibilidade que a boa e velha regress√£o de m√≠nimos quadrados √© t√£o usada, at√© mesmo aonde n√£o deveria. O seu uso √© t√£o indiscriminado que uma vez, em aula, um professor extraordinariamente admir√°vel me disse que ‚Äú90% dos problemas do mundo podem ser resolvidos com uma regress√£o linear‚Äù. Sendo bastante honesto, √© prov√°vel que o meu professor esteja certo, mas este texto n√£o √© sobre isso. Este √© um post sobre o que fazer quando a regress√£o linear simples n√£o basta. No que segue, vamos discutir uma pequena (e poderosa) extens√£o do modelo de regress√£o linear simples, mas antes de prosseguir para o problema propriamente dito (e sua implementa√ß√£o em R), vamos discutir da teoria que existe por tr√°s dele. 5.1.1 Regress√£o linear √© programa√ß√£o quadr√°tica Embora seja pouco enfatizado nos bacharelados de estat√≠stica, uma regress√£o linear pode ser formulada como um problema de programa√ß√£o quadr√°tica. Entrando nos detalhes, essa afirma√ß√£o deve-se a dois fatos: Existe uma teoria, que chama-se programa√ß√£o quadr√°tica, que soluciona problemas da forma \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big),\\] onde \\(x \\in \\mathbb{R}^p\\) e \\(Q\\) e \\(c\\) tem dimens√µes que fazem a conta acima ter sentido. A teoria ocupa-se desenvolvendo algoritmos exatos e aproximados para obter solu√ß√µes desses problemas, inclusive com generaliza√ß√µes: \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }Ax \\geq 0.\\] Uma regress√£o linear consiste em resolver \\[\\min_\\beta (Y - \\beta X)&#39;(Y-\\beta X),\\] que, com um pouco de √°lgebra, √© equivalente √† \\[ \\min_\\beta (-2Y&#39;X\\beta + \\beta&#39;X&#39;X\\beta).\\] Logo, tomando \\(Q = 2X&#39;X\\) e \\(c = \\frac{1}{2}X&#39;Y\\) tem-se que esse √© um problema de programa√ß√£o quadr√°tica, que por sua vez √© um problema convexo e que, segundo a teoria, tem uma √∫nica solu√ß√£o no ponto \\(\\beta = (X&#39;X)^{-1}X&#39;Y\\). 5.1.2 Uma regress√£o linear simples mais flex√≠vel Talvez o jeito mais simples de flexibilizar uma regress√£o linear no sentido mencionado no come√ßo desse texto √© restringir os seus par√¢metros. Em muitos contextos, esse √© o √∫nico jeito de colocar conhecimentos pr√©vios na modelagem2. Um caso bastante emblem√°tico aparece nas curvas de cr√©dito divulgadas pela ANBIMA3. L√°, ajusta-se um conjunto de curvas que depende de 6 par√¢metros e cada curva representa uma classifica√ß√£o de risco (que nem aquela em que o Brasil pode tomar downgrade4). Como os n√≠veis de risco est√£o ordenados, √© natural exigir que tamb√©m exista uma ordena√ß√£o entre as curvas. Sem entrar em detalhes, a ideia pode ser expressa assim: \\[\\beta_{AAA} &lt; \\beta_{AA} &lt; \\beta_{A} &lt; \\beta_{BBB} &lt; ...\\] O que √© que isso tem a ver com programa√ß√£o quadr√°tica? A resposta √© que a inequa√ß√£o acima pode ser escrita como \\(A\\beta \\geq 0\\), de tal forma j√° existe uma teoria para resolver uma regress√£o linear simples com restri√ß√µes desse tipo! Basta que ela seja vista como um problema de programa√ß√£o quadr√°tica. 5.1.3 O pacote quadprog Existe um pacote de R para quase tudo, ent√£o, como n√£o poderia deixar de ser, existe um pacote em R para resolver problemas do tipo: \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }Ax \\geq 0.\\] Para ilustrar o seu uso, vamos considerar um exemplo. Vamos simular um conjunto de dados em que \\(\\beta_5 = 0.31, \\beta_4 = 0.43, \\beta_3 = 1.31, \\beta_2 = 2.19, \\beta_1 = 2.29\\) s√£o os valores reais que precisamos estimar, considere que vale \\[Y \\approx \\beta_1X_1 + \\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5\\] e que o erro de regress√£o tem distribui√ß√£o normal. Se soubermos antecipadamente que valem as seguintes afirma√ß√µes \\[ \\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5 &gt; 0 \\text{ e } \\beta_1 &gt; \\beta_2 &gt; \\beta_3 &gt; \\beta_4 &gt; \\beta_5,\\] a minimiza√ß√£o de \\((Y-\\beta X)&#39;(Y-\\beta X)\\) pode ser resolvida usando a fun√ß√£o solve.QP. Tudo que precisamos fazer √© escrever o conjunto de inequa√ß√µes na forma \\(A\\beta \\geq 0\\). Mas isso √© bem f√°cil! Basta notar que as restri√ß√µes s√£o equivalentes √† \\[ \\left(\\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\\\ \\end{array}\\right) \\times \\left(\\begin{array}{c}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\end{array}\\right) \\geq 0.\\] Dessa forma, o problema est√° prontinho pra passar no moedor de carne, com uma √∫ltima ressalva. O problema resolvido no solve.QP √© \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }A&#39;x \\geq 0,\\] ent√£o vamos ter que tomar o cuidado de passar as nossas restri√ß√µes atrav√©s do transposto da matriz que obtivemos acima. Isso resultar√° na matriz \\(A\\). Para checar como valeu a pena todo esse esfor√ßo, d√° uma olhada na diferen√ßa entre as estimativas! Os pontinhos vermelhos s√£o as estimativas do modelo irrestrito, enquanto as barras s√£o as estimativas do modelo com restri√ß√µes. 5.1.4 Conclus√µes Regress√£o linear simples √© um problema de programa√ß√£o quadr√°tica. Algumas restri√ß√µes interessantes podem ser escritas na forma \\(B\\beta \\geq 0\\). Programa√ß√£o quadr√°tica resolve regress√£o linear simples com restri√ß√µes lineares. Se em algum dia voc√™ topar com um bicho desses, o quadprog pode resolver o problema pra voc√™. A menos que voc√™ seja uma pessoa razo√°vel bayesiano.‚Ü© http://www.anbima.com.br/data/files/05/43/3E/84/E12D7510E7FCF875262C16A8/metodologia-curvas_20credito_20131104_v2_1_.pdf‚Ü© http://economia.estadao.com.br/noticias/geral,agravamento-da-crise-politica-eleva-risco-de-rebaixamento-do-brasil-diz-sep,70001824274‚Ü© "],
["5-2-filtros-de-bloom-em-r.html", "5.2 Filtros de Bloom em R", " 5.2 Filtros de Bloom em R Filtro de Bloom √© um algoritmo muito interessante para testar se um elemento pertence a um conjunto. Ele √© considerado uma estrutura de dados probabil√≠stica, ou seja, o resultado pode n√£o estar correto com alguma probabilidade. Especificamente para o filtro de bloom, existe a possibilidade de falsos positivos mas n√£o de falsos negativos: o algoritmo pode dizer que o elemento pertence ao conjunto, mas na verdade n√£o pertencer, mas nunca dir√° que ele n√£o pertence sendo que ele pertence. Bloom Filters s√£o √∫teis em diversas situa√ß√µes, geralmente relacionadas ao ganho de velocidade e de espa√ßo que o seu uso pode trazer. Muitos sistemas de bancos de dados usam bloom filters para reduzir o n√∫mero de buscas no disco (ex. Cassandra). O Medium usa para evitar recomendar uma pa«µina que voc√™ j√° leu. Recentemente, encontraram at√© aplica√ß√µes para bloom filters em machine learning. Nesse post vamos implementar uma vers√£o simplificada, nada otimizada dos filtros de Bloom em R. Mas antes disso, vale a pena ler o verbete da Wikipedia sobre o assunto. Essencialmente, um filtro de bloom √© um vetor de TRUEs e FALSES de tamanho \\(m\\). Inicializamos esse vetor com FALSES. Em seguida para cada elemento do conjunto que voc√™ deseja representar pelo filtro, repetimos o seguinte processo: Hasheamos o elemento usando \\(k\\) fun√ß√µes de hash diferentes. Cada uma dessas fun√ß√µes indicar√° um elemento do vetor que deve ser marcado como TRUE. Armazenamos ent√£o esse vetor de bits. S√£o os valores de \\(m\\) e de \\(k\\) que controlam a probabilidade de falsos positivos. Veja como podemos criar uma fun√ß√£o em R para fazer essas opera√ß√µes. Essa fun√ß√£o inicializa o vetor de bits de tamanho \\(m\\) com FALSES e em seguida, para cada uma das \\(k\\) fun√ß√µes de hash (no caso apenas variamos a semente do hash MurMur32) e para cada elemento de x calculamos o elemento do vetor vec que deve se tornar TRUE. No final, ela retorna o vetor vec, onde armazenamos como atributos os par√¢metros usados na sua constru√ß√£o. library(digest) library(magrittr) criar_vetor_de_bits &lt;- function(x, m = 1000, k = 7){ vec &lt;- rep(FALSE, m) for (i in 1:k) { for (j in 1:length(x)) { hash &lt;- digest(x[j], algo = &quot;murmur32&quot;, serialize = FALSE, seed = i) %&gt;% Rmpfr::mpfr(base = 16) %% m %&gt;% as.integer() vec[hash + 1] &lt;- TRUE } } # armazenamos os par√¢metros usados na constru√ß√£o attributes(vec) &lt;- list(m = m, k= k) return(vec) } Dado um conjunto de strings, podemos criar o vetor de bits que o representa. vect &lt;- criar_vetor_de_bits(c(&quot;eu&quot;, &quot;pertenco&quot;, &quot;ao&quot;, &quot;conjunto&quot;, &quot;de&quot;, &quot;strings&quot;), m = 1000, k = 7) Agora vamos definir uma fun√ß√£o que verifica se uma string pertence ao conjunto, dada apenas a representa√ß√£o dos bits desse conjunto. Hasheamos o elemento que desejamos verificar a presen√ßa no conjunto com a primeira fun√ß√£o de hash. Se ela indicar um elemento do vetor que j√° est√° marcado com TRUE ent√£o continuamos, se n√£o, retorna FALSE indicando que o elemento n√£o pertence ao conjunto. Continuamos at√© acabarem as fun√ß√µes de hash ou at√© 1 FALSE ter sido retornado. verificar_presenca &lt;- function(x, vetor_de_bits){ k &lt;- attr(vetor_de_bits, &quot;k&quot;) m &lt;- attr(vetor_de_bits, &quot;m&quot;) for(i in 1:k){ hash &lt;- digest(x, algo = &quot;murmur32&quot;, serialize = FALSE, seed = i) %&gt;% Rmpfr::mpfr(base = 16) %% m %&gt;% as.integer() if(!vetor_de_bits[hash + 1]) { return(FALSE) } } return(TRUE) } verificar_presenca(&quot;nao&quot;, vect) verificar_presenca(&quot;eu&quot;, vect) verificar_presenca(&quot;abc&quot;, vect) Com m = 1000 e k = 7 n√£o consegui encontrar nenhum falso positivo, mas basta diminuir o tamanho de m e de k que encontraremos. No verbete da Wikipedia a conta est√° bonitinha mas de fato a probabilidade de falsos positivos pode ser estimada em fun√ß√£o dos par√¢metros \\(k\\) e \\(m\\) e \\(n\\) (tamanho do conjunto representado) √© dada por \\[(1 - e^{-kn/m})^k\\] No caso apresentado, a probabilidade de colis√£o √© de 1.991256e-10. "],
["5-3-modelando-a-variancia-da-normal.html", "5.3 Modelando a vari√¢ncia da normal", " 5.3 Modelando a vari√¢ncia da normal Verificar as suposi√ß√µes dos modelos √© muito importante quando fazemos infer√™ncia estat√≠stica. Em particular, a suposi√ß√£o de homocedasticidade5 dos modelos de regress√£o linear √© especialmente importante, pois modifica o c√°lculo de erros padr√£o, intervalos de confian√ßa e valores-p. Neste post, vou mostrar tr√™s pacotes do R que ajustam modelos da forma \\[ Y_i = \\beta_0 + \\sum_{k=1}^p\\beta_kx_{ik} + \\epsilon_i, \\ i = 1,\\ldots,n\\] \\[ \\epsilon_{i} \\sim \\textrm{N}(0,\\sigma_i), \\ i = 1,\\ldots,n \\ \\textrm{independentes, com }\\sigma_i^2 = \\alpha x_i^2. \\] Al√©m de mostrar como se faz, tamb√©m vou ilustrar o desempenho dos pacotes em um exemplo simulado. O modelo que gerar√° os dados do exemplo ter√° a seguinte forma funcional \\[ Y_i = \\beta x_i + \\epsilon_i, \\ i = 1,...n \\] \\[ \\epsilon_i \\sim N(0, \\sigma_i)\\text{ independentes, com }\\sigma_i = \\alpha\\sqrt{|x_i|},\\] e os par√¢metros do modelo ser√£o os valores \\(\\beta = 1\\) e \\(\\alpha = 4\\). A heterocedasticidade faz com que os pontos desenhem um cone ao redor da reta de regress√£o. 5.3.1 Usando o pacote gamlss Quando se ajusta um GAMLSS, voc√™ pode modelar os par√¢metros de loca√ß√£o, escala e curtose ao mesmo tempo em que escolhe a distribui√ß√£o dos dados dentre uma grande gama de op√ß√µes. Escolhendo a distribui√ß√£o normal e modelando apenas os par√¢metros de loca√ß√£o e escala, o GAMLSS ajusta modelos lineares normais com heterocedasticidade. No c√≥digo abaixo, o par√¢metro formula = Y ~ X-1 indica que a fun√ß√£o de regress√£o ser√° constitu√≠da por um preditor linear em X sem intercepto. J√° o par√¢metro sigma.formula = ~X2-1 indica que o desvio padr√£o ser√° modelado por um preditor linear em X2 (ou raiz de X), tamb√©m sem intercepto. FALSE GAMLSS-RS iteration 1: Global Deviance = 17872.29 FALSE GAMLSS-RS iteration 2: Global Deviance = 17870.67 FALSE GAMLSS-RS iteration 3: Global Deviance = 17870.67 Conforme descrito no sum√°rio abaixo, a estimativa de alfa est√° muito abaixo do valor simulado. FALSE ****************************************************************** FALSE Family: c(&quot;NO&quot;, &quot;Normal&quot;) FALSE FALSE Call: gamlss::gamlss(formula = Y ~ X - 1, sigma.formula = ~X2 - FALSE 1, family = NO(), data = dataset) FALSE FALSE Fitting method: RS() FALSE FALSE ------------------------------------------------------------------ FALSE Mu link function: identity FALSE Mu Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X 0.996942 0.005131 194.3 &lt;2e-16 *** FALSE --- FALSE Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 FALSE FALSE ------------------------------------------------------------------ FALSE Sigma link function: log FALSE Sigma Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X2 0.1791449 0.0009606 186.5 &lt;2e-16 *** FALSE --- FALSE Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 FALSE FALSE ------------------------------------------------------------------ FALSE No. of observations in the fit: 1000 FALSE Degrees of Freedom for the fit: 2 FALSE Residual Deg. of Freedom: 998 FALSE at cycle: 3 FALSE FALSE Global Deviance: 17870.67 FALSE AIC: 17874.67 FALSE SBC: 17884.49 FALSE ****************************************************************** 5.3.2 Usando o pacote dglm Quando se ajusta um Modelo Linear Generalizado Duplo (MLGD em portugu√™s e DGLM em ingl√™s), voc√™ tem uma flexibilidade parecida com a de um GAMLSS. Entretanto, voc√™ n√£o pode definir um modelo para a curtose e a classe de distribui√ß√µes dispon√≠vel √© bem menor. O c√≥digo abaixo, similar ao utilizado para ajustar o GAMLSS, ajusta um DGLM aos dados simulados. Novamente, verifica-se que o alfa estimado est√° muito distante do verdadeiro alfa. FALSE FALSE Call: dglm(formula = Y ~ X - 1, dformula = ~X2 - 1, family = gaussian, FALSE data = dataset, method = &quot;reml&quot;) FALSE FALSE Mean Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X 0.9969432 0.008981392 111.001 0 FALSE (Dispersion Parameters for gaussian family estimated as below ) FALSE FALSE Scaled Null Deviance: 27197.48 on 1000 degrees of freedom FALSE Scaled Residual Deviance: 3090.08 on 999 degrees of freedom FALSE FALSE Dispersion Coefficients: FALSE Estimate Std. Error z value Pr(&gt;|z|) FALSE X2 0.3577322 0.001166004 306.8019 0 FALSE (Dispersion parameter for Gamma family taken to be 2 ) FALSE FALSE Scaled Null Deviance: 1628.301 on 1000 degrees of freedom FALSE Scaled Residual Deviance: 6526.59 on 999 degrees of freedom FALSE FALSE Minus Twice the Log-Likelihood: 17870.76 FALSE Number of Alternating Iterations: 18 5.3.3 Usando o pacote rstan Stan √© uma linguagem de programa√ß√£o voltada para descrever e manipular objetos probabil√≠sticos, como por exemplo vari√°veis aleat√≥rias, processos estoc√°sticos, distribui√ß√µes de probabilidades etc. Essa linguagem foi projetada para tornar intuitivo e simples o ajuste de modelos estat√≠sticos. Em particular, a forma de descrever modelos bayesianos √© bem c√¥moda. O stan possui v√°rias interfaces para R. A mais b√°sica √© o rstan, que ser√° utilizada aqui. A principal fun√ß√£o desse pacote √© a fun√ß√£o rstan, que possui dois par√¢metros b√°sicos: um par√¢metro model_code =, que recebe um c√≥digo que descreve o modelo na linguagem stan. um par√¢metro data =, que recebe uma lista contendo os inputs do modelo, tais como dados coletados, par√¢metros de distribui√ß√µes a priori, etc. Embora esse seja o m√≠nimo que a fun√ß√£o precisa, tamb√©m podemos passar outras componentes. O par√¢metro verbose = FALSE faz com que a fun√ß√£o n√£o imprima nada enquanto roda e o par√¢metro control = list(...) passa uma lista de op√ß√µes de controle para o algoritmo de ajuste. O retorno da fun√ß√£o stan() √© um objeto do tipo stanfit, que pode ser sumarizado da mesma forma que outros modelos em R, utilizando a fun√ß√£o summary() e a fun√ß√£o plot(). O c√≥digo abaixo ilustra a aplica√ß√£o da fun√ß√£o stan() ao nosso exemplo. A figura abaixo descreve os intervalos de credibilidade obtidos para cada par√¢metro do modelo. O ponto central de cada intervalo representa as estimativas pontuais dos par√¢metros. Como se nota, as estimativas do modelo utilizando stan est√£o bem pr√≥ximas dos valores verdadeiros. Uma regress√£o linear √© homoced√°stica quando a variabilidade dos erros n√£o depende das covari√°veis do modelo.‚Ü© "],
["6-transformacao.html", "Cap√≠tulo 6 Transforma√ß√£o ", " Cap√≠tulo 6 Transforma√ß√£o "],
["6-1-arrumando-banco-de-dados-o-pacote-janitor.html", "6.1 Arrumando banco de dados: o pacote janitor", " 6.1 Arrumando banco de dados: o pacote janitor No primeiro post sobre arruma√ß√£o de base de dados, a gente viu como usar as fun√ß√µes do stringr para arrumar o nome das vari√°veis. Seguindo a dica do Julio, o quebrador de captchas, vamos falar do pacote janitor, que traz algumas fun√ß√µes para dar aquele trato nas BDs. Antes de mais nada, instale e carregue o pacote: 6.1.1 Arrumando o nome das vari√°veis Assim como no post passado, utilizaremos a base com informa√ß√µes de pacientes com arritmia card√≠aca, cujas vari√°veis selecionadas foram: FALSE [1] &quot;ID&quot; &quot;Sexo&quot; &quot;Nascimento&quot; FALSE [4] &quot;Idade&quot; &quot;Inclus√£o&quot; &quot;Cor&quot; FALSE [7] &quot;Peso&quot; &quot;Altura&quot; &quot;cintura&quot; FALSE [10] &quot;IMC&quot; &quot;Superf√≠cie corporal&quot; &quot;Tabagismo&quot; FALSE [13] &quot;cg.tabag (cig/dia)&quot; &quot;Alcool (dose/semana)&quot; &quot;Drogas il√≠citas&quot; FALSE [16] &quot;Cafe√≠na/dia&quot; &quot;Refrig/dia&quot; &quot;Sedentario&quot; FALSE [19] &quot;ativ. Fisica&quot; Os nomes t√™m letras mai√∫sculas, acentos, par√™nteses, pontos e barras, o que atrapalha na hora da programa√ß√£o. Para resolver esse problema, usamos a fun√ß√£o clean_names(). FALSE [1] &quot;id&quot; &quot;sexo&quot; &quot;nascimento&quot; FALSE [4] &quot;idade&quot; &quot;inclusao&quot; &quot;cor&quot; FALSE [7] &quot;peso&quot; &quot;altura&quot; &quot;cintura&quot; FALSE [10] &quot;imc&quot; &quot;superficie_corporal&quot; &quot;tabagismo&quot; FALSE [13] &quot;cg_tabag_cig_dia&quot; &quot;alcool_dose_semana&quot; &quot;drogas_ilicitas&quot; FALSE [16] &quot;cafeina_dia&quot; &quot;refrig_dia&quot; &quot;sedentario&quot; FALSE [19] &quot;ativ_fisica&quot; Veja que a fun√ß√£o removeu os par√™nteses, pontos e barras e substituiu os espa√ßos por _. No entanto, ela n√£o remove os acentos. Assim, podemos adicionar mais uma linha ao pipeline para chegar onde queremos. FALSE [1] &quot;id&quot; &quot;sexo&quot; &quot;nascimento&quot; FALSE [4] &quot;idade&quot; &quot;inclusao&quot; &quot;cor&quot; FALSE [7] &quot;peso&quot; &quot;altura&quot; &quot;cintura&quot; FALSE [10] &quot;imc&quot; &quot;superficie_corporal&quot; &quot;tabagismo&quot; FALSE [13] &quot;cg_tabag_cig_dia&quot; &quot;alcool_dose_semana&quot; &quot;drogas_ilicitas&quot; FALSE [16] &quot;cafeina_dia&quot; &quot;refrig_dia&quot; &quot;sedentario&quot; FALSE [19] &quot;ativ_fisica&quot; E para substituir na base. 6.1.2 Removendo linhas e colunas vazias Esse banco de dados tamb√©m tinha outro problema: linhas vazias. Na verdade, elas n√£o eram completamente vazias, pois havia algumas informa√ß√µes de identifica√ß√£o do paciente, mas nenhuma outra vari√°vel tinha sido computada. FALSE # A tibble: 1 x 19 FALSE id sexo nascimento idade inclusao cor peso FALSE &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; FALSE 1 3 &lt;NA&gt; NA NA NA &lt;NA&gt; NA FALSE # ‚Ä¶ with 12 more variables: altura &lt;dbl&gt;, cintura &lt;chr&gt;, imc &lt;dbl&gt;, FALSE # superficie_corporal &lt;chr&gt;, tabagismo &lt;chr&gt;, cg_tabag_cig_dia &lt;dbl&gt;, FALSE # alcool_dose_semana &lt;dbl&gt;, drogas_ilicitas &lt;chr&gt;, cafeina_dia &lt;dbl&gt;, FALSE # refrig_dia &lt;dbl&gt;, sedentario &lt;chr&gt;, ativ_fisica &lt;chr&gt; Essa foi a solu√ß√£o que eu pensei para resolver o problema utilizando a fun√ß√£o remove_empty_row(). FALSE # A tibble: 4 x 19 FALSE id sexo nascimento idade inclusao cor peso FALSE &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; FALSE 1 1 F 1964-01-31 00:00:00 41 2006-02-17 00:00:00 bran‚Ä¶ 75 FALSE 2 2 M 1959-01-28 00:00:00 45 2005-11-29 00:00:00 negra 71 FALSE 3 4 M 1957-09-13 00:00:00 50 2008-02-13 00:00:00 NT 80 FALSE 4 5 F 1938-02-06 00:00:00 71 2009-06-25 00:00:00 parda 56 FALSE # ‚Ä¶ with 12 more variables: altura &lt;dbl&gt;, cintura &lt;chr&gt;, imc &lt;dbl&gt;, FALSE # superficie_corporal &lt;chr&gt;, tabagismo &lt;chr&gt;, cg_tabag_cig_dia &lt;dbl&gt;, FALSE # alcool_dose_semana &lt;dbl&gt;, drogas_ilicitas &lt;chr&gt;, cafeina_dia &lt;dbl&gt;, FALSE # refrig_dia &lt;dbl&gt;, sedentario &lt;chr&gt;, ativ_fisica &lt;chr&gt; Eu precisei converter para data.frame primeiro porque n√£o √© poss√≠vel definir os nomes das linhas de uma tibble. Se a linha estivesse completamente vazia, bastaria usar diretamente a fun√ß√£o remove_empty_rows(). Equivalentemente para colunas, existe a fun√ß√£o remove_empty_cols(). 6.1.3 Identificando linhas duplicadas O pacote janitor possui uma fun√ß√£o para identificar entradas duplicadas numa base de dados: get_dupes(). Vamos criar uma base gen√©rica para test√°-la. FALSE # A tibble: 16 x 4 FALSE nome sobrenome dupe_count variavel_importante FALSE &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; FALSE 1 Athos Damiani 2 0.305 FALSE 2 Athos Damiani 2 -0.00796 FALSE 3 Athos Falbel 2 0.198 FALSE 4 Athos Falbel 2 -0.0443 FALSE 5 Athos Trecenti 2 -1.24 FALSE 6 Athos Trecenti 2 -0.729 FALSE 7 Daniel Damiani 2 0.335 FALSE 8 Daniel Damiani 2 3.07 FALSE 9 Fernando Trecenti 2 -0.190 FALSE 10 Fernando Trecenti 2 -0.868 FALSE 11 Julio Trecenti 4 -0.188 FALSE 12 Julio Trecenti 4 -1.39 FALSE 13 Julio Trecenti 4 0.528 FALSE 14 Julio Trecenti 4 1.06 FALSE 15 William Trecenti 2 -0.786 FALSE 16 William Trecenti 2 -0.505 Todas as linhas na tibble resultante representam uma combina√ß√£o de nome-sobrenome repetida. 6.1.4 Outras fun√ß√µes Por fim, o janitor tamb√©m tem fun√ß√µes equivalentes √† table() para produzir tabelas de frequ√™ncia: tabyl() - similar a table(), mas pipe-√°vel e com mais recursos. crosstab() - para tabelas de conting√™ncia. adorn_totals() - acrescenta o total das linhas ou colunas. adorn_crosstab() - deixa tabelas de conting√™ncia mais bonitas. FALSE cyl n percent FALSE 4 11 0.34375 FALSE 6 7 0.21875 FALSE 8 14 0.43750 FALSE cyl n percent FALSE 4 11 0.34375 FALSE 6 7 0.21875 FALSE 8 14 0.43750 FALSE Total 32 1.00000 FALSE cyl 0 1 FALSE 4 3 8 FALSE 6 4 3 FALSE 8 12 2 FALSE cyl 0 1 FALSE 1 4 27.3% (3) 72.7% (8) FALSE 2 6 57.1% (4) 42.9% (3) FALSE 3 8 85.7% (12) 14.3% (2) √â isso! Espero que essas dicas e o pacote janitor ajudem a agilizar as suas an√°lises :) "],
["6-2-skimr-estatisticas-basicas-com.html", "6.2 Skimr: estat√≠sticas b√°sicas com ‚ù§Ô∏è", " 6.2 Skimr: estat√≠sticas b√°sicas com ‚ù§Ô∏è Entre os dias 25 e 27 de maio aconteceu a ROpenSci Unconf 2017. O encontro reuniu v√°rios pop stars da comunidade R como Hadley Wickham, Joe Cheng (criador do shiny), Jeroen Ooms (criador do OpenCPU e autor de v√°rios pacotes bacanas), Jenny Bryan (autora de v√°rios pacotes bacanas como googlesheets), v√°rias pessoas do #R-Ladies e muito mais. Uma coisa muito legal dessa confer√™ncia √© que ela funcionou como uma hackathon. Foi criada uma nova organiza√ß√£o no github chamada ROpenSci Labs, e os presentes simplesmente come√ßaram a subir pacotes fant√°sticos l√° dentro. Recomendo muito dar uma olhada. Dentre os pacotes que olhei, o que mais me chamou aten√ß√£o foi o skimr e por isso estou fazendo esse post! O prop√≥sito do skimr √© simples: fazer algumas estat√≠sticas b√°sicas univariadas de uma base de dados. O skimr ainda n√£o est√° no CRAN, ent√£o para instalar recomendamos utilizar o devtools para instalar direto do GitHub, conforme c√≥digo abaixo. Note que tamb√©m ser√° necess√°rio instalar o pacote colformat do Hadley. A fun√ß√£o skim() calcula estat√≠sticas b√°sicas das vari√°veis e imprime no seu console. Note que a fun√ß√£o separa estat√≠sticas para vari√°veis num√©ricas ou fatores. FALSE Skim summary statistics FALSE n obs: 150 FALSE n variables: 5 FALSE FALSE ‚îÄ‚îÄ Variable type:factor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FALSE variable missing complete n n_unique top_counts FALSE Species 0 150 150 3 set: 50, ver: 50, vir: 50, NA: 0 FALSE ordered FALSE FALSE FALSE FALSE ‚îÄ‚îÄ Variable type:numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FALSE variable missing complete n mean sd p0 p25 p50 p75 p100 FALSE Petal.Length 0 150 150 3.76 1.77 1 1.6 4.35 5.1 6.9 FALSE Petal.Width 0 150 150 1.2 0.76 0.1 0.3 1.3 1.8 2.5 FALSE Sepal.Length 0 150 150 5.84 0.83 4.3 5.1 5.8 6.4 7.9 FALSE Sepal.Width 0 150 150 3.06 0.44 2 2.8 3 3.3 4.4 FALSE hist FALSE ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ FALSE ‚ñá‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ FALSE ‚ñÇ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ FALSE ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ E tem mais! O mais legal do skimr √© que ele usa a fun√ß√£o colformat::spark_bar() para desenhar histogramas direto no seu console! Tabela 5.1: HISTOGRAMA NA TABELA PORQUE SIM! variable type stat level value formatted Sepal.Length numeric hist .all NA ‚ñÇ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ Sepal.Width numeric hist .all NA ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ Petal.Length numeric hist .all NA ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ Petal.Width numeric hist .all NA ‚ñá‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ O skimr tamb√©m possui padr√µes de estat√≠sticas b√°sicas para cada tipo de vari√°vel. Voc√™ pode checar esses tipos com show_skimmers(): Tabela 5.2: Estat√≠sticas b√°sicas para cada tipo de vari√°vel. tipo stats AsIs missing, complete, n, n_unique, min_length, max_length character missing, complete, n, min, max, empty, n_unique complex missing, complete, n date missing, complete, n, min, max, median, n_unique Date missing, complete, n, min, max, median, n_unique difftime missing, complete, n, min, max, median, n_unique factor missing, complete, n, n_unique, top_counts, ordered integer missing, complete, n, mean, sd, p0, p25, p50, p75, p100, hist list missing, complete, n, n_unique, min_length, median_length, max_length logical missing, complete, n, mean, count numeric missing, complete, n, mean, sd, p0, p25, p50, p75, p100, hist POSIXct missing, complete, n, min, max, median, n_unique ts missing, complete, n, start, end, frequency, deltat, mean, sd, min, max, median, line_graph 6.2.1 Criando suas pr√≥prias fun√ß√µes Voc√™ tamb√©m pode usar fun√ß√µes pr√≥prias com o skimr. Por exemplo, digamos que voc√™ queira calcular o coeficiente de varia√ß√£o. Primeiro, adicione sua fun√ß√£o dentro de uma lista: e depois aplique a fun√ß√£o skim_with(): E pronto! Agora voc√™ pode rodar skim() novamente: Tabela 5.3: Histograma e coeficiente de varia√ß√£o. variable type stat level value formatted Sepal.Length numeric hist .all NA ‚ñÇ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ Sepal.Length numeric cv .all 0.1417113 0.14 Sepal.Width numeric hist .all NA ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ Sepal.Width numeric cv .all 0.1425642 0.14 Petal.Length numeric hist .all NA ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ Petal.Length numeric cv .all 0.4697441 0.47 Petal.Width numeric hist .all NA ‚ñá‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ Petal.Width numeric cv .all 0.6355511 0.64 Para retornar ao skim() padr√£o, rode skim_with_defaults(). 6.2.2 Wrap up Instale usando devtools::install_github() Rode a fun√ß√£o skim(). Use dplyr::filter() para filtrar as estat√≠sticas de interesse. Voc√™ pode adicionar suas pr√≥prias estat√≠sticas com skim_with(). Acompanhe a evolu√ß√£o do skimr nesta p√°gina. O pacote ainda vai evoluir muito e n√£o duvido nada que seja um bom candidado a entrar no tidyverse. O que voc√™s acham? Escrevam nos coment√°rios! √â isso. Happy coding ;) "],
["7-reflexoes.html", "Cap√≠tulo 7 Reflex√µes ", " Cap√≠tulo 7 Reflex√µes "],
["7-1-manifesto-tidy.html", "7.1 Manifesto tidy", " 7.1 Manifesto tidy Autor: Daniel Dificuldade baixa program O manifesto das ferramentas tidy do Hadley Wickham √© um dos documentos mais importantes sobre R dos √∫ltimos tempos. Esse documento formaliza uma s√©rie de princ√≠pios que norteiam o desenvolvimento dotidyverse. O tidyverse √© um conjunto de pacotes que, por compartilharem esses princ√≠pios do manifesto tidy, podem ser utilizados naturalmente em conjunto. Pode-se dizer que existe o R antes do tidyverse e o R depois do tidyverse. A linguagem mudou muito, a comunidade abra√ßou fortemente o uso desses princ√≠pios e tem muita gente criando pacotes para conversar uns com os outros dessa forma. No entanto, usar a filosofia tidy n√£o √© a √∫nica forma de fazer pacotes do R, existem muitos pacotes excelentes que n√£o utilizam essa filosofia. Como o pr√≥prio texto diz ‚ÄúO contr√°rio de tidyverse n√£o √© o messyverse, e sim muitos outros universos de pacotes interconectados.‚Äù. Os princ√≠pios fundamentais do tidyverse s√£o: Reutilizar estruturas de dados existentes. Organizar fun√ß√µes simples usando o pipe. Aderir √† programa√ß√£o funcional. Projetado para ser usado por seres humanos. No texto do manifesto tidy cada um dos lemas √© descrito de forma detalhada. Aqui, selecionei os aspectos que achei mais importante de cada um deles. 7.1.1 Reutilizar estruturas de dados existentes Quando poss√≠vel, √© melhor utilizar estruturas de dados comuns do que criar uma estrutura espec√≠fica para o seu pacote. Geralmente, √© melhor reutilizar uma estrutura existente mesmo que ela n√£o se encaixe perfeitamente. 7.1.2 Organizar fun√ß√µes simples usando o pipe Fa√ßa com que suas fun√ß√µes sejam o mais simples poss√≠veis. Uma fun√ß√£o deve poder ser descrita com apenas uma senten√ßa. A sua fun√ß√£o deve fazer uma transforma√ß√£o no estilo copy-on-modify ou ter um efeito colateral. Nunca os dois. O nome das fun√ß√µes devem ser verbos. Exceto quando as fun√ß√µes do pacote usam sempre o mesmo verbo. Ex: adicionar ou modificar. 7.1.3 Aderir √† programa√ß√£o funcional O R √© uma linguagem de programa√ß√£o funcional, n√£o lute contra isso. 7.1.4 Projetado para ser usado por seres humanos Desenvolva o seu pacote para ser usado por humanos. Foque em ter uma API clara para que voc√™ escreva o c√≥digo de maneira intuitiva e r√°pida. Efici√™ncia dos algoritmos √© uma preocupa√ß√£o secund√°ria, pois gastamos mais tempo escrevendo o c√≥digo do que executando. Esses princ√≠pios s√£o bem gerais, mas ajudam bastante a tomar decis√µes quando estamos escrevendo o nosso c√≥digo. Para finalizar, clique aqui e veja uma busca no Github por ‚Äútidy‚Äù em reposit√≥rios de R. S√£o mais de 3000 resultados, quase todos seguindo essa filosofia e estendendo o universo arrumado. "],
["7-2-eu-a-estatistica-e-a-programacao.html", "7.2 Eu, a Estat√≠stica e a programa√ß√£o", " 7.2 Eu, a Estat√≠stica e a programa√ß√£o Autor: William Dificuldade baixa program ‚Äî N√£o sabia que nessa cidade a cada 20 minutos atropelam um homem? ‚Äî Nossa! E como est√° o coitado? O epis√≥dio ‚ÄúEstat√≠sticas‚Äù do Chaves foi o meu primeiro contato com o conceito de Estat√≠stica (pelo menos que eu possa me lembrar). Claro que naquela √©poca, com 5 ou 6 anos, eu nunca imaginaria que seria essa a minha profiss√£o. Assim como o Quico e o Chaves, eu n√£o fazia muita ideia do que as ‚Äúsenhoras estat√≠sticas‚Äù&quot; eram e continuei sem saber de fato at√© entrar na gradua√ß√£o, em 2007. Reassistindo o epis√≥dio, depois de mais de dez anos estudando a disciplina, me identifiquei bastante com a dificuldade que a Dona Florinda e o Professor Girafales t√™m para explicar o que s√£o estat√≠sticas, o que antes via apenas como uma escada para as piadas que constroem a cena. Quando saio da minha bolha de colegas de faculdade e trabalho, percebo o quanto conceitos b√°sicos de probabilidade e estat√≠stica s√£o desconhecidos pela popula√ß√£o, mesmo aqueles presentes no dia a dia. Recentemente, lendo um coment√°rio de um radialista sobre a derrota do S√£o Paulo para o Coritiba, na rodada 18 do Campeonato Brasileiro, uma frase me chamou aten√ß√£o. ‚ÄúAo iniciar o jogo dessa quinta √† noite, o Tricolor, de acordo com as estat√≠sticas, tinha 1,78% de chances de vencer o Coritiba.‚Äù A tese do radialista √© que a probabilidade dos quatro grandes times de S√£o Paulo vencerem na mesma rodada do campeonato √© de 1,78%, a frequ√™ncia relativa desse evento na era de pontos corridos do Campeonato Brasileiro. Como o S√£o Paulo foi o √∫ltimo grande a jogar e os outros tr√™s j√° haviam vencido, o pobre tricolor paulista teve suas chances reduzidas pelas estat√≠sticas e acabou perdendo o jogo. Essa interpreta√ß√£o com certeza pode ser refutada por v√°rios motivos, mas o que mais me incomodou foi o desconhecimento de probabilidade condicional, ou simplesmente como novas informa√ß√µes modificam as probabilidades dos eventos. Encucado, eu deixei uma resposta, cuja parte central √© essa: Mesmo se consider√°ssemos que a probabilidade dos 4 grandes de SP ganharem numa rodada n√£o dependesse de fatores como a fase dos times, os advers√°rios, o momento do campeonato etc., esse n√∫mero, 1,78%, seria a probabilidade dos quatro ganharem antes da rodada come√ßar. Dado que j√° sabemos que os outros tr√™s ganharam, e considerando que o resultado desses jogos n√£o influenciam o jogo do SP, a probabilidade do evento em quest√£o ocorrer passa a ser apenas a probabilidade do SP ganhar o jogo dele. Em seguida, recebi alguns coment√°rios de outros torcedores dizendo (jocosamente) que n√£o tinham entendido nada do que escrevi. Comecei ent√£o a refletir sobre o assunto, pensando no quanto a minha explica√ß√£o poderia estar confusa e de que forma poderia ter explicado melhor, no quanto as pessoas n√£o costumam se esfor√ßar para entender temas que elas n√£o dominam e no quanto a falta de uma base matem√°tica adequada atrapalha nessas horas. Eu acredito que a Probabilidade e a Estat√≠stica s√£o v√≠timas da onda do ‚Äú√© legal odiar Matem√°tica‚Äù, que muitas pessoas se orgulham de surfar. Crian√ßas saem da escola com um conhecimento superficial dessas disciplinas (quando muito!), achando que √© tudo uma quest√£o de jogar dados, calcular m√©dias e fazer gr√°ficos. Comunicadores sofrem para interpretar os n√∫meros de uma pesquisa e pesquisadores encaram a an√°lise estat√≠stica como o grande vil√£o que os separa da publica√ß√£o. Felizmente, esse comportamento vem mudando, mesmo que a passos lentos. Profissionais est√£o buscando cursos de data science e programa√ß√£o, empresas est√£o promovendo cursos para qualificarem seus funcion√°rios e o mercado para estat√≠sticos continua um c√©u estrelado, tanto para analistas e programadores quanto para educadores. Eu vejo essa mudan√ßa, e as pessoas ao meu redor tamb√©m a veem. Mas o exemplo que citei acima me faz acreditar que preciso espiar fora da minha bolha. Por isso, vou come√ßar uma pequena s√©rie de posts, dando a minha opini√£o sobre algumas coisas que orbitam a educa√ß√£o estat√≠stica e a programa√ß√£o, com o objetivo de gerar reflex√£o e discuss√£o sobre o assunto. A Estat√≠stica vem crescendo como carreira, o estat√≠stico vem se tornando cada vez mais protagonista, e vejo esse momento como o ideal para melhorarmos a educa√ß√£o da nossa disciplina. Dividirei o texto nos seguintes t√≥picos: Por que amar a Estat√≠stica? Preconceitos no aprendizado Estat√≠stica e programa√ß√£o Espero que esses posts possam contribuir para mostrarmos para mais gente a import√¢ncia da Estat√≠stica e da Computa√ß√£o e por que amamos tanto trabalhar com essas ci√™ncias. 7.2.1 Por que amar a estat√≠stica? Escolher uma profiss√£o, para quem tem esse privil√©gio, √© uma das decis√µes mais importantes das nossas vidas. Aos 17, 18 anos, a imaturidade, o pouco auto-conhecimento e a falta de informa√ß√£o sobre as alternativas podem nos desviar da op√ß√£o que nos vestiria melhor, um erro que muitas vezes nunca ser√° reparado. √Äs vezes, eu me pergunto o que levou amigos e conhecido a escolherem suas profiss√µes na hora do vestibular. No meu caso, eu quase segui um caminho da ‚Äúprofiss√µes da moda‚Äù. O que me impediu de prestar Administra√ß√£o foi descobrir, na hora da inscri√ß√£o, que era uma carreira da √°rea de Humanas, n√£o Exatas. Sim, eu era bem perdido. Na √©poca, a segunda fase da FUVEST era diferente para cada √°rea, e eu n√£o tinha perspectiva nenhuma de ir bem se tivesse que fazer uma prova dissertativa de Hist√≥ria e Geografia em vez de Matem√°tica e F√≠sica, disciplinas que eu dominava muito mais. Por isso, ap√≥s uma (muito breve) pesquisa na internet, fui convencido a prestar Estat√≠stica, e o que me convenceu foi a frase ‚Äú[‚Ä¶] envolve bastante matem√°tica e o mercado de trabalho √© muito bom‚Äù. Foi baseado nisso que eu tomei uma das decis√µes mais importantes da minha vida e era basicamente tudo o que eu sabia sobre a carreira quando comecei a gradua√ß√£o. Prestar vestibular para Estat√≠stica foi um tiro no escuro t√£o certeiro que √†s vezes me pego pensando em destino e esoterismos desse tipo. Durante a gradua√ß√£o, conheci pessoas que n√£o tiveram a mesma sorte e acabaram desistindo nos primeiros semestres, que s√£o bem pesados na matem√°tica. A primeira parte da informa√ß√£o que eu tinha sobre realmente estava certa, e o curso de Estat√≠stica pode assustar quem n√£o estiver na pegada de provar v√°rios teoremas. Mas, neste post, n√£o quero falar sobre as dificuldades da escalada, mas sim sobre a vista ao se chegar ao topo. Conforme fui conhecendo a Estat√≠stica, eu descobri que ela √© a profiss√£o mais nerd que existe6. Eu sustento essa opini√£o porque a melhor defini√ß√£o de nerd que j√° escutei √© ‚Äúpessoa ama aprender‚Äù e, gra√ßas √† Estat√≠stica, tenho a oportunidade de estudar muita coisa diferente. Nesses dez anos como estat√≠stico, j√° fiz an√°lises na √°rea de engenharia, finan√ßas, educa√ß√£o, jornalismo, zoologia, farm√°cia, fisioterapia, medicina, psicologia, odontologia, educa√ß√£o f√≠sica‚Ä¶ e essas s√£o apenas as que eu lembrei de cabe√ßa. Estat√≠stica √© parte essencial do m√©todo cient√≠fico e est√° presente em todas as ci√™ncias. Pegar trabalhos novos para um estat√≠stico nerd √© extremamente motivante, porque n√£o √© apenas uma troca de tempo por dinheiro, √© uma √≥tima chance para aprender coisas novas. A Estat√≠stica te estimula a ser curioso e criativo, e isso √© o que eu mais amo nela. Outra coisa para se amar √© o mercado de trabalho. A segunda parte da informa√ß√£o que eu tinha tamb√©m estava correta: o mercado de trabalho para o estat√≠stico √© excelente! N√£o s√≥ pelo n√∫mero de oportunidades, mas pela gama de lugares diferentes onde somos necess√°rios. N√£o vou listar aqui porque √© praticamente qualquer √°rea. E sobre sal√°rios, como diria um professor do IME, d√° para alimentar fam√≠lias. Apesar de ter sido um dos poucos dos meus colegas a n√£o mergulhar de cabe√ßa no mercado, j√° tive duas experi√™ncias. A primeira foi como estagi√°rio em um banco, onde aprendi bastante sobre o que eu n√£o queria fazer na vida. Tudo o que eu fazia era rodar modelos pr√©-estabelecidos para gerar relat√≥rios pr√©-formatados. Tinha aprendido tanta coisa legal na gradua√ß√£o e n√£o podia usar nada, o que me fazia sentir como um p√°ssaro engaiolado. A segunda foi no Instituto Butantan, onde eu era o √∫nico estat√≠stico ao lado de v√°rios bi√≥logos, farmac√™uticos e veterin√°rios. Foi uma √≥tima experi√™ncia, na qual conheci muita gente bacana e aprendi muita coisa de biologia, farm√¢cia e controle de qualidade. Trabalhar com pessoas diferentes de voc√™, com outras formas de pensar, √© outra parte legal de ser estat√≠stico. O pessoal do Butantan me ensinou bastante, principalmente sobre como a ci√™ncia e a pesquisa funcionam na pr√°tica. Al√©m disso, foi l√° que nasceu o meu interesse em ensinar Estat√≠stica. Bom, essa foi uma parte da hist√≥ria de como eu me apaixonei pela Estat√≠stica. Talvez eu n√£o tenha acrescentado nada se voc√™ j√° compartilha desse sentimento, mas espero que esse texto chegue a pessoas que ainda estejam escolhendo sua profiss√£o e jogue luz sobre essa alternativa. Essa √© a hora de mudarmos gr√°ficos como esse. Resumindo: Estat√≠stica √© a profiss√£o para quem gosta de aprender. Um bom estat√≠stico no mercado √© uma crian√ßa com cart√£o de cr√©dito numa loja de brinquedos. No pr√≥ximo post desta s√©rie, vou levantar um pouco de pol√™mica desabafando sobre alguns preconceitos de aprendizagem. At√© breve! 7.2.2 Preconceitos no aprendizado Volta e meia eu escuto as famosas frases FALSE [1] &quot;Eu sou de Humanas&quot; &quot;Eu sou de Exatas&quot; &quot;Eu sou de Biol√≥gicas&quot; de algu√©m tentando justificar por que n√£o vai fazer alguma coisa. Muitas vezes, n√£o passa de uma brincadeira na hora de dividir a conta do bar. Muitas outras, me soa como uma desculpa pronta para n√£o encarar problemas complicados. Para mim, todo aprendizado √© dif√≠cil, n√£o acho que existe conhecimento de gra√ßa, ent√£o realmente importa se ele √© de Humanas, Exatas ou Biol√≥gicas? A divis√£o do conhecimento nessas tr√™s grandes √°reas tem a sua import√¢ncia organizacional, mas acaba motivando muita gente a criar limita√ß√µes que n√£o existem de verdade. Por que algu√©m de Exatas n√£o conseguiria assimilar as ideias de um texto filos√≥fico? Ou por que algu√©m de Biol√≥gicas n√£o conseguiria aprender C√°lculo? Acredito que cada um de n√≥s tem afinidade por uma das √°reas e maior facilidade em estudar um t√≥pico ou outro. Normal. Mas fico triste quando vejo pessoas inteligentes se diminuindo ao se declararem incapazes de aprender outras compet√™ncias que n√£o a delas. Sei que essa incapacidade n√£o existe e enxergo apenas como uma forma sofisticada de dizer ‚ÄúEstou com pregui√ßa‚Äù. Uma das belezas da Estat√≠stica √© nos fazer perder esse preconceito. Por mais que tenhamos nossos gostos, descobrimos que n√£o estamos presos ao dom√≠nio de apenas uma √°rea. N√≥s trabalhamos com pessoas que pensam e aprendem de formas diferentes de nossa e constru√≠mos juntos pontes para trocarmos conhecimento. Ser estat√≠stico √© n√£o ter medo de estudar, seja l√° o que for. Trazendo a reflex√£o aqui para o nosso mundinho, j√° ouvi muitas vezes colegas dizendo, principalmente na Gradua√ß√£o, que n√£o usam o R porque ele √© dif√≠cil ou porque n√£o gostam de programar. A minha opini√£o sobre a primeira desculpa est√° nos par√°grafos acima. Sobre a segunda, vou discutir no pr√≥ximo e √∫ltimo post desta s√©rie: a rela√ß√£o entre Estat√≠stica e programa√ß√£o. Resumindo a √≥pera: sempre vamos apanhar aprendendo, e vamos apanhar mais ainda quando n√£o gostamos do que estamos estudando, mas cedo ou tarde, com a quantidade certa de esfor√ßo, o conhecimento d√° as caras. E no bar, na hora de dividir a conta, o problema n√£o √© voc√™ ser de Humanas. O problema √© a sua pregui√ßa. :D 7.2.3 Estat√≠stica e programa√ß√£o N√£o importa a √°rea de atua√ß√£o, a maior parte do dia do estat√≠stico √© atr√°s do computador. E desse tempo, a maior parte √© atr√°s de um (geralmente √∫nico) programa estat√≠stico. Os principais programas hoje em dia permitem a execu√ß√£o das etapas essenciais de uma an√°lise: intera√ß√£o com banco de dados, transforma√ß√£o, cria√ß√£o de visualiza√ß√µes e modelagem. Alguns v√£o al√©m e auxiliam na comunica√ß√£o dos resultados. Tamb√©m √© comum a exist√™ncia de ambientes de programa√ß√£o, mesmo quando o programa √© bem estruturado no point and click. Eu considero a programa√ß√£o primordial para um estat√≠stico. Ela nos d√° a liberdade para sermos criativos, para n√£o nos limitarmos em t√©cnicas que algu√©m criou e todo mundo usa. Para mim, um estat√≠stico que n√£o sabe/gosta de programar √© igual a um piloto que s√≥ dirige carro autom√°tico. √â por isso que o R √© uma ferramenta t√£o incr√≠vel para se trabalhar. Ele pega a sua m√£o no momento em que voc√™ recebe a base de dados, estando ela arrumada ou n√£o, e s√≥ solta depois da sua an√°lise estar devidamente divulgada. Para cada problema, o R te fornece todas as pe√ßas e te deixa montar do jeito que quiser. E mesmo quando uma pe√ßa n√£o existe, voc√™ mesmo pode cri√°-la ou pedir socorro para a comunidade mais que fant√°stica de erreiros pelo mundo. Claro que aprender a programar √© bem custoso. Para quem nunca foi familiar com a computa√ß√£o, vai ser um caminho bem tortuoso no in√≠cio. Mas como discutimos no √∫ltimo post, n√£o existe aprendizado de gra√ßa, e por mais que voc√™ n√£o goste de estudar programa√ß√£o, √© um investimento com retorno mais do que garantido. 7.2.4 Wrap-up Fazendo um resum√£o do que falamos at√© aqui, podemos enumerar os seguintes itens: A Estat√≠stica √© uma disciplina fant√°stica, principalmente para quem gosta de aprender, e o mercado est√° bombando. Aprender Estat√≠stica √© dif√≠cil, assim como todo conhecimento. O que vai limitar a sua capacidade de aprender √© o quanto voc√™ vai conseguir dominar a sua pregui√ßa de estudar. A Estat√≠stica e a programa√ß√£o andam lado a lado. O Estat√≠stico que sabe programar tem muito mais poder para resolver problemas complicados. O R √© o ambiente mais legal para trabalhar com Estat√≠stica. :D √â isso! Espero que possamos continuar discutindo o quanto √© legal trabalhar com Estat√≠stica e que cada vez mais pessoas se interessem por esse caminho dif√≠cil, mas recompensador. Se voc√™ ainda v√™ alguma conota√ß√£o negativa na palavra nerd, mande as minha lembran√ßas aos anos 90. üòâ‚Ü© "],
["7-3-o-fluxo-do-web-scraping.html", "7.3 O Fluxo do Web Scraping", " 7.3 O Fluxo do Web Scraping Autor: Caio Dificuldade m√©dia program Web scraping (ou raspagem web) n√£o √© nada mais que o ato de coletar dados da internet. Hoje em dia √© muito comum termos acesso r√°pido e f√°cil a qualquer conjunto de informa√ß√µes pela web, mas raramente esses dados est√£o estruturados e em uma forma de f√°cil obten√ß√£o pelo usu√°rio. Isso faz com que precisemos aprender a coletar esses dados por conta pr√≥pria. Neste post vou descrever o fluxo do web scraping, um passo a passo para explicar aos iniciantes como funciona a cria√ß√£o de um raspador. 7.3.1 O fluxo Caso voc√™ j√° tenha visto o fluxo da ci√™ncia de dados descrito por Hadley Wickham, o fluxo do web scraping vai ser bastante simples de entender. Todos os itens a seguir v√£o se basear neste diagrama: Cada verbo indica um fase do processo de raspar dados da internet. A caixa azulada no meio do diagrama denominada reprodu√ß√£o indica um procedimento iterativo que devemos repetir at√© que a coleta funcione, mas, de resto, o fluxo √© um processo linear. Nas pr√≥ximas se√ß√µes, vamos explorar um exemplo bem simples para entender como esses passos se dariam no mundo real: extrair os t√≠tulos de artigos da Wikip√©dia. 7.3.1.1 Identificar O primeiro passo do fluxo se chama identificar porque nele identificamos a informa√ß√£o que vamos coletar. Aqui precisamos entender bem qual √© a estrutura das p√°ginas que queremos raspar e tra√ßar um plano para extrair tudo que precisamos. No nosso exemplo, precisar√≠amos entrar em algumas p√°ginas da Wikip√©dia para entender se os t√≠tulos se comportam da mesma forma em todas. Como a Wikip√©dia √© um site organizado, todos os t√≠tulos s√£o criados da mesma forma em absolutamente todos os artigos. 7.3.1.2 Navegar Agora precisamos entender de onde vem o dado que queremos extrair. Esse passo pode ser extremamente simples, mas de vez em quando ele se tornara algo bastante complexo. Usando as ferramentas de desenvolvedor do nosso navegador, vamos navegar para encontrar a fonte dos dados. Sem entrar em muitos detalhes, poder√≠amos analisar o networking do navegador para entender as chamadas HTTP que s√£o feitas, poder√≠amos estudar os resultados das fun√ß√µes JavaScript invocadas pela p√°gina e assim por diante. No nosso caso, como escolhi um exemplo simples, precisamos apenas inspecionar o elemento do t√≠tulo e ver qual √© o seu XPath (basicamente o endere√ßo do elemento no HTML da p√°gina): //*[@id=&quot;firstHeading&quot;]. 7.3.1.3 Replicar Se tiv√©ssemos que fazer v√°rias requests HTTP para chegar at√© a informa√ß√£o que queremos, seria aqui em que tentar√≠amos replicar essas chamadas. Neste passo √© importante compreender absolutamente tudo que a p√°gina est√° fazendo para trazer o conte√∫do at√© voc√™, ent√£o √© necess√°rio analisar o seu networking a fim de entender tais requests e seus respectivos queries. No nosso caso, basta fazer uma chamada GET para obter a p√°gina do artigo desejado. Tamb√©m se faz necess√°rio salvar a p√°gina localmente para que possamos dar continuidade ao fluxo. url &lt;- &quot;https://en.wikipedia.org/wiki/R_(programming_language)&quot; httr::GET(url, httr::write_disk(&quot;~/Desktop/wiki.html&quot;)) 7.3.1.4 Parsear O anglicismo parsear vem do verbo to parse, que quer dizer algo como analisar ou estudar, mas que, no contexto do web scraping, significa extrair os dados desejados de um arquivo HTML. Aqui vamos usar a informa√ß√£o obtida no passo 2 para retirar do arquivo que chamei de wiki.html o t√≠tulo do artigo. &quot;~/Desktop/wiki.html&quot; %&gt;% xml2::read_html() %&gt;% rvest::html_node(xpath = &quot;//*[@id=&#39;firstHeading&#39;]&quot;) %&gt;% rvest::html_text() #&gt; [1] &quot;R (programming language)&quot; 7.3.1.5 Validar Se tivermos feito tudo certo at√© agora, validar os resultados ser√° uma tarefa simples. Precisamos apenas reproduzir o procedimento descrito at√© agora para algumas outras p√°ginas de modo verificar se estamos de fato extraindo corretamente tudo o que queremos. Caso encontremos algo de errado precisamos voltar ao passo 3, tentar replicar corretamente o comportamento do site e parsear os dados certos nas p√°ginas. 7.3.1.6 Iterar O √∫ltimo passo consiste em colocar o nosso scraper em produ√ß√£o. Aqui, ele j√° deve estar funcionando corretamente para todos os casos desejados e estar pronto para raspar todos os dados dos quais precisamos. Na maior parte dos casos isso consiste em encapsular o scraper em uma fun√ß√£o que recebe uma s√©rie de links e aplica o mesmo procedimento em cada um. Se quisermos aumentar a efici√™ncia desse processo, podemos paralelizar ou distribuir o nosso raspador. scraper &lt;- function(url, path) { httr::GET(url, httr::write_disk(path)) path %&gt;% xml2::read_html() %&gt;% rvest::html_node(xpath = &quot;//*[@id=&#39;firstHeading&#39;]&quot;) %&gt;% rvest::html_text() } purrr::map2_chr(links, paths, scraper) 7.3.2 Conclus√£o Fazer um scraper n√£o √© uma tarefa f√°cil, mas, se toda vez seguirmos um m√©todo consistente e robusto, podemos melhorar um pouco o nosso trabalho. O fluxo do web scraping tenta ser este m√©todo, englobando em passos simples e razoavelmente bem definidos essa arte que √© fazer raspadores web. "],
["7-4-tidy-data-teste-t-pareado-e-modelos-mistos.html", "7.4 Tidy Data, Teste t Pareado e Modelos Mistos", " 7.4 Tidy Data, Teste t Pareado e Modelos Mistos Autor: Daniel Dificuldade alta model O que teste \\(t\\)-pareado, modelos mistos e tidy data podem ter em comum? 7.4.1 Tidy Data Para come√ßar, vamos relemebrar o que √© tidy data para depois seguir ao ponto do post. Tidy data √© um conceito introduzido pelo Hadley Wickham neste paper. Esse paper √©, para mim, o melhor artigo do Hadley. A primeira frase da defini√ß√£o cita Tolstoi e diz: Like families, tidy datasets are all alike but every messy dataset is messy in its own way. ‚Äì Leo Tolstoi Essa frase resume a vida de qualquer um que trabalha ou j√° trabalhou com an√°lise de dados. O ponto mais importante do que significa tidy data tamb√©m est√° neste primeiro par√°grafo: s√£o datasets em que a estrutura dos dados est√° ligada com o seu significado. A forma padronizada √©: Cada vari√°vel √© uma coluna de uma tabela Cada observa√ß√£o √© uma linha de uma tabela Cada tipo de unidade observacional forma uma tabela O exemplo c√°ssico √© o seguinte. Primeiro vamos ver um banco de dados desarrumado. Pais Idh2015 Idh2014 Brasil 0.754 0.755 Argentina 0.827 0.836 Chile 0.847 0.832 Esse dataset est√° desarrumado pois existem duas colunas Idh2015 e Idh2014 que representam a mesma vari√°vel: IDH e uma vari√°vel impl√≠cita ANO, que tamb√©m aparece nesta duas colunas. A forma tidy de representar este dataset seria: Pais ano idh Brasil 2015 0.754 Argentina 2015 0.827 Chile 2015 0.847 Brasil 2014 0.755 Argentina 2014 0.836 Chile 2014 0.832 7.4.2 O que isso tem a ver com teste \\(t\\)-pareado e modelos mistos? Suponha que queremos inferir se houve alguma mudan√ßa na m√©dia do IDH de um ano para o outro. Ou seja, testar se a m√©dia do IDH de 2015 √© diferente da m√©dia do IDH de 2014. Vamos considerar um banco de dados simulado: Uma forma de fazer isso √© usar o teste \\(t\\)-pareado, ensinado nos cursos introdut√≥rios de estat√≠stica. Basicamente o que ele faz √© testar se a m√©dia da diferen√ßa entre o IDH2015 e o IDH 2014 √© diferente de zero. Isso √© diferente de um teste \\(t\\) usual, pois o teste \\(t\\)-pareado ajusta o seu c√°lculo da vari√¢ncia para considerar que existem duas fontes de incerteza, eventualmente correlacionadas. No R a forma mais natural de fazer isso √©: Note que o nosso banco de dados est√° desarrumado e mesmo assim foi muito simples fazer esse teste no R. Agora vamos arrumar o banco de dados. Agora para fazer o mesmo teste, poder√≠amos filtrar o banco de dados duas vezes, por exemplo: Paired t-test data: df$idh[df$ano == 2015] and df$idh[df$ano == 2014] t = 27.355, df = 49, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 0.09000554 0.10427822 sample estimates: mean of the differences 0.09714188 Mas a√≠ estamos voltando para a forma desarrumada para fazer o teste. Outra forma de fazer √© considerar essa compara√ß√£o de m√©dias como um problema de regress√£o em que a suposi√ß√£o independ√™ncia das observa√ß√µes n√£o √© v√°lida, uma vez que para cada pa√≠s, os IDHs de 2014 e de 2015 s√£o correlacionados. Vamos ajustar um modelo com efeitos aleat√≥rios para esse problema e comparar os resultados. Linear mixed-effects model fit by REML Data: df AIC BIC logLik -184.7518 -174.4119 96.37588 Random effects: Formula: ~1 | Pais (Intercept) Residual StdDev: 0.3009017 0.01775584 Fixed effects: idh ~ as.factor(ano) Value Std.Error DF t-value p-value (Intercept) 0.4840132 0.04262795 49 11.35436 0 as.factor(ano)2015 0.0971419 0.00355117 49 27.35491 0 Correlation: (Intr) as.factor(ano)2015 -0.042 Standardized Within-Group Residuals: Min Q1 Med Q3 Max -1.88877770 -0.44544521 -0.01239249 0.39934207 1.84475543 Number of Observations: 100 Number of Groups: 50 Estamos interessados em comparar a signific√¢ncia do efeito fixo da vari√°vel ano nesse modelo com a do teste \\(t\\)-pareado. Veja que no caso a estat√≠stica \\(t\\) do testes √© id√™ntica: 27.35. Vimos que a forma como os dados est√£o estruturados no seu banco de dados pode influenciar a opera√ß√£o utilizada para realizar a an√°lise. Se ele estivesse na forma desarrumada o mais natural seria aplicar um teste \\(t\\)-pareado, se ele estivesse em formado tidy o natural seria usar um modelo misto. Em seu paper, Hadley argumenta que a maioria dos softwares esperam que o seu banco de dados esteja arrumado no sentido de que cada vari√°vel √© uma coluna e cada observa√ß√£o √© uma linha. "],
["referencias.html", "Refer√™ncias", " Refer√™ncias "]
]
