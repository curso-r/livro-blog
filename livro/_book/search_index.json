[
["index.html", "Como faz no R Capítulo 1 Introdução ", " Como faz no R Curso-R 06 de março de 2019 Capítulo 1 Introdução "],
["1-1-objetivos.html", "1.1 Por quê ler esse livro", " 1.1 Por quê ler esse livro "],
["1-2-organizacao.html", "1.2 Organização", " 1.2 Organização "],
["2-analises.html", "Capítulo 2 Análises", " Capítulo 2 Análises "],
["3-tutoriais.html", "Capítulo 3 Tutoriais ", " Capítulo 3 Tutoriais "],
["3-1-por-que-usar-o.html", "3.1 Por que usar o %&gt;%", " 3.1 Por que usar o %&gt;% Provavelmente você já ouviu falar do operador pipe (%&gt;%). Muita gente acha que ele é uma sequência mágica de símbolos que muda completamente o visual do seu código, mas na verdade ele não passa de uma função como outra qualquer. Vamos explorar um pouco da história do pipe, como ele funciona e por que utilizá-lo. 3.1.1 Origem O conceito de pipe existe pelo menos desde os anos 1970. De acordo com seu criador, o operador foi concebido em “uma noite febril” e tinha o objetivo de simplificar comandos cujos resultados deveriam ser passados para outros comandos. ls | cat #&gt; Desktop #&gt; Documents #&gt; Downloads #&gt; Music #&gt; Pictures #&gt; Public #&gt; Templates #&gt; Videos Por essa descrição já conseguimos ter uma ideia de onde vem o seu nome: pipe em inglês significa “cano”, referindo-se ao transporte das saídas dos comandos. Em português o termo é traduzido como “canalização” ou “encadeamento”, mas no dia-a-dia é mais comum usar o termo em inglês. A partir daí o pipe tem aparecido nas mais diversas aplicações, desde HTML até o nosso tão querido R. Ele pode ter múltiplos disfarces, mas o seu objetivo é sempre o mesmo: transportar resultados. 3.1.2 Como funciona Em R o pipe tem uma cara meio estranha (%&gt;%), mas no fundo ele não passa de uma função infixa, ou seja, uma função que aparece entre os seus argumentos (como a + b ou a %in% b). Na verdade é por isso mesmo que ele tem porcentagens antes e depois: porque no R uma função infixa só pode ser declarada assim. Vamos começar demonstrando sua funcionalidade básica. Carregue o pacote magrittr e declare o pipe usando Ctrl + Shift + M. library(magrittr) `%&gt;%`(&quot;oi&quot;, print) #&gt; [1] &quot;oi&quot; Não ligue para os acentos graves em volta do pipe, o comando acima só serve para demonstrar que ele não é nada mais que uma função; perceba que o seu primeiro argumento (&quot;oi&quot;) virou a entrada do seu segundo argumento (print). &quot;oi&quot; %&gt;% print() #&gt; [1] &quot;oi&quot; Observe agora o comando abaixo. Queremos primeiro somar 3 a uma sequência de números e depois dividí-los por 2: mais_tres &lt;- function(x) { x + 3 } sobre_dois &lt;- function(x) { x / 2 } x &lt;- 1:3 sobre_dois(mais_tres(x)) #&gt; [1] 2.0 2.5 3.0 Perceba como fica difícil de entender o que está acontecendo primeiro? A linha relevante começa com a divisão por 2, depois vem a soma com 3 e, por fim, os valores de entrada. Nesse tipo de situação é mais legível usar a notação de composição de funções, com as funções sendo exibidas na ordem em que serão aplicadas: \\(f \\circ g\\). Isso pode ser realizado se tivermos uma função que passa o resultado do que está à sua esquerda para a função que está à sua direita… x %&gt;% mais_tres() %&gt;% sobre_dois() #&gt; [1] 2.0 2.5 3.0 No comando acima fica evidente que pegamos o objeto x, somamos 3 e dividimos por 2. Você pode já ter notado isso, mas a entrada (esquerda) de um pipe sempre é passada como o primeiro argumento agumento da sua saída (direita). Isso não impede que as funções utilizadas em uma sequência de pipes tenham outros argumentos. mais_n &lt;- function(x, n) { x + n } x %&gt;% mais_n(4) %&gt;% sobre_dois() #&gt; [1] 2.5 3.0 3.5 3.1.3 Vantagens A grande vantagem do pipe não é só enxergar quais funções são aplicadas primeiro, mas sim nos ajudar a programar pipelines (“encanamento” em inglês) de tratamentos de dados. library(dplyr) starwars %&gt;% mutate(bmi = mass/((height/100)^2)) %&gt;% select(name, bmi, species) %&gt;% group_by(species) %&gt;% summarise(bmi = mean(bmi)) #&gt; # A tibble: 38 x 2 #&gt; species bmi #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aleena 24.0 #&gt; 2 Besalisk 26.0 #&gt; 3 Cerean 20.9 #&gt; 4 Chagrian NA #&gt; 5 Clawdite 19.5 #&gt; 6 Droid NA #&gt; 7 Dug 31.9 #&gt; 8 Ewok 25.8 #&gt; 9 Geonosian 23.9 #&gt; 10 Gungan NA #&gt; # ... with 28 more rows Acima fica extremamente claro o que está acontecendo em cada passo da pipeline. Partindo da base starwars, primeiro transformamos, depois selecionamos, agrupamos e resumimos; em cada linha temos uma operação e elas são executadas em sequência. Isso não melhora só a legibilidade do código, mas também a sua debugabilidade! Se tivermos encontrado um bug na pipeline, basta executar linha a linha do encadeamento até que encontremos a linha problemática. Com o pipe podemos programar de forma mais compacta, legível e correta. Todos os exemplos acima envolvem passar a entrada do pipe como o primeiro argumento da função à direita, mas não é uma obrigatoriedade. Com um operador placeholder . podemos indicar exatamente onde deve ser colocado o valor que chega no pipe: y_menos_x &lt;- function(x, y) { y - x } x %&gt;% mais_tres() %&gt;% purrr::map2(4:6, ., y_menos_x) # [[1]] # [1] 0 # # [[2]] # [1] 0 # # [[3]] # [1] 0 3.1.4 Bônus Agora que você já sabe dos usos mais comuns do pipe, aqui está uma outra funcionalidade interessante: funções unárias. Se você estiver familiarizado com o pacote purrr, esse é um jeito bastante simples de criar funções descartáveis. m3_s2 &lt;- . %&gt;% mais_tres() %&gt;% sobre_dois() m3_s2(x) #&gt; [1] 2.0 2.5 3.0 Usando novamente o . definimos uma função que recebe apenas um argumento com uma sequência de aplicações de outras funções. 3.1.5 Conclusão O pipe não é apenas algo que deve ser usado pelos fãs do tidyverse. Ele é uma função extremamente útil que ajuda na legibilidade e programação de código, independentemente de quais pacotes utilizamos. Se quiser saber mais sobre o mundo do pipe, leia este post do Daniel sobre o Manifesto Tidy e o nosso tutorial mais aprofundado sobre o próprio pipe. "],
["3-2-o-que-e-um-grafico-estatistico.html", "3.2 O que é um gráfico estatístico?", " 3.2 O que é um gráfico estatístico? Os gráficos são técnicas de visualização de dados amplamente utilizadas em todas as áreas da pesquisa. A sua popularidade se deve à maneira como elucidam informações que estavam escondidas nas colunas do banco de dados, sendo que muitos deles podem ser compreendidos até mesmo por leigos no assunto que está sendo discutido. Mas será que podemos definir formalmente o que é um gráfico estatístico? Graças ao estatístico norte-americano Leland Wilkinson, a resposta é sim. Em 2005, Leland publicou o livro The Grammar of Graphics, uma fonte de princípios fundamentais para a construção de gráficos estatísticos. No livro, ele defende que um gráfico é o mapeamento dos dados a partir de atributos estéticos (posição, cor, forma, tamanho) e de objetos geométricos (pontos, linhas, barras, caixas). Simples assim. Além de responder a pergunta levantada nesse post, os conceitos de Leland tiveram outra grande importância para a visualização de dados. Alguns anos mais tarde, o seu trabalho inspirou Hadley Wickham a criar o pacote ggplot2, que enterrou com muitas pás de terra as funções gráficas do R base. Em A Layered Grammar of Graphics, Hadley sugeriu que os principais aspectos de um gráfico (dados, sistema de coordenadas, rótulos e anotações) podiam ser divididos em camadas, construídas uma a uma na elaboração do gráfico. Essa é a essência do ggplot2. No gráfico abaixo, temos informação de 32 carros com respeito a 4 variáveis: milhas por galão, tonelagem, transmissão e número de cilindros. O objeto geométrico escolhido para representar os dados foi o ponto. As posições dos pontos no eixo xy mapeia a associação entre a tonelagem e a quantidade de milhas por galão. A cor dos pontos mapeia o número de cilindros de cada carro, enquanto a forma dos pontos mapeia o tipo de transmissão. Observando o código, fica claro como cada linha/camada representa um aspecto diferente do gráfico. Os conceitos criados por Leland e Hadley defendem que essa estrutura pode ser utilizada para construir e entender qualquer tipo de gráfico, dando a eles, dessa maneira, a sua definição formal. ggplot(mtcars) + geom_point(aes(x = disp, y = mpg, shape = as.factor(am), color = cyl)) + labs(x = &quot;Tonelagem&quot;, y = &quot;Milhas por galão&quot;, shape = &quot;Transmissão&quot;, color = &quot;Cilindros&quot;) + scale_shape_discrete(labels = c(&quot;Automática&quot;,&quot;Manual&quot;)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) Por fim, é preciso frisar que, apesar de a gramática prover uma forte fundação para a construção de gráficos, ela não indica qual gráfico deve ser usado ou como ele deve parecer. Essas escolhas, fundamentadas na pergunta a ser respondida, nem sempre são triviais, e negligenciá-las pode gerar gráficos mal construídos e conclusões equivocadas. Cabe a nós, pesquisadores, desenvolver, aprimorar e divulgar as técnicas de visualização adequadas para cada tipo de variável, assim como apontar ou denunciar os usos incorretos e mal-intencionados. Mas, em um mundo cuja veracidade das notícias é cada vez menos importante, é papel de todos ter senso crítico para entender e julgar as informações trazidas por um gráfico. "],
["3-3-colando-textos-no-r.html", "3.3 Colando textos no R", " 3.3 Colando textos no R Uma tarefa muito comum no R é a de colar textos. As funções mais importantes para isso são paste() e sprintf(), que vêm com o pacote base. Nesse texto, vamos falar dessas duas funções e de um novo pacote do tidyverse, o glue. 3.3.1 paste() A função paste() recebe um conjunto indeterminado de objetos como argumento através do ... e vai colando os objetos passados elemento a elemento. Isso significa que se você passar dois vetores de tamanho n, a função paste() retornará um vetor de tamanho n sendo cada posição a colagem dos dois vetores nessa posição. Por padrão, a colagem é feita com um separador de espaço simples (o &quot; &quot;). Exemplo: paste(c(1, 2, 3), c(4, 5, 6)) FALSE [1] &quot;1 4&quot; &quot;2 5&quot; &quot;3 6&quot; É possível alterar o separador pelo argumento sep =. Um atalho útil para o separador vazio (&quot;&quot;) é a função paste0: paste0(c(1, 2, 3), c(4, 5, 6)) FALSE [1] &quot;14&quot; &quot;25&quot; &quot;36&quot; Algumas vezes nosso interesse não é juntar vetores elemento a elemento, mas sim passar um vetor e colar todos seus elementos. Isso é feito com o parâmetro collapse =: paste(c(1, 2, 3, 4, 5, 6), collapse = &#39;@&#39;) FALSE [1] &quot;1@2@3@4@5@6&quot; Se você passar mais de um vetor e mandar colapsar os elementos, o paste() vai primeiro colar e depois colapsar: paste(c(1, 2, 3), c(4, 5, 6), collapse = &#39;@&#39;) FALSE [1] &quot;1 4@2 5@3 6&quot; 3.3.1.1 Cuidado Tenha muito cuidado ao passar vetores com comprimentos diferentes no paste()! Assim como muitas funções do R, o paste() faz reciclagem, ou seja, ele copia os elementos do menor vetor até ele ficar com o comprimento do maior vetor1. O problema é que o paste() faz isso silenciosamente e não avisa se você inserir um vetor com comprimento que não é múltiplo dos demais. Veja que resultado bizarro: paste(5:9, 1:3, 4:5) FALSE [1] &quot;5 1 4&quot; &quot;6 2 5&quot; &quot;7 3 4&quot; &quot;8 1 5&quot; &quot;9 2 4&quot; Por essas e outras que dizemos que às vezes o R funciona bem demais… 3.3.2 sprintf() O sprintf() é similar ao printf do C. Primeiro escrevemos um texto com %s no lugar das coisas que queremos substituir. Depois colocamos esses objetos nos outros argumentos da função, na ordem em que eles aparecem no texto. sprintf(&#39;Aba%ste&#39;, &#39;ca&#39;) FALSE [1] &quot;Abacate&quot; Quando o argumento é um vetor, a função retorna um vetor com as substituições ponto a ponto. sprintf(&#39;Aba%ste&#39;, c(&#39;ca&#39;, &#39;ixas&#39;)) FALSE [1] &quot;Abacate&quot; &quot;Abaixaste&quot; Se o texto contém mais de um %s e os objetos correspondentes são vetores, o sprintf() tenta reciclar os vetores para ficarem do mesmo tamanho. Isso só funciona quando todos os objetos têm comprimentos que são múltiplos do comprimento do maior objeto. Por exemplo, isso funciona: sprintf(&#39;Aba%s%s&#39;, c(&#39;ca&#39;), c(&#39;xi&#39;, &#39;te&#39;)) # ca foi reciclado FALSE [1] &quot;Abacaxi&quot; &quot;Abacate&quot; Isso não funciona: sprintf(&#39;Aba%s%s&#39;, c(&#39;ca&#39;, &#39;ixaste&#39;), c(&#39;xi&#39;, &#39;te&#39;, &#39;.&#39;)) FALSE Error in sprintf(&quot;Aba%s%s&quot;, c(&quot;ca&quot;, &quot;ixaste&quot;), c(&quot;xi&quot;, &quot;te&quot;, &quot;.&quot;)): arguments cannot be recycled to the same length Nem sempre queremos substituir pedaços do nosso texto por outros textos. No lugar do %s, é possível colocar padrões para números, por exemplo. Eu uso bastante o %d, que recebe inteiros. Uma funcionalidade legal do %d é a possibilidade de adicionar zeros à esquerda quando um número não atinge certa quantidade de dígitos. Assim, quando ordenamos um vetor de textos que começa com números, a ordenação é a mesma da versão numérica do vetor. Exemplo: nums &lt;- 1:11 sort(as.character(nums)) # ordenado pela string: 10 vem antes de 2 FALSE [1] &quot;1&quot; &quot;10&quot; &quot;11&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; sort(sprintf(&#39;%02d&#39;, nums)) # ordenado pela string: 02 vem antes de 10 FALSE [1] &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;04&quot; &quot;05&quot; &quot;06&quot; &quot;07&quot; &quot;08&quot; &quot;09&quot; &quot;10&quot; &quot;11&quot; 3.3.3 glue O glue é um pacote recente. Sua primeira aparição no GitHub foi em 23/12/2016. Isso significa que é provável que algumas estejam em constante desenvolvimento, mas isso não nos impede de aproveitar o que a ferramenta tem de bom. A função glue() é uma generalização do sprintf() que permite chamar objetos do R diretamente ao invés de utilizar o %s. Os objetos podem estar no global environment ou descritos por meio de objetos nomeados nos argumentos do glue(). Basta inserir os objetos entre chaves {}: library(glue) planeta &lt;- &#39;mundo&#39; glue(&#39;Olá {planeta} pela {y}a vez&#39;, y = 1) FALSE Olá mundo pela 1a vez Tembém é possível adicionar expressões dentro das chaves: p &lt;- 1.123123123 glue(&#39;{p * 100}% das pessoas adoram R.&#39;) FALSE 112.3123123% das pessoas adoram R. glue(&#39;{scales::percent(p)} das pessoas adoram R.&#39;) FALSE 112% das pessoas adoram R. A função glue_collapse() é parecida com o paste() quando collapse = '', mas só aceita um objeto como entrada: x &lt;- glue_collapse(1:10) x FALSE 12345678910 x == paste(1:10, collapse = &#39;&#39;) FALSE [1] TRUE Se quiser colar os objetos elemento a elemento e depois colapsar, faça isso explicitamente em duas operações: library(magrittr) glue(&#39;{letters}/{LETTERS}&#39;) %&gt;% glue_collapse(&#39;, &#39;) FALSE a/A, b/B, c/C, d/D, e/E, f/F, g/G, h/H, i/I, j/J, k/K, l/L, m/M, n/N, o/O, p/P, q/Q, r/R, s/S, t/T, u/U, v/V, w/W, x/X, y/Y, z/Z O glue também tem uma função extra para trabalhar melhor com o %&gt;%, o glue_data(). O primeiro argumento dessa função é uma lista ou data.frame, e seus nomes são utilizados como variáveis para alimentar as chaves das strings. Use o . para fazer operações com toda a base de dados: mtcars %&gt;% head() %&gt;% glue_data(&#39;O carro {row.names(.)} rende {mpg} milhas por galão.&#39;) FALSE O carro Mazda RX4 rende 21 milhas por galão. FALSE O carro Mazda RX4 Wag rende 21 milhas por galão. FALSE O carro Datsun 710 rende 22.8 milhas por galão. FALSE O carro Hornet 4 Drive rende 21.4 milhas por galão. FALSE O carro Hornet Sportabout rende 18.7 milhas por galão. FALSE O carro Valiant rende 18.1 milhas por galão. 3.3.4 Resumo Use paste() para colar ou colapsar elementos usando um separador fixado. Use sprintf() quando quiser colocar objetos dentro de um texto complexo. Em todos os casos existe uma solução usando glue. Atualmente sempre que tenho um problema desse tipo uso o glue. Até o momento, não encontrei nenhum problema ou dificuldade. A vida do cientista de dados é mais feliz com o tidyverse! 3.3.5 Extra: O Guilherme Jardim Duarte fez uma ótima sugestão sobre este artigo. No pacote stringi existe um operador %s+% que cola textos iterativamente, com uma sintaxe similar à linguagem python, e que permite a colagem de textos usando um simples +. Exemplo: library(stringi) &#39;a&#39; %s+% &#39;ba&#39; %s+% &#39;ca&#39; %s+% &#39;xi&#39; FALSE [1] &quot;abacaxi&quot; Você pode adicionar esse operador como um atalho no RStudio, análogo ao Ctrl+Shift+M que usamos para escrever o %&gt;%. Para isso, veja esse tutorial sobre RStudio Addins. Mais sobre isso no livro R inferno↩ "],
["3-4-leaflet-com-markercluster.html", "3.4 leaflet com markerCluster", " 3.4 leaflet com markerCluster Autor: Julio Dificuldade baixa model O leaflet é uma biblioteca javascript para criação de mapas interativos. O pacote leaflet do R é um htmlwidget que permite gerar esses mapas de forma direta no R, para usar em documentos RMarkdown e Shiny. Uma das ferramentas que mais gosto do leaflet é a função markerClusterOptions(), que permite agrupar pontos no mapa em conjuntos de vários pontos. Como exemplo, utilizaremos uma base de dados que contém a localização e informações das varas da Justiça Estadual no Brasil. A Tabela abaixo mostra as primeiras linhas dessa base. A coluna lab já foi trabalhada para ser adicionada nos marcadores do mapa como popup. lat long uf municipio nome lab -21.243369 -48.80407 SP Santa Adélia VARA ÚNICA VARA ÚNICAPRAÇA DR. ADHEMAR DE BARROS 255Santa Adélia - SP, CEP 15950-000Santa Adélia - SPTelefone indisponível -3.102226 -67.95186 AM Santo Antônio do Içá VARA DA COMARCA DE SANTO ANTÔNIO DO IÇÁ VARA DA COMARCA DE SANTO ANTÔNIO DO IÇÁHUGO RIBEIRO S/NSanto Antônio do Içá - AM, CEP 69680-000Santo Antônio do Içá - AM(097) 9791-8763 -3.067617 -59.95668 AM Manaus 2º VARA DE FAMÍLIA E SUCESSÕES 2º VARA DE FAMÍLIA E SUCESSÕESRUA PARAIBA S/NManaus - AM, CEP 69079-265Manaus - AM(092) 9233-0351 Para utilizar o pacote leaflet, basta instalar o pacote via install.packages(). Na Figura 2.1, experimente passear pelo mapa. Procure também algum lugar que tenha várias varas juntas, para ver o que o markerCluster faz nesse caso. library(magrittr) library(leaflet) dados_aj_lab %&gt;% leaflet() %&gt;% addTiles() %&gt;% addMarkers( lng = ~long, lat = ~lat, popup = ~lab, clusterOptions = markerClusterOptions() ) Figura 2.1: Mapa das varas estaduais do Brasil. A função leaflet() carrega o motor do leaflet, ainda em branco. A função addTiles() adiciona as camadas de mapas de acordo com o zoom. É possível escolher temas para essas camadas. A função addMarkers() mapeia as varas da base de dados de acordo com as respectivas latitude e longitude. Note que é necessário adicionar um ~ antes das variáveis para mapeá-las da base de dados. A opção popup permite adicionar um balão com informações ao clicar num marcador. A opção clusterOptions faz a mágica que agrupa os pontos. A região azul observada ao colocar o mouse sobre um cluster é a casca convexa dos marcadores agrupados. É isso! "],
["4-modelagem.html", "Capítulo 4 Modelagem ", " Capítulo 4 Modelagem "],
["4-1-monty-hall-e-diagramas-de-influencia.html", "4.1 Monty Hall e diagramas de influência", " 4.1 Monty Hall e diagramas de influência Autor: Julio Dificuldade alta model Você está num jogo na TV e o apresentador pede para escolher uma entre 3 portas. Atrás de uma dessas portas tem uma Ferrari e nas outras duas temos cabras. Você escolhe uma porta. Depois, o apresentador retira uma porta que tem uma cabra e pergunta: você quer trocar de porta? A princípio, você pode achar que sua probabilidade de ganhar é 1/2, já que uma das portas foi retirada, então não importa se você troca ou não. Mas a resposta é que sim, vale à pena trocar de porta! A probabilidade de vencer o jogo trocando a porta é de 2/3. Figura 2.2: Brincadeira do XKCD. O problema de Monty Hall é talvez o mais eloquente exemplo de como a probabilidade pode confundir a mente humana. Esse problema desafiou a comunidade científica no final do século XX e chegou até a ser considerado um paradoxo. Recomendo ler o livro O Andar do Bêbado, de Leonard Mlodinow, que conta essa e muitas outras histórias interessantes sobre a probabilidade. Existem várias formas de explicar por quê trocar a porta é a melhor estratégia. A que eu mais gosto é a do próprio Andar do Bêbado, que mostra que, quando você escolhe a primeira porta, você está apostando se acertou ou não a Ferrari. Se você apostar que acertou a Ferrari, não deve trocar a porta e, se você apostar que errou a Ferrari, deve trocar. A aposta de errar a Ferrari de primeira tem probabilidade 2/3, logo, vale à pena trocar. Nesse post, mostramos uma solução alternativa, simples e elegante para o problema usando diagramas de influência e o pacote bnlearn. 4.1.1 Redes bayesianas As redes Bayesianas são o resultado da combinação de conceitos probabilísticos e conceitos da teoria dos grafos. Segundo Pearl, tal união tem como consequências três benefícios: i) prover formas convenientes para expressar suposições do modelo; ii) facilitar a representação de funções de probabilidade conjuntas; e iii) facilitar o cálculo eficiente de inferências a partir de observações. Da teoria de probabilidades precisamos apenas de alguns resultados básicos sobre probabilidade condicional. Primeiramente, pela definição de probabilidade condicional, sabemos que \\[ p(x_1, x_2) = p(x_1)p(x_2|x_1). \\] Aplicando essa regra iterativamente para \\(n\\) variáveis, temos \\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|x_1,\\dots, x_{j-1}). \\] Agora, imagine que, no seu problema, a variável aleatória \\(X_j\\) não dependa probabilisticamente de todas as variáveis \\(X_1,\\dots, X_{j-1}\\), e sim apenas de um subconjunto \\(\\Pi_j\\) dessas variáveis. Fazendo isso, a equação pode ser escrita como \\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|\\pi_j). \\] Chamamos \\(\\Pi_j\\) de pais de \\(X_j\\). Esse conjunto pode ser pensado como as variáveis que são suficientes para determinar as probabilidades de \\(X_j\\). A parte mais legal das redes Bayesianas é que elas podem ser representadas a partir de DAGs (grafos direcionados acíclicos). No grafo, se \\(X_1\\) aponta para \\(X_2\\), então \\(X_1\\) é pai de \\(X_2\\). Por exemplo, esse grafo aqui representa a distribuição de probabilidades \\(p(x_1, \\dots, x_5)\\) com \\[ p(x_1, \\dots, x_5) = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_3,x_2)p(x_5|x_4). \\] 4.1.2 Diagrama de influência Um diagrama e influências é uma rede Bayesiana com nós de decisão e utilidade (ganhos). Ou seja, é uma junção de três conceitos: \\[ \\underbrace{\\text{prob. condicional} + \\text{grafos}}_{\\text{rede Bayesiana}} + \\text{teoria da decisão} = \\text{diagrama de influência} \\] Na teoria da decisão, usualmente estamos interessados em maximizar a utilidade esperada. No diagrama, considerando a estrutura de probabilidades dada pela rede Bayesiana e as informações disponíveis, queremos escolher a decisão que faz com que, em média, nosso retorno seja mais alto. Com diagramas de influências, é possível organizar sistemas complexos com múltiplas decisões, considerando diferentes conjuntos de informações disponíveis. É uma ferramenta realmente muito poderosa. 4.1.3 Voltando ao Monty Hall Agora que sabemos um pouquinho de diagramas de influência, podemos desenhar o do Monty Hall: O jogador tem duas decisões a tomar: \\(D_1\\) (escolha_inicial): A escolha da porta inicial (1, 2, 3). \\(D_2\\) (trocar): Trocar a porta ou não (s, n). Também temos duas fontes de incerteza: \\(X_1\\) (ferrari): Em qual porta está a Ferrari (1, 2, 3). \\(X_2\\) (porta_retirada): Qual porta foi retirada (1, 2, 3). Essa variável não é sempre aleatória: se eu escolho a porta 1 e a Ferrari está em 2, o apresentador é obrigado a retirar a porta 3. Se o apresentador tiver a opção de escolher (que acontece no caso da escolha inicial ser a Ferrari), o apresentador escolhe uma porta para retirar aleatoriamente. Finalmente, temos um nó de utilidade: \\(U_1\\) (result): Ganhei a Ferrari (ganhei, perdi). Em R, podemos construir a rede Bayesiana do problema utilizando o pacote bnlearn: FALSE [,1] [,2] FALSE [1,] &quot;escolha_inicial&quot; &quot;porta_retirada&quot; FALSE [2,] &quot;ferrari&quot; &quot;porta_retirada&quot; FALSE [3,] &quot;porta_retirada&quot; &quot;trocar&quot; FALSE [4,] &quot;trocar&quot; &quot;result&quot; FALSE [5,] &quot;ferrari&quot; &quot;result&quot; FALSE [6,] &quot;escolha_inicial&quot; &quot;result&quot; O output desse conjunto de operações é um objeto do tipo bn com várias propriedades pré calculadas pelo pacote bnlearn: Random/Generated Bayesian network model: [escolha_inicial][ferrari][porta_retirada|escolha_inicial:ferrari][trocar|porta_retirada] [result|escolha_inicial:ferrari:trocar] nodes: 5 arcs: 6 undirected arcs: 0 directed arcs: 6 average markov blanket size: 3.60 average neighbourhood size: 2.40 average branching factor: 1.20 generation algorithm: Empty Com as especificação do problema dada, se gerarmos aleatoriamente todos os cenários, chegamos à essa combinação de casos equiprováveis (ver Extra 2) Agora, vamos escrever todas as combinações possíveis de cenários e guardar num data.frame chamado dados: |escolha_inicial |ferrari |porta_retirada |trocar |result | |:---------------|:-------|:--------------|:------|:------| |1 |1 |2 |n |ganhei | |1 |1 |2 |s |perdi | |1 |1 |3 |n |ganhei | |1 |1 |3 |s |perdi | |1 |2 |3 |n |perdi | |1 |2 |3 |s |ganhei | |1 |3 |2 |n |perdi | |1 |3 |2 |s |ganhei | |2 |1 |3 |n |perdi | |2 |1 |3 |s |ganhei | |2 |2 |1 |n |ganhei | |2 |2 |1 |s |perdi | |2 |2 |3 |n |ganhei | |2 |2 |3 |s |perdi | |2 |3 |1 |n |perdi | |2 |3 |1 |s |ganhei | |3 |1 |2 |n |perdi | |3 |1 |2 |s |ganhei | |3 |2 |1 |n |perdi | |3 |2 |1 |s |ganhei | |3 |3 |2 |n |ganhei | |3 |3 |2 |s |perdi | |3 |3 |1 |n |ganhei | |3 |3 |1 |s |perdi | Finalmente, ajustamos nossa rede Bayesiana, usando a função bnlearn::bn.fit(). A função bnlearn::cpquery() (conditional probability query) serve para realizar uma consulta de probabilidades dada a rede ajustada. No nosso caso, a partir de uma escolha inicial qualquer \\(d_1\\), queremos saber o ganho ao trocar é maior que o ganho ao não trocar. \\[ \\mathbb E(U_1\\; |\\; D_2 = \\text{s}, D_1 = d_1) &gt; \\mathbb E(U_1\\; |\\; D_2 = \\text{n}, D_1 = d_1). \\] Fazendo contas, isso equivale matematicamente a consultar se \\[ \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s}) &gt; \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{n}) \\] Agora, podemos consultar \\(\\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s})\\) com nosso modelo! [1] 0.6666704 E não é que dá 2/3 mesmo? Da mesma forma, temos [1] 0.3333187 Resolvido! 4.1.4 Wrap-up Vale à pena trocar a porta! Redes Bayesianas juntam grafos e probabilidades condicionais Diagramas de influência juntam redes Bayesianas e teoria da decisão Essas ferramentas podem ser utilizadas tanto para resolver Monty Hall quanto para ajudar em sistemas complexos. É isso pessoal. Happy coding ;) 4.1.5 Extra Se você ficou interessado(a) em como eu fiz o diagrama, utilizei o pacote DiagrammeR. O código está aqui: 4.1.6 Extra 2 É possível simular os dados que coloquei no post com uma função simples, que adicionei abaixo. Na verdade, o fato de eu ter considerado somente as combinações únicas de cenários e não os dados simulados abaixo é um pouco roubado, e só funciona porque os cenários calham de ser, de fato, equiprováveis. Observations: 10,000 Variables: 5 $ escolha_inicial &lt;fct&gt; 3, 1, 2, 1, 1, 1, 3, 1, 2, 3, 3, 1, 3, 1, 2, 2, 2,... $ ferrari &lt;fct&gt; 1, 1, 2, 1, 1, 2, 3, 3, 1, 2, 3, 3, 2, 1, 1, 3, 1,... $ porta_retirada &lt;fct&gt; 2, 3, 1, 3, 2, 3, 2, 2, 3, 1, 1, 2, 1, 2, 3, 1, 3,... $ trocar &lt;fct&gt; n, s, s, n, s, n, n, n, n, s, s, s, s, n, n, s, n,... $ result &lt;fct&gt; perdi, perdi, perdi, ganhei, perdi, perdi, ganhei,... Os dados do post podem ser obtidos fazendo isso aqui: Agradecimentos: Rafael Stern, que me convenceu de que vale à pena mostrar os dados simulados 😉 "],
["4-2-construindo-autoencoders.html", "4.2 Construindo Autoencoders", " 4.2 Construindo Autoencoders Autoencoders são redes neurais treinadas com o objetivo de copiar o seu input para o seu output. Esse interesse pode parecer meio estranho, mas na prática o objetivo é aprender representações (encodings) dos dados, que podem ser usadas para redução de dimensionalidade ou até mesmo compressão de arquivos. Basicamente, um autoencoder é dividido em duas partes: um encoder que é uma função \\(f(x)\\) que transforma o input para uma representação \\(h\\) um decoder que é uma função \\(g(x)\\) que transforma a representação \\(h\\) em sua reconstrução \\(r\\) Imagem do blog do Keras 4.2.1 Construindo o seu primeiro autoencoder Nesse pequeno tutorial, vou usar o keras para definir e treinar os nossos autoencoders. Como base de dados vou usar algumas simulações e o banco de dados mnist (famoso para todos que já mexeram um pouco com deep learning). O mnist é um banco de dados de imagens de tamanho 28x28 de dígitos escritos à mão. Esse dataset promoveu grandes avanços na área de reconhecimento de imagens. Com esse código definimos um modelo da seguinte forma: \\[ X = (X*W_1 + b_1)*W_2 + b_2 \\] Em que: \\(X\\) é o nosso input com dimensão (?, 784) \\(W_1\\) é uma matriz de pesos com dimensões (784, 32) \\(b_1\\) é uma matriz de forma (?, 32) \\(W_2\\) é uma matriz de pesos com dimensões (32, 784) \\(b_2\\) é uma matriz de forma (?, 784) Note que ? aqui é o número de observaçãoes da base de dados. Agora vamos estimar \\(W_1\\), \\(W_2\\), \\(b_1\\) e \\(b_2\\) de modo a minimizar alguma função de perda. Inicialmente vamos usar a binary crossentropy por pixel que é definida por: \\[-\\sum_{i=1}y_i*log(\\hat{y}_i)\\] Isso é definido no keras usando: Não vou entrar em detalhes do que é o adadelta, mas é uma variação do método de otimização conhecido como gradient descent. Agora vamos carregar a base de dados e em seguida treinar o nosso autoencoder`. Estimamos os parâmetros desse modelo no keras fazendo: Depois de rodar todas as iterações, você poderá usar o seu encoder e o seu decoder para entender o que eles fazem com as imagens. Veja o exemplo a seguir em que vamos obter os encodings para as 10 primeiras imagens da base de teste e depois reconstruir a imagem usando o decoder. FALSE [1] 10 32 FALSE [1] 0.0000000 10.1513205 3.5742311 2.6635208 6.3097358 3.4840517 FALSE [7] 9.1041250 6.6329145 1.6385922 9.8017225 9.5529270 1.6670935 FALSE [13] 5.7208562 4.8035479 3.9149191 0.6408147 1.2716029 3.1215091 FALSE [19] 13.7575903 0.0000000 1.8692881 3.2142215 0.7444992 5.0728440 FALSE [25] 8.2932110 9.9866810 2.7651572 11.1291723 5.2460670 5.6875997 FALSE [31] 10.6097431 3.6338394 O encoder transforma a matriz de (10, 784) para uma matriz com dimensao (10, 2). Podemos reconstruir a imagem, a pardir da imagem que foi comprimida usando o nosso decoder. Compare as reconstruções com as imagens originais abaixo: Um ponto interessante é que esse modelo faz uma aproximação da solução por componentes principais! Na verdade, a definição do quanto são parecidos é quase-equivalente. Isso quer dizer que os pesos \\(W\\) encontrados pelo PCA e pelo autoencoder serão diferentes, mas o sub-espaço criado pelos mesmos será equivalente. Se são equivalentes, qual a vantagem de usar autoencoders ao invés de PCA? O PCA para por aqui, você define que serão apenas relações lineares, e você reduz dimensão apenas reduzindo o tamanho da matriz. Em autoencoders você tem diversas outras saídas para aprimorar o método. A primeira delas é simplesmente adicionar uma condição de esparsidade nos pesos. Isso vai reduzir o tamanho do vetor latente (como é chamada a camada do meio do autoencoder) também, pois ele terá mais zeros. Isso pode ser feito rapidamente com o keras. Basta adicionar um activity_regularizer em nossa camada de encoding. Isso vai adicionar na função de perda um termo que toma conta do valor dos outputs da camada intermediária. Outra forma de melhorar o seu autoencoder é permitir que o encoder e o decoder sejam redes neurais profundas. Com isso, ao invés de tentar encontrar transformações lineares, você permitirá que o autoencoder encontre transformações não lineares. Mais uma vez fazemos isso com o keras: Existem formas ainda mais inteligentes de construir esses autoencoders, mas o post iria ficar muito longo e não ia sobrar asssunto para o próximo. Se você quiser saber mais, recomendo fortemente a leitura deste artigo do blog do Keras e desse capítulo. Uma família bem moderna de autoencoders são os VAE (Variational Autoencoders). Esses autoencoders aprendem modelos de variáveis latentes. Isso é interessante porque permite que você gere novos dados, parecidos com os que você usou para treinar o seu autoencoder. Você pode encontrar uma implementação desse modelo aqui. "],
["4-3-modelos-beseados-em-arvores-e-a-multicolinearidade.html", "4.3 Modelos beseados em árvores e a multicolinearidade", " 4.3 Modelos beseados em árvores e a multicolinearidade Modelos baseados em árvores como árvores de decisão, random forest, ligthGBM e xgboost são conhecidos, dentre outras qualidades, pela sua robustês diante do problema de multicolinearidade. É sabido que seu poder preditivo não se abala na presença de variáveis extremamente correlacionadas. Porém, quem nunca usou um Random Forest pra fazer seleção de variáveis? Pegar, por exemplo, as top 10 mais importantes e descartar o resto? Ou até mesmo arriscou uma interpretação e concluiu sobre a ordem das variáveis mais importantes? Abaixo mostraremos o porquê não devemos ignorar a questão da multicolinearidade completamente! 4.3.1 Um modelo bonitinho Primeiro vamos ajustar um modelo bonitinho, livre de multicolinearidade. Suponha que queiramos prever Petal.Length utilizando as medidas das sépalas (Sepal.Width e Sepal.Length) da nossa boa e velha base iris. O gráfico acima mostra que as variáveis explicativas não são fortemente correlacionadas. Ajustando uma random fores, temos a seguinte ordem de importância das variáveis: Sem surpresas. Agora vamos para o problema! 4.3.2 Um modelo com feinho Vamos forjar uma situação extrema em que muitas variáveis sejam multicolineares. Vou fazer isso repetindo a coluna Sepal.Length várias vezes. Agora a coisa tá feia! Temos 20 variáveis perfeitamente colineares. Mesmo assim um random forest nessa nova base não perderia poder preditivo. Mas como ficou a importância das variáveis? Aqui o jogo já se inverteu: concluiríamos que Sepal.Width é mais importante de todas as variáveis! 4.3.3 Seleção de variáveis furado O gráfico abaixo mostra que quanto mais variáveis correlacionadas tivermos, menor a importância de TODAS ELAS SIMULTANEAMENTE! É como se as variáveis colineares repartissem a importância entre elas. Na prática, se estabelecessemos um corte no valor de importância pra descartar variáveis (como ilustrado pela linha vermelha), teríamos um problema em potencial: poderíamos estar jogando fora informação muito importante. 4.3.4 Como tratar multicolinearidade, então? Algumas maneiras de lidar com multicolinearidade são: Observar a matriz de correlação VIF Recursive feature elimination 4.3.5 Conclusão Cuidado ao jogar tudo no caldeirão! Devemos sempre nos preocupar com multicolinearidade, mesmo ajustando modelos baseados em árvores. "],
["4-4-woe-em-r-com-tidywoe.html", "4.4 WoE em R com tidywoe", " 4.4 WoE em R com tidywoe WoE (weight of evidence) é uma ferramenta bastante usada em aplicações de regressão logística, principalmente na área de score de crédito. Simploriamente falando, ele transforma categorias em números que refletem a diferença entre elas pelo critério de separação do Y = 1 e Y = 0. Se você ainda não sabe o que é ou quer ler mais sobre o assunto, um texto que eu gostei de ler: Data Exploration with Weight of Evidence and Information Value in R O autor desse texto é o Kim Larsen, criador do pacote Information que é completo e cheio de ferramentas sofisticadas em torno do WoE. Porém, no dia a dia do meu trabalho volta e meia eu tinha que construir rotinas próprias para fazer as versões em WoE das minhas variáveis, mesmo com vários pacotes completos disponíveis. A principal motivação era que eles não eram muito práticos e não se encaixavam na filosofia do tidyverse. Daí acabei juntando essas rotinas num pacote chamado tidywoe e deixando no ar. A ideia é que ela faça o analista ganhar em tempo, legibilidade e reprodutibilidade. Abaixo segue como usar. 4.4.1 Instalação e dados Para instalar, basta rodar abaixo. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;athospd/tidywoe&quot;) library(tidyverse) library(tidywoe) # install.packages(&quot;FactoMineR&quot;) data(tea, package = &quot;FactoMineR&quot;) tea_mini &lt;- tea %&gt;% dplyr::select(breakfast, how, where, price) 4.4.2 Como usar Tem duas funções que importam: - add_woe() - adiciona os woe’s num data frame. - woe_dictionary() - cria dicionário que mapeia as categorias com os woe’s. 4.4.3 add_woe() A função add_woe() serve para adicionar as versões WoE’s das variáveis em sua amostra de dados. tea_mini %&gt;% add_woe(breakfast) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 -0.2564295 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Você pode selecionar as variáveis que vc quiser selecionando-as como se fosse no dplyr::select(). tea_mini %&gt;% add_woe(breakfast, where:price) 4.4.4 woe_dictionary() A função woe_dictionary() é uma das duas partes necessárias para fazer o add_woe() funcionar (a outra parte são os dados). Ele constrói o dicionário de categorias e seus respectivos woe’s. tea_mini %&gt;% woe_dictionary(breakfast) variable explanatory n_tot n_breakfast n_Not.breakfast p_breakfast p_Not.breakfast woe how tea bag 170 80 90 0.5555556 0.5769231 -0.0377403 how tea bag+unpackaged 94 50 44 0.3472222 0.2820513 0.2078761 how unpackaged 36 14 22 0.0972222 0.1410256 -0.3719424 where chain store 192 90 102 0.6250000 0.6538462 -0.0451204 4.4.5 Usando um dicionário customizado Muitas vezes há o interesse em ajustar na mão alguns valores de woe para consertar a ordem dos efeitos de uma dada variável ordinal. Esse é o motivo de o add_woe() poder receber um dicionário passado pelo usuário. Isso se faz por meio do argumento .woe_dictionary. A maneira mais fácil de se fazer isso é montar um dicionário inicial com o woe_dictionary() e depois alterar os valores nele para alcançar os ajustes desejados. Exemplo: # Construa um dicionário inicial tea_mini_woe_dic &lt;- tea_mini %&gt;% woe_dictionary(breakfast) # Mexa um pouquinho nos woes tea_mini_woe_dic_arrumado &lt;- tea_mini_woe_dic %&gt;% mutate(woe = if_else(explanatory == &quot;p_unknown&quot;, 0, woe)) # Passe esse dicionário para o add_woe() tea_mini %&gt;% add_woe(breakfast, .woe_dictionary = tea_mini_woe_dic_arrumado) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 0.0000000 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 4.4.6 Exemplo de exploração O woe_dictionary() devolve uma tabela arrumada, bem conveniente para explorar mais. Por exemplo, a tabela está pronta para o ggplot. Aqui está o github do pacote para contribuições. Pretendo colocar bastante coisa nova no pacote ainda. "],
["4-5-regressao-logistica-aspectos-computacionais.html", "4.5 Regressão logística: aspectos computacionais", " 4.5 Regressão logística: aspectos computacionais Neste texto vamos discutir um pouco sobre regressão logística, tensorflow e Modelos Lineares Generalizados (Generalized Linear Models, GLMs). Não vou economizar nas matemáticas nem nos códigos. Se você não conhece GLMs, recomendo dar uma lida, pelo menos na introdução, do livro do professor Gilberto A. Paula. Se você não conhece o Tensorflow, recomendo ver a página do RStudio sobre Tensorflow. Se você curte a parte computacional da estatística, esse livro do LEG-UFPR é obrigatório. Eles são os melhores. 4.5.1 Introdução: o tensorglm Um de meus interesses no momento é implementar GLMs usando Tensorflow. O Tensorflow é uma biblioteca computacional mantida pela Google que utiliza paralelização e o poder das GPUs (Graphical Processing Units) para fazer contas. O Tensorflow foi especialmente desenhado para facilitar o ajuste de redes neurais profundas e outros modelos sofisticados. GLMs são casos particulares de redes neurais. Uma rede neural com apenas uma camada e com funções de perda / verossimilhanças baseadas na Divergência de Kullback-Leibler são exatamente iguais aos GLMs. Por exemplo, essa divergência equivale ao erro quadrático médio para a distribuição gaussiana e binary-crossentropy para logística. Por isso, não é de se surpreender que já existam soluções prontas para modelos específicos, como regressão linear normal, logística, e até Poisson. No entanto, essas soluções têm duas limitações: Não são extensivas. Por exemplo, não achei códigos para as distribuições normal inversa, gama e binomial negativa. As soluções atuais utilizam o algoritmo descida de gradiente para otimização, que é muito legal, mas não se aproveita de alguns resultados que temos na área de GLMs, como o IWLS (Iterated Weighted Least Squares), que é uma derivação do algoritmo Fisher-scoring, que reduz o problema do ajuste ao cálculo iterado de inversas e multiplicações de matrizes. Meu intuito é, então, montar uma solução alternativa que funcione igual à função glm() do R, mas usando Tensorflow no backend ao invés do algoritmo atual, que é em Fortran. Com isso, espero que o ajuste seja mais eficiente quando os dados são grandes e permita trabalhar com dados que não cabem na memória. 4.5.2 A regressão logística Meu primeiro experimento com o tensorglm foi implementar a regressão logística usando tensorflow, com descida de gradiente. Considere o problema \\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta x),\\] em que \\(Y\\) é nossa variável resposta, \\(x\\) é nossa variável explicativa, \\(\\alpha\\) e \\(\\beta\\) são os parâmetros que queremos estimar e \\(\\sigma(\\cdot)\\) é a função sigmoide, cuja inversa é a função de ligação logística. \\[\\sigma(\\eta) = \\frac{1}{1 + e^{-\\eta}}\\] Considerando que temos observações \\(Y_1, \\dots, Y_n\\) condicionalmente independentes, já temos o suficiente para especificar nosso modelo de regressão logística. O próximo passo é definir, com base nisso, a função que queremos otimizar. A partir de uma amostra \\(y_1, \\dots, y_n\\) e observando que \\(\\mu_i = \\sigma(\\alpha + \\beta x_i)\\), a verossimilhança do modelo é dada por \\[ \\mathcal L((\\alpha, \\beta)|\\mathbf y) = \\prod_{i=1}^n f(y_i|(\\alpha, \\beta), x_i) = \\prod_{i=1}^n\\mu_i^{y_i}(1-\\mu_i)^{1-y_i} \\] O logaritmo da verossimilhança é dado por \\[ \\begin{aligned} l((\\alpha, \\beta)|\\mathbf y) &amp;= \\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\\\ &amp;= \\sum_{i=1}^n y_i\\log(\\sigma(\\alpha + \\beta x_i)) + (1-y_i)\\log(1 - \\sigma(\\alpha + \\beta x_i)) \\end{aligned} \\] Nosso objetivo é maximizar \\(l\\) com relação à \\(\\alpha\\) e \\(\\beta\\). Detalhe: essa soma, se multiplicada por -1, também é chamada de função de perda binary cross-entropy. Por isso que tanto faz você definir GLMs a partir de \\(P(Y|x)\\) ou a partir da função de perda! OK, problema dado! vamos implementar usando tensorflow! Feito! Agora podemos usar a magia do tensorflow, que é esperto o suficiente para otimizar essa perda sem a gente se preocupar em calcular derivadas na mão. Para quem não conhece o algoritmo de descida de gradiente, ele funciona assim: \\[ (\\alpha, \\beta)_{\\text{novo}} = (\\alpha, \\beta)_{\\text{velho}} + k \\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}}), \\] onde \\(\\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}})\\) é o gradiente da verossimilhança em relação ao vetor \\((\\alpha, \\beta)\\), ou seja, são as derivadas parciais de \\(l\\) em relação à \\(\\alpha\\) e \\(\\beta\\). Isso dá a direção e intensidade em que os valores devem ser atualizados. \\(k\\) é chamado de learning rate, é um fator usado para controlar o tamanho do passo dado pelo gradiente. Esse valor normalmente é definido à mão. No caso dos GLMs, \\(k\\) é substituído pelo inverso da segunda derivada da \\(l\\) em relação aos parâmetros, gerando assim os algoritmos de Newton-Raphson e Fisher-scoring. Detalhe: se você procurar esse algoritmo na internet, você vai encontrar um \\(-\\) e não um \\(+\\). Isso acontece porque estamos usando a verossimilhança e não a perda. Iter: 01, alpha=2.32, beta=3.593 Iter: 02, alpha=1.56, beta=3.409 Iter: 03, alpha=1.411, beta=2.989 Iter: 04, alpha=1.261, beta=2.665 Iter: 05, alpha=1.153, beta=2.422 Iter: 06, alpha=1.078, beta=2.257 Iter: 07, alpha=1.033, beta=2.154 Iter: 08, alpha=1.006, beta=2.095 Iter: 09, alpha=0.992, beta=2.062 Iter: 10, alpha=0.984, beta=2.045 Parece que funcionou! Agora sabemos ajustar uma regressão logística na mão, com o algoritmo de descida de gradiente… ou será que não? 4.5.3 O Problema Vamos considerar o mesmo problema, mas agora com duas explicativas. temos \\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta_1 x_2+ \\beta_2 x_2),\\] As contas são exatamente as mesmas e vou omitir, mostrando apenas o código novo. Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Oops! Explodiu! Por que será??? Uma forma de corrigir esse problema é considerando uma taxa de aprendizado k um pouco menor. Com os mesmos dados e modelo acima, ao fazer e rodar novamente, já conseguimos chegar nos resultados abaixo. Iter: 01, alpha=1.525, beta1=2.492, beta2=3.205 Iter: 02, alpha=1.183, beta1=2.32, beta2=3.36 Iter: 03, alpha=1.122, beta1=2.248, beta2=3.34 Iter: 04, alpha=1.101, beta1=2.208, beta2=3.296 Iter: 05, alpha=1.085, beta1=2.178, beta2=3.254 Iter: 06, alpha=1.073, beta1=2.152, beta2=3.216 Iter: 07, alpha=1.062, beta1=2.13, beta2=3.183 Iter: 08, alpha=1.053, beta1=2.112, beta2=3.154 Iter: 09, alpha=1.044, beta1=2.095, beta2=3.13 Iter: 10, alpha=1.037, beta1=2.082, beta2=3.109 Mais algumas iterações e o modelo converge. Mas nós não queremos ficar fazendo um ajuste tão fino no valor de k, certo? Afinal, queremos resolver problemas do mundo real, não ficar escolhendo valores de k… Outra forma de resolver isso é evitando problemas numéricos nas contas. O cálculo da função de perda, por exemplo, pode ser melhorado. Mas como? Bom, problemas numéricos não são minha especialidade, então agora é hora de seguir os mestres. Vamos olhar como o R e como o Tensorflow implementam as funções de perda para regressão logística. 4.5.3.1 Os objetos de classe family no R No R, os GLMs buscam informações de objetos da classe family() para realizar os ajustes. No caso da logística, o objeto é retornado por uma função chamada binomial(). O resultado disso é uma lista com vários métodos implementados. Por exemplo, a variância da binomial é dada por: function (mu) mu * (1 - mu) &lt;bytecode: 0x55fc8e220a18&gt; &lt;environment: 0x55fca4eb5040&gt; A função de perda é dada pelo método fam$dev.resids() (resíduos deviance), e o código fonte é: function (y, mu, wt) .Call(C_binomial_dev_resids, y, mu, wt) &lt;bytecode: 0x55fc8e2253a0&gt; &lt;environment: 0x55fca4eb5040&gt; Hmm, parece que é uma função feita em C. Como as contas da nossa perda (soma, logaritmo, multiplicação e divisão) já são todas implementadas em C, provavelmente a conta foi implementada em C para garantir estabilidade numérica. Olhando o código-fonte do pacote stats, encontramos a definição da função. A função é um pouco longa, então eu mantive apenas as partes importantes: static R_INLINE double y_log_y(double y, double mu) { return (y != 0.) ? (y * log(y/mu)) : 0; } SEXP binomial_dev_resids(SEXP y, SEXP mu, SEXP wt) { /* inicialização de variáveis e verificações */ /* rmu e ry são os valores de mu e y transformados para reais */ /* rmu e ry são os valores de mu e y transformados para reais */ for (i = 0; i &lt; n; i++) { mui = rmu[i]; yi = ry[i]; rans[i] = 2 * rwt[lwt &gt; 1 ? i : 0] * (y_log_y(yi, mui) + y_log_y(1 - yi, 1 - mui)); } /* outros códigos não muito importantes */ UNPROTECT(nprot); return ans; } Eu não programo muito em C, mas desse código dá para ver duas coisas importantes: i) a função y_log_y só faz a conta se o valor de \\(y\\) for diferente de zero, se não, ela já retorna zero; ii) a função y_log_y faz a conta \\(y\\log({y}/{\\mu})\\), ao invés de apenas \\(y\\log({\\mu})\\). Isso acontece pois no R estamos minimizando o Desvio do modelo, dado por \\[ \\begin{aligned} &amp;D(\\mathbf y, \\mu) = 2[l(\\mathbf y|\\mathbf y) - l(\\mathbf y|(\\alpha, \\beta))]\\\\ &amp;=2\\left[\\sum_{i=1}^n y_i\\log(y_i) + (1-y_i)\\log(1-y_i)\\right. - \\\\ &amp;\\left. -\\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\right] \\\\ &amp;=2\\left[\\sum_{i=1}^n y_i\\log\\left(\\frac{y_i}{\\mu_i}\\right) + (1-y_i)\\log\\left(\\frac{1-y_i}{1-\\mu_i}\\right)\\right]. \\end{aligned} \\] Essa é a formulação usual na literatura de GLMs, que apresenta uma série de propriedades estatísticas. Minimizar o desvio equivale a maximizar a verossimilhança. Será que isso ajuda nos problemas numéricos? Vamos ver: Iter: 01, alpha=NaN, beta1=NaN, beta2=NaN Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Hmm, parece que não. Se olharmos mais atentamente para a função desvio, como \\(y\\) pode assumir apenas os valores zero ou um, é possível observar que a conta é equivalente à perda calculada anteriormente. Possivelmente o problema aqui é que o tensorflow não trabalha muito bem com essas condições (tf$where) na perda, e isso dá problemas na hora de calcular o gradiente. Essa função do R simplesmente não resolve o problema inicial. Melhor olhar o que o tensorflow faz! 4.5.4 A binary cross-entropy no Tensorflow Eu escondi de vocês, mas o tensorflow já tem a função de perda implementada: tf$nn$sigmoid_cross_entropy_with_logits. Ela já assume que a função de ligação é logística, por isso o sigmoid_ no início. Traduzindo livremente o help da função, temos o seguinte (z=\\(y\\) e x=\\(\\eta = \\alpha + \\beta x\\)) z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x)) = (1 - z) * x + log(1 + exp(-x)) = x - x * z + log(1 + exp(-x)) Para \\(\\eta &lt; 0\\) para evitar problemas numéricos com \\(\\exp(-\\eta)\\), reformulamos para x - x * z + log(1 + exp(-x)) = log(exp(x)) - x * z + log(1 + exp(-x)) = - x * z + log(1 + exp(x)) Então, para garantir estabilidade e evitar problemas numéricos, a implementação usa essa formulação equivalente max(x, 0) - x * z + log(1 + exp(-abs(x))) Beleza, vamos tentar! Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=1.276, beta1=2.495, beta2=3.608 Iter: 03, alpha=1.197, beta1=2.396, beta2=3.562 Iter: 04, alpha=1.164, beta1=2.335, beta2=3.489 Iter: 05, alpha=1.14, beta1=2.287, beta2=3.42 Iter: 06, alpha=1.12, beta1=2.245, beta2=3.358 Iter: 07, alpha=1.102, beta1=2.21, beta2=3.303 Iter: 08, alpha=1.086, beta1=2.178, beta2=3.256 Iter: 09, alpha=1.073, beta1=2.152, beta2=3.215 Iter: 10, alpha=1.061, beta1=2.129, beta2=3.18 Funcionou! :) 4.5.5 Wrap-up Tensorflow é uma biblioteca interessante a ser explorada. É possível implementar uma regressão logística do zero em poucos passos. Precisamos tomar cuidado com problemas numéricos! No futuro, brincaremos também com o algoritmo IWLS. Será que ele roda mais rápido que a descida de gradiente? "],
["4-6-sao-paulo-e-o-problema-da-mochila.html", "4.6 São Paulo e o problema da mochila", " 4.6 São Paulo e o problema da mochila Autor: Julio Dificuldade média model São Paulo é a minha cidade preferida, não só porque moro aqui, mas também porque é uma cidade cheia de diversidade, boa gastronomia e oportunidades. Para sentir um pouco dessa vibe, recomendo passear na avenida Paulista aos domingos. É sensacional! Mas a cidade da diversidade só é o que é porque temos muita, muita gente nela. O município tem 12 milhões de habitantes. Esse número é tão grande que temos um paulistano para cada 17 brasileiros! Se São Paulo fosse um país, seria o 77 do mundo, ganhando de países como a Bélgica, Grécia, Portugal, Bolívia e muitas outras. Outro dia eu estava pensando na seguinte problemática: qual é a área do Brasil ocupada pela população de São Paulo? Ou seja, se pegarmos os municípios com grandes áreas, quanto do país conseguiríamos preencher com 12 milhões de habitantes? O interessante é que essa questão recai exatamente no problema da mochila, que é um famoso desafio de programação inteira. Depois de estudar profundamente no wikipedia, vi que o problema não é tão trivial como parece. 4.6.1 O problema da mochila Considere o seguinte contexto: você tem uma mochila com capacidade de 15kg e precisa carregar a combinação de itens com maior valor, com cada item possuindo valores e pesos diferentes. Figura 2.3: Knapsack problem. Retirado do Wikipedia. Outra forma de pensar nesse problema é com um cardápio de restaurante: Figura 2.4: XKCD sobre o knapsack problem. Em linguagem matemática, o que temos é a task: \\[ \\begin{aligned} &amp; \\text{maximizar } \\sum_{i=1}^n v_i x_i \\\\ &amp; \\text{sujeito à } \\sum_{i=1}^n w_i x_i \\leq W, \\text{ com } x_i \\in\\{0,1\\}\\\\ \\end{aligned} \\] No nosso caso essas letras significam isso aqui: \\(n\\) é o número de municípios no Brasil (5570). \\(v_i\\) é a área do município \\(i\\). \\(w_i\\) é a população do município \\(i\\). \\(W\\) é a população de São Paulo (12 milhões). \\(x=(x_1,\\dots,x_n)^\\top\\) é o vetor que seleciona os municípios. Se o município \\(i\\) faz parte da solução \\(x_i=1\\) e, caso contrário, \\(x_i=0\\). Ou seja, queremos escolher municípios para colocar na mochila tentando maximizar a área, mas o máximo de população que podemos contemplar é 12 milhões. O problema da mochila é muito interessante pois trata-se de um problema NP-difícil, ou seja, não existe um algoritmo de polinomial capaz de resolvê-lo. Se \\(w_i &gt; 0, \\forall i\\in1,\\dots,n\\) então a solução pode ser encontrada com um algoritmo pseudo-polinomial. 4.6.2 Forma ad-hoc Se \\(x_i\\) pudesse assumir valores entre zero e um (ou seja, se pudéssemos selecionar apenas pedaços de municípios), a solução seria trivial. Bastaria colocar os municípios em ordem decrescente pela razão \\(v_i/w_i\\) e escolher os municípios ou parte deles até obter \\(W\\). Isso indica uma forma sub-ótima de resolver o problema. Chamamos essa solução de ad-hoc. A solução é encontrada assim: Colocar os municípios em ordem decrescente pela razão \\(v_i/w_i\\), Escolher os municípios de maior razão até que a população do próximo município estoure \\(W\\). Escolher outros municípios com maior razão na ordem até não ser possível incluir mais nenhum município. 4.6.3 Solução ótima A solução ótima pode ser encontrada usando a função mknapsack() do pacote adagio. Por exemplo, considere os vetores de pesos w, valores p e máximo cap abaixo. O vetor-solução é dado por FALSE [1] 1 1 1 1 0 1 0 0 4.6.4 Dados As áreas e estimativas das populações dos municípios do Brasil em 2016 foram obtidas do IBGE. A leitura é realizada usando pacotes do tidyverse. Pacotes: Dados: 4.6.5 Resultados A solução ad-hoc e ótima são computadas com esse código: Agora, vamos melhorar a solução ad-hoc incluindo os melhores municípios. A Tabela 2.1 mostra os municípios que foram classificados diferentemente nos dois métodos. Note que a solução ótima trocou apenas um município da solução adhoc (Coivaras - PI) pelo município de Angélica - MS. Tabela 2.1: Municípios diferentes nas duas soluções. uf nome area pop s_adhoc s_knapsack PIAUÍ COIVARAS 485373828 3953 1 0 MATO GROSSO DO SUL ANGÉLICA 1282110939 10458 0 1 A Tabela 2.2 mostra a diferença dos resultados dos dois métodos. A solução ótima fica com apenas 165 pessoas a menos que São Paulo. Tabela 2.2: Diferença dos resultados. Método Área total População total Diferença para sp adhoc 5574729418852.4 12100250 6670 knapsack 5575526155963.55 12106755 165 São Paulo - 12106920 0 4.6.6 Mapa final Visualmente, a solução ótima e a solução adhoc são idênticas. Por isso vou mostrar apenas como fica o mapa para a solução ótima. O resultado aparece na Figura 2.5. É realmente impressionante ver que aquela regiãozinha vermelha tem a mesma população que toda a região azul do mapa. Figura 2.5: Mapa do Brasil final. É isso! Happy coding ;) "],
["4-7-as-cores-da-marvel-vs-dc.html", "4.7 As cores da Marvel vs DC", " 4.7 As cores da Marvel vs DC Autor: William Dificuldade média model A cor é uma diferença notável entre os filmes da Marvel e da DC. Enquanto a Disney/Marvel Studios costuma lançar filmes com tons mais claros e alegres, a Warner tem optado por cenários escuros, com um aspecto mais sombrios. Essas escolhas são um reflexo do clima das histórias de cada universo: aventuras engraçaralhas com um drama superficial vs seja lá o que passa na cabeça do Zack Snyder. Para estudar melhor a paleta de cores utilizadas nos filmes, vamos aplicar a análise introduzida pelo Dani neste post, com pequenas alterações. Como amostra, selecionei 10 imagens de Batman vs Superman e 10 do Capitão América: guerra civil. Tentando deixar a análise o menos subjetiva possível, escolhi imagens de cenas emblemáticas e dos principais personagens. Abaixo as imagens que peguei de cada filme. Seguindo a análise do Dani, vamos utilizar as seguintes bibliotecas para a análise. Eu salvei as imagens em arquivos do tipo bvs_n.jpg e cw_n.jpg, com n variando de 1 a 10. Isso facilitou a leitura desses arquivos. O código abaixo mostra como criar um vetor com o caminho das 10 imagens de cada filme. Como vamos trabalhar com mais de uma imagem, eu criei a função ler_imagem() para ler os arquivos. Podemos então usar a função map() para aplicá-la a todos os 10 arquivos. A função reduce(rbind) transforma as 10 matrizes de pixels em uma matriz só, como se as imagens estivessem coladas uma embaixo da outra. Abaixo estão as funções cria_paleta() e exibir() do post do Dani. A única diferença aqui é que a função cria_paleta() já recebe a matriz representando a imagem. Assim, basta aplicar essas funções aos objetos img_bvs e img_cw para obter as paletas. Primeiro para o Batman vs Superman: E agora para o Capitão América: Observe que o filme da DC tem cores mais escuras e fortes, com vários tons de azul, indicando as cenas noturnas e de chuva. Já a paleta da Marvel apresenta cores mais claras, com vários tons representando o céu pálido das cenas externas. Podemos fazer a análise agora para o pôster de cada filme (o que aparece no IMDB): Veja que os diferentes tons de azul se repetem no pôster do Batman vs Superman. Já o pôster do Capitão América é bem cinzento, com metade da paleta representando tons de cinza. Fica então o desafio de repetir a análise para outros filmes e compartilhar o resultado com a gente. Faça a sua! "],
["4-8-minimos-quadrados-com-restricoes-lineares.html", "4.8 Mínimos quadrados com restrições lineares", " 4.8 Mínimos quadrados com restrições lineares A característica mais importante de um modelo estatístico é a sua flexibilidade. Esse termo pode ser entendido de várias formas, mas neste texto vamos considerar que um modelo é flexível se ele explica de forma coerente uma ampla gama de fenômenos reais. Pensando assim, a regressão linear pode ser considerada um modelo flexível, já que muitas relações funcionais cotidianas são do tipo \\(y = \\beta x\\). É justamente por causa dessa flexibilidade que a boa e velha regressão de mínimos quadrados é tão usada, até mesmo aonde não deveria. O seu uso é tão indiscriminado que uma vez, em aula, um professor extraordinariamente admirável me disse que “90% dos problemas do mundo podem ser resolvidos com uma regressão linear”. Sendo bastante honesto, é provável que o meu professor esteja certo, mas este texto não é sobre isso. Este é um post sobre o que fazer quando a regressão linear simples não basta. No que segue, vamos discutir uma pequena (e poderosa) extensão do modelo de regressão linear simples, mas antes de prosseguir para o problema propriamente dito (e sua implementação em R), vamos discutir da teoria que existe por trás dele. 4.8.1 Regressão linear é programação quadrática Embora seja pouco enfatizado nos bacharelados de estatística, uma regressão linear pode ser formulada como um problema de programação quadrática. Entrando nos detalhes, essa afirmação deve-se a dois fatos: Existe uma teoria, que chama-se programação quadrática, que soluciona problemas da forma \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big),\\] onde \\(x \\in \\mathbb{R}^p\\) e \\(Q\\) e \\(c\\) tem dimensões que fazem a conta acima ter sentido. A teoria ocupa-se desenvolvendo algoritmos exatos e aproximados para obter soluções desses problemas, inclusive com generalizações: \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }Ax \\geq 0.\\] Uma regressão linear consiste em resolver \\[\\min_\\beta (Y - \\beta X)&#39;(Y-\\beta X),\\] que, com um pouco de álgebra, é equivalente à \\[ \\min_\\beta (-2Y&#39;X\\beta + \\beta&#39;X&#39;X\\beta).\\] Logo, tomando \\(Q = 2X&#39;X\\) e \\(c = \\frac{1}{2}X&#39;Y\\) tem-se que esse é um problema de programação quadrática, que por sua vez é um problema convexo e que, segundo a teoria, tem uma única solução no ponto \\(\\beta = (X&#39;X)^{-1}X&#39;Y\\). 4.8.2 Uma regressão linear simples mais flexível Talvez o jeito mais simples de flexibilizar uma regressão linear no sentido mencionado no começo desse texto é restringir os seus parâmetros. Em muitos contextos, esse é o único jeito de colocar conhecimentos prévios na modelagem2. Um caso bastante emblemático aparece nas curvas de crédito divulgadas pela ANBIMA3. Lá, ajusta-se um conjunto de curvas que depende de 6 parâmetros e cada curva representa uma classificação de risco (que nem aquela em que o Brasil pode tomar downgrade4). Como os níveis de risco estão ordenados, é natural exigir que também exista uma ordenação entre as curvas. Sem entrar em detalhes, a ideia pode ser expressa assim: \\[\\beta_{AAA} &lt; \\beta_{AA} &lt; \\beta_{A} &lt; \\beta_{BBB} &lt; ...\\] O que é que isso tem a ver com programação quadrática? A resposta é que a inequação acima pode ser escrita como \\(A\\beta \\geq 0\\), de tal forma já existe uma teoria para resolver uma regressão linear simples com restrições desse tipo! Basta que ela seja vista como um problema de programação quadrática. 4.8.3 O pacote quadprog Existe um pacote de R para quase tudo, então, como não poderia deixar de ser, existe um pacote em R para resolver problemas do tipo: \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }Ax \\geq 0.\\] Para ilustrar o seu uso, vamos considerar um exemplo. Vamos simular um conjunto de dados em que \\(\\beta_5 = 0.31, \\beta_4 = 0.43, \\beta_3 = 1.31, \\beta_2 = 2.19, \\beta_1 = 2.29\\) são os valores reais que precisamos estimar, considere que vale \\[Y \\approx \\beta_1X_1 + \\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5\\] e que o erro de regressão tem distribuição normal. Se soubermos antecipadamente que valem as seguintes afirmações \\[ \\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5 &gt; 0 \\text{ e } \\beta_1 &gt; \\beta_2 &gt; \\beta_3 &gt; \\beta_4 &gt; \\beta_5,\\] a minimização de \\((Y-\\beta X)&#39;(Y-\\beta X)\\) pode ser resolvida usando a função solve.QP. Tudo que precisamos fazer é escrever o conjunto de inequações na forma \\(A\\beta \\geq 0\\). Mas isso é bem fácil! Basta notar que as restrições são equivalentes à \\[ \\left(\\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\\\ \\end{array}\\right) \\times \\left(\\begin{array}{c}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\end{array}\\right) \\geq 0.\\] Dessa forma, o problema está prontinho pra passar no moedor de carne, com uma última ressalva. O problema resolvido no solve.QP é \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }A&#39;x \\geq 0,\\] então vamos ter que tomar o cuidado de passar as nossas restrições através do transposto da matriz que obtivemos acima. Isso resultará na matriz \\(A\\). Para checar como valeu a pena todo esse esforço, dá uma olhada na diferença entre as estimativas! Os pontinhos vermelhos são as estimativas do modelo irrestrito, enquanto as barras são as estimativas do modelo com restrições. 4.8.4 Conclusões Regressão linear simples é um problema de programação quadrática. Algumas restrições interessantes podem ser escritas na forma \\(B\\beta \\geq 0\\). Programação quadrática resolve regressão linear simples com restrições lineares. Se em algum dia você topar com um bicho desses, o quadprog pode resolver o problema pra você. A menos que você seja uma pessoa razoável bayesiano.↩ http://www.anbima.com.br/data/files/05/43/3E/84/E12D7510E7FCF875262C16A8/metodologia-curvas_20credito_20131104_v2_1_.pdf↩ http://economia.estadao.com.br/noticias/geral,agravamento-da-crise-politica-eleva-risco-de-rebaixamento-do-brasil-diz-sep,70001824274↩ "],
["4-9-filtros-de-bloom-em-r.html", "4.9 Filtros de Bloom em R", " 4.9 Filtros de Bloom em R Filtro de Bloom é um algoritmo muito interessante para testar se um elemento pertence a um conjunto. Ele é considerado uma estrutura de dados probabilística, ou seja, o resultado pode não estar correto com alguma probabilidade. Especificamente para o filtro de bloom, existe a possibilidade de falsos positivos mas não de falsos negativos: o algoritmo pode dizer que o elemento pertence ao conjunto, mas na verdade não pertencer, mas nunca dirá que ele não pertence sendo que ele pertence. Bloom Filters são úteis em diversas situações, geralmente relacionadas ao ganho de velocidade e de espaço que o seu uso pode trazer. Muitos sistemas de bancos de dados usam bloom filters para reduzir o número de buscas no disco (ex. Cassandra). O Medium usa para evitar recomendar uma paǵina que você já leu. Recentemente, encontraram até aplicações para bloom filters em machine learning. Nesse post vamos implementar uma versão simplificada, nada otimizada dos filtros de Bloom em R. Mas antes disso, vale a pena ler o verbete da Wikipedia sobre o assunto. Essencialmente, um filtro de bloom é um vetor de TRUEs e FALSES de tamanho \\(m\\). Inicializamos esse vetor com FALSES. Em seguida para cada elemento do conjunto que você deseja representar pelo filtro, repetimos o seguinte processo: Hasheamos o elemento usando \\(k\\) funções de hash diferentes. Cada uma dessas funções indicará um elemento do vetor que deve ser marcado como TRUE. Armazenamos então esse vetor de bits. São os valores de \\(m\\) e de \\(k\\) que controlam a probabilidade de falsos positivos. Veja como podemos criar uma função em R para fazer essas operações. Essa função inicializa o vetor de bits de tamanho \\(m\\) com FALSES e em seguida, para cada uma das \\(k\\) funções de hash (no caso apenas variamos a semente do hash MurMur32) e para cada elemento de x calculamos o elemento do vetor vec que deve se tornar TRUE. No final, ela retorna o vetor vec, onde armazenamos como atributos os parâmetros usados na sua construção. library(digest) library(magrittr) criar_vetor_de_bits &lt;- function(x, m = 1000, k = 7){ vec &lt;- rep(FALSE, m) for (i in 1:k) { for (j in 1:length(x)) { hash &lt;- digest(x[j], algo = &quot;murmur32&quot;, serialize = FALSE, seed = i) %&gt;% Rmpfr::mpfr(base = 16) %% m %&gt;% as.integer() vec[hash + 1] &lt;- TRUE } } # armazenamos os parâmetros usados na construção attributes(vec) &lt;- list(m = m, k= k) return(vec) } Dado um conjunto de strings, podemos criar o vetor de bits que o representa. vect &lt;- criar_vetor_de_bits(c(&quot;eu&quot;, &quot;pertenco&quot;, &quot;ao&quot;, &quot;conjunto&quot;, &quot;de&quot;, &quot;strings&quot;), m = 1000, k = 7) Agora vamos definir uma função que verifica se uma string pertence ao conjunto, dada apenas a representação dos bits desse conjunto. Hasheamos o elemento que desejamos verificar a presença no conjunto com a primeira função de hash. Se ela indicar um elemento do vetor que já está marcado com TRUE então continuamos, se não, retorna FALSE indicando que o elemento não pertence ao conjunto. Continuamos até acabarem as funções de hash ou até 1 FALSE ter sido retornado. verificar_presenca &lt;- function(x, vetor_de_bits){ k &lt;- attr(vetor_de_bits, &quot;k&quot;) m &lt;- attr(vetor_de_bits, &quot;m&quot;) for(i in 1:k){ hash &lt;- digest(x, algo = &quot;murmur32&quot;, serialize = FALSE, seed = i) %&gt;% Rmpfr::mpfr(base = 16) %% m %&gt;% as.integer() if(!vetor_de_bits[hash + 1]) { return(FALSE) } } return(TRUE) } verificar_presenca(&quot;nao&quot;, vect) verificar_presenca(&quot;eu&quot;, vect) verificar_presenca(&quot;abc&quot;, vect) Com m = 1000 e k = 7 não consegui encontrar nenhum falso positivo, mas basta diminuir o tamanho de m e de k que encontraremos. No verbete da Wikipedia a conta está bonitinha mas de fato a probabilidade de falsos positivos pode ser estimada em função dos parâmetros \\(k\\) e \\(m\\) e \\(n\\) (tamanho do conjunto representado) é dada por \\[(1 - e^{-kn/m})^k\\] No caso apresentado, a probabilidade de colisão é de 1.991256e-10. "],
["4-10-modelando-a-variancia-da-normal.html", "4.10 Modelando a variância da normal", " 4.10 Modelando a variância da normal Verificar as suposições dos modelos é muito importante quando fazemos inferência estatística. Em particular, a suposição de homocedasticidade5 dos modelos de regressão linear é especialmente importante, pois modifica o cálculo de erros padrão, intervalos de confiança e valores-p. Neste post, vou mostrar três pacotes do R que ajustam modelos da forma \\[ Y_i = \\beta_0 + \\sum_{k=1}^p\\beta_kx_{ik} + \\epsilon_i, \\ i = 1,\\ldots,n\\] \\[ \\epsilon_{i} \\sim \\textrm{N}(0,\\sigma_i), \\ i = 1,\\ldots,n \\ \\textrm{independentes, com }\\sigma_i^2 = \\alpha x_i^2. \\] Além de mostrar como se faz, também vou ilustrar o desempenho dos pacotes em um exemplo simulado. O modelo que gerará os dados do exemplo terá a seguinte forma funcional \\[ Y_i = \\beta x_i + \\epsilon_i, \\ i = 1,...n \\] \\[ \\epsilon_i \\sim N(0, \\sigma_i)\\text{ independentes, com }\\sigma_i = \\alpha\\sqrt{|x_i|},\\] e os parâmetros do modelo serão os valores \\(\\beta = 1\\) e \\(\\alpha = 4\\). A heterocedasticidade faz com que os pontos desenhem um cone ao redor da reta de regressão. 4.10.1 Usando o pacote gamlss Quando se ajusta um GAMLSS, você pode modelar os parâmetros de locação, escala e curtose ao mesmo tempo em que escolhe a distribuição dos dados dentre uma grande gama de opções. Escolhendo a distribuição normal e modelando apenas os parâmetros de locação e escala, o GAMLSS ajusta modelos lineares normais com heterocedasticidade. No código abaixo, o parâmetro formula = Y ~ X-1 indica que a função de regressão será constituída por um preditor linear em X sem intercepto. Já o parâmetro sigma.formula = ~X2-1 indica que o desvio padrão será modelado por um preditor linear em X2 (ou raiz de X), também sem intercepto. FALSE GAMLSS-RS iteration 1: Global Deviance = 17872.29 FALSE GAMLSS-RS iteration 2: Global Deviance = 17870.67 FALSE GAMLSS-RS iteration 3: Global Deviance = 17870.67 Conforme descrito no sumário abaixo, a estimativa de alfa está muito abaixo do valor simulado. FALSE ****************************************************************** FALSE Family: c(&quot;NO&quot;, &quot;Normal&quot;) FALSE FALSE Call: gamlss::gamlss(formula = Y ~ X - 1, sigma.formula = ~X2 - FALSE 1, family = NO(), data = dataset) FALSE FALSE Fitting method: RS() FALSE FALSE ------------------------------------------------------------------ FALSE Mu link function: identity FALSE Mu Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X 0.996942 0.005131 194.3 &lt;2e-16 *** FALSE --- FALSE Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 FALSE FALSE ------------------------------------------------------------------ FALSE Sigma link function: log FALSE Sigma Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X2 0.1791449 0.0009606 186.5 &lt;2e-16 *** FALSE --- FALSE Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 FALSE FALSE ------------------------------------------------------------------ FALSE No. of observations in the fit: 1000 FALSE Degrees of Freedom for the fit: 2 FALSE Residual Deg. of Freedom: 998 FALSE at cycle: 3 FALSE FALSE Global Deviance: 17870.67 FALSE AIC: 17874.67 FALSE SBC: 17884.49 FALSE ****************************************************************** 4.10.2 Usando o pacote dglm Quando se ajusta um Modelo Linear Generalizado Duplo (MLGD em português e DGLM em inglês), você tem uma flexibilidade parecida com a de um GAMLSS. Entretanto, você não pode definir um modelo para a curtose e a classe de distribuições disponível é bem menor. O código abaixo, similar ao utilizado para ajustar o GAMLSS, ajusta um DGLM aos dados simulados. Novamente, verifica-se que o alfa estimado está muito distante do verdadeiro alfa. FALSE FALSE Call: dglm(formula = Y ~ X - 1, dformula = ~X2 - 1, family = gaussian, FALSE data = dataset, method = &quot;reml&quot;) FALSE FALSE Mean Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X 0.9969432 0.008981392 111.001 0 FALSE (Dispersion Parameters for gaussian family estimated as below ) FALSE FALSE Scaled Null Deviance: 27197.48 on 1000 degrees of freedom FALSE Scaled Residual Deviance: 3090.08 on 999 degrees of freedom FALSE FALSE Dispersion Coefficients: FALSE Estimate Std. Error z value Pr(&gt;|z|) FALSE X2 0.3577322 0.001166004 306.8019 0 FALSE (Dispersion parameter for Gamma family taken to be 2 ) FALSE FALSE Scaled Null Deviance: 1628.301 on 1000 degrees of freedom FALSE Scaled Residual Deviance: 6526.59 on 999 degrees of freedom FALSE FALSE Minus Twice the Log-Likelihood: 17870.76 FALSE Number of Alternating Iterations: 18 4.10.3 Usando o pacote rstan Stan é uma linguagem de programação voltada para descrever e manipular objetos probabilísticos, como por exemplo variáveis aleatórias, processos estocásticos, distribuições de probabilidades etc. Essa linguagem foi projetada para tornar intuitivo e simples o ajuste de modelos estatísticos. Em particular, a forma de descrever modelos bayesianos é bem cômoda. O stan possui várias interfaces para R. A mais básica é o rstan, que será utilizada aqui. A principal função desse pacote é a função rstan, que possui dois parâmetros básicos: um parâmetro model_code =, que recebe um código que descreve o modelo na linguagem stan. um parâmetro data =, que recebe uma lista contendo os inputs do modelo, tais como dados coletados, parâmetros de distribuições a priori, etc. Embora esse seja o mínimo que a função precisa, também podemos passar outras componentes. O parâmetro verbose = FALSE faz com que a função não imprima nada enquanto roda e o parâmetro control = list(...) passa uma lista de opções de controle para o algoritmo de ajuste. O retorno da função stan() é um objeto do tipo stanfit, que pode ser sumarizado da mesma forma que outros modelos em R, utilizando a função summary() e a função plot(). O código abaixo ilustra a aplicação da função stan() ao nosso exemplo. A figura abaixo descreve os intervalos de credibilidade obtidos para cada parâmetro do modelo. O ponto central de cada intervalo representa as estimativas pontuais dos parâmetros. Como se nota, as estimativas do modelo utilizando stan estão bem próximas dos valores verdadeiros. Uma regressão linear é homocedástica quando a variabilidade dos erros não depende das covariáveis do modelo.↩ "],
["5-transformacao.html", "Capítulo 5 Transformação ", " Capítulo 5 Transformação "],
["5-1-unindo-mapas-the-tidy-way.html", "5.1 Unindo mapas: the tidy way", " 5.1 Unindo mapas: the tidy way Autor: Julio Dificuldade alta model Eu precisava escrever esse post porque estou morrendo de tesão por essa linguagem maravilhosa. O R é incrível. O tidyverse é incrível. Estatística é incrível. Não me aguento! Hoje mais uma vez fui salvo por uma feature pensada no universo tidy. Dessa vez, o grande herói foi o pacote sf, um pacote ainda em estágio de desenvolvimento, mas que já considero pacas. O sf, a.k.a. Simple Features, é um pacote para trabalhar com mapas. Com ele é possível fazer projeções, gráficos, leitura/gravação de diversos formatos de mapas, entre muitas outras coisas. Esse é um dos pacotes patrocinados pelo R Consortium, uma iniciativa criada por várias empresas e pela R Foundation para injetar dinheiro em projetos de R que tenham alta relevância. O pacote sf é tão sensacional que merece um post só para ele. Hoje, eu falarei de apenas de uma de suas vantagens, que é sua integração perfeita com o tidyverse. 5.1.1 Contexto Eu faço parte da Associação Brasileira de Jurimetria (ABJ), uma entidade sem fins lucrativos que faz pesquisa aplicada na área do Direito. Na ABJ produzimos diversas soluções tecnológicas para problemas do Direito, especialmente em questões relacionadas à elaboração de políticas públicas e administração dos tribunais. Num projeto recente, eu precisava levantar uma tabela de municípios, comarcas, circunscrições e regiões administrativas do Tribunal de Justiça de São Paulo. Minha tarefa era visualizar essas informações em mapas. Fazendo curta uma história longa, podemos dizer que: uma comarca é o conjunto de um ou mais municípios contíguos (contíguos = municípios que se tocam); uma circunscrição é um conjunto de uma ou mais comarcas contíguas; e uma região é… adivinha? No Estado de São Paulo, temos no total: 645 municípios 319 comarcas 57 circunscrições 10 regiões administrativas O problema é que não existe uma base de dados pública que relacione todos os 645 municípios com as respectivas regiões que as contêm. Então partimos para a obtenção via robizinhos. P.S.: Não se esqueça do ciclo da ciência de dados! Vou falar disso sem parar para explicar. Na dúvida, leia o http://r4ds.had.co.nz/ As partes de import e tidy desse projeto processo foram sofríveis. Precisei fazer vários web scrapers e diversos códigos para lidar com nomes zoados dos municípios de São Paulo, como Moji/Mogi, Estrela D’Oeste/Doeste, Brodowski/Brodosqui etc. Quem quiser pode dar uma olhada nos códigos do github da ABJ. 5.1.2 O problema Após passar o dia todo pegando esses dados, finalmente cheguei na parte de transform e visualize. Nossos objetivos nessa parte são: Carregar um mapa dos municípios de São Paulo no R. Agrupar as formas dos municípios em comarcas, circunscrições e regiões. Fazer mapas dos resultados obtidos. Em experiências anteriores já passei por maus bocados tentando completar a parte (2). Antigamente, a única função conhecida que fazia a união de polígonos era a maptools::unionSpatialPolygons(), que é contraintuitiva, mal documentada e difícil de usar na prática. Na verdade, tudo era difícil: desde ler o arquivo .shp baixado da internet até a parte de fazer o gráfico. Era R raiz mesmo. Mas hoje, para a felicidade de todos, temos a solução nutella ;) Curiosamente, o Edzer Pebesma, autor do sf, também é contributor do maptools e mantenedor do sp, os pacotes mais importantes de análise espacial pré-tidyverse. Ou seja, ele não só manja do assunto como sabia que os pacotes dele precisavam melhorar. E, olha, ele mandou bem dessa vez. 5.1.3 Solução Vamos utilizar o tidyverse e o pacote sf. Para instalar o tidyverse, rode Para quem não sabe: o tidyverse é um pacote que instala muitos outros pacotes por trás, como httr, ggplot2 e dplyr. Isso significa que pode demorar bastante tempo para instalar! A instalação do sf pode ser um tanto trabalhosa. Se você usa Windows, basta instalar o Rtools e depois rodar Se você usa Mac ou Linux, recomendo ler a primeira página da documentação oficial do pacote. Lá você pode checar todos os requerimentos do pacote em detalhe. Eu recomendo que você instale logo a versão de desenvolvimento: Para rodar as funções gráficas, também recomendo que você instale a versão de desenvolvimento do ggplot2, rodando Outros pacotes que usaremos no meio do código são 5.1.3.1 Parte 1: baixando os dados Primeiro, vamos baixar da internet! Obviamente, o melhor lugar para baixar esses arquivos é no FTP do Instituto Brasileiro de Geografia e Estatística. Utilizamos o pacote httr para baixar o arquivo zipado. E usamos unzip() para dezipar os dados. No final, você terá esses arquivos na pasta: Para ler esses arquivos estranhos num objeto do R, utilizamos a função sf::st_read(), simples assim: Observe que temos três colunas na base de dados, nome do município, código do município e geometry. Essa terceira é do tipo simple_feature e carrega objetos do tipo MULTIPOLYGON. Ou seja, um objeto lido pelo sf nada mais é do que um data.frame que tem uma coluna especial, capaz de guardar objetos mais complexos, como polígonos. Chamamos esse tipo de coluna especial de list-column. Quem faz nossos cursos avançados acaba aprendendo esses conceitos a partir do aninhamento de objetos e utilização de algoritmos mais complexos usando o pacote purrr. Agora, queremos juntar essa base com nossos dados de comarcas, circunscrições e regiões. Para isso, vamos primeiro arrumar os nomes de d_sf_municipio e depois usar dplyr::inner_join(), assim: Se você quiser fazer um gráfico feinho só pra saber o que está rolando, use plot(): 5.1.3.2 Parte 2: agrupando os municípios Aqui é onde a mágica acontece! Para unir os polígonos do mapa, por incrível que pareça, utilizaremos o pacote dplyr. O autor do pacote sf, Edzer Pebesma, teve a incrível ideia de criar um método S3 para objetos do tipo sf (como é nosso caso), já fazendo algumas operações para nós. Na prática, o pacote estende e reimplementa mais de 20 funções provenientes do dplyr. Veja ?sf::dplyr para detalhes. No nosso caso, vamos usar group_by() (ou sf::group_by.sf()) e summarise() (ousf::summarise.sf()): Simples assim! Note que agora temos 319 objetos, que é exatamente o número de comarcas. Se quiser adicionar mais variáveis, basta incluí-las no summarise(): Para criar d_sf_circunscricao e d_sf_regiao utilizamos o mesmo procedimento. 5.1.3.3 Parte 3: montando os gráficos Agora digamos que você tenha essa lista de mapas E você quer desenhar mapas com esses títulos Vamos usar o purrr::map2() para montar nossos gráficos em ggplot2 e guardar numa lista. Internamente, utilizaremos o geom_sf(), uma extensão do ggplot2 criada para tratar objetos do pacote sf. Um código minimalista seria Você pode usar a função gridExtra::grid.arrange() para juntar essa lista de gráficos em um gráfico só, rodando O resultado final é o gráfico abaixo. Coisa mais linda! 5.1.4 Wrap-up O pacote sf coloca a análise espacial no tidyverse Use st_read() para ler shapefiles (arquivos SHP) O objeto resultante é um data.frame com uma coluna geometry especial que guarda os polígonos. Várias funções do dplyr foram reimplementadas para funcionar com o sf. summarise, por exemplo, faz a união de polígonos, o que resolve nosso problema. Você pode usar geom_sf() para fazer gráficos com sf usando o poderoso ggplot2. É isso pessoal. Espero que seja tão life saver para vocês como foi para mim. Happy coding ;) "],
["5-2-environments.html", "5.2 Environments", " 5.2 Environments Autor: William Dificuldade alta conceito Se você utiliza o R regularmente, com certeza já se deparou com o termo environment. Ele aparece como um painel do RStudio, quando acessamos o código de uma função e (implicitamente) quando carregamos pacotes. Neste post, vamos tentar responder as três perguntas básicas sobre qualquer coisa no R: 1. o que é? 2. para que serve? e 3. como NÃO usar? 5.2.1 O que é? Definindo de uma maneira bem simples, environments são locais onde objetos são armazenados, isto é, conjuntos de ligações entre símbolos e valores. Por exemplo, quando fazemos a atribuição abaixo, a &lt;- 4 estamos criando uma associação do símbolo a ao valor 4, que, por padrão, é guardada dentro do global environment. ls(globalenv()) [1] &quot;a&quot; Assim, quando rodarmos o símbolo a, o R, por padrão, vai procurar dentro desse environment um valor para devolver. No caso, o valor 4. a [1] 4 Mais formalmente, environments podem ser definidos como a junção de duas coisas: um conjunto de pares (símbolo, valor); e um ponteiro para um outro environment. Quando o R não encontra um valor para um símbolo no environment em que está procurando, ele passa a procurar no próximo, o environment para qual o primeiro está apontando, chamado de environment pai. Assim, os environments se estruturam como uma árvore, cuja raiz é um environment vazio. emptyenv() &lt;environment: R_EmptyEnv&gt; 5.2.2 O que faz? É possível criar novos environments com a função new.env() magrathea &lt;- new.env() e criar objetos dentro desse environments com a função assign() assign(&quot;a&quot;, 8, envir = magrathea) ls(magrathea) [1] &quot;a&quot; Agora temos um objeto chamado a no global environment e no magrathea, que nós criamos. Note que o R inicia a busca no global environment. a [1] 4 Vamos agora criar outro objeto dentro de magrathea. assign(&quot;b&quot;, 15, envir = magrathea) Observe que se procurarmos simplesmente por b, o R não vai encontrar um valor para associar. b Error: object &#39;b&#39; not found Acontece que magrathea é um environment “abaixo” do global na hierarquia, e o R só estende a sua busca para environments acima (sim, estou pensando numa árvore de ponta-cabeça). parent.env(magrathea) &lt;environment: R_GlobalEnv&gt; Se criarmos agora um objeto no global c &lt;- 16 e usarmos a função get() para procurá-lo no environment que criamos, o R irá encontrá-lo porque o global é o environment pai de magrathea. get(&quot;c&quot;, envir = magrathea) [1] 16 Essa estrutura é muito útil na hora de utilizar funções. Sempre que uma função é chamada, um novo environment é criado, o environment de avaliação, que contém os objetos usados como argumento da função, os objetos criados dentro da função e aponta para o environment onde a função foi criada (geralmente o global). f &lt;- function(a, b) { c &lt;- a + b return(c) } environment(f) &lt;environment: R_GlobalEnv&gt; Esse comportamento nos permite fazer duas coisas. Primeiro, os cálculos realizados dentro das funções não modificam os objetos do global. f(23, 42) c [1] 65 [1] 16 Segundo, podemos utilizar objetos dentro da função sem defini-los lá dentro. f &lt;- function(b) { return(a + b) } f(108) [1] 112 Neste caso, como o R não encontrou o símbolo a dentro do environment de avaliação, ele foi buscar no pai, o global. 5.2.3 Como não usar? Agora que temos uma visão ao menos superficial da estrutura de environments, podemos entender melhor porque usar a função attach() é uma prática não recomendada ao programar em R. Se utilizarmos a função search(), ela nos devolverá o “caminho” de environments, começando do global (magrathea não será exibido). search() [1] &quot;.GlobalEnv&quot; &quot;tools:rstudio&quot; [3] &quot;package:stats&quot; &quot;package:graphics&quot; [5] &quot;package:grDevices&quot; &quot;package:utils&quot; [7] &quot;package:datasets&quot; &quot;package:methods&quot; [9] &quot;Autoloads&quot; &quot;package:base&quot; Repare que os pacotes carregados geram um novo environment na árvore. library(ggplot2) search() [1] &quot;.GlobalEnv&quot; &quot;package:ggplot2&quot; [3] &quot;tools:rstudio&quot; &quot;package:stats&quot; [5] &quot;package:graphics&quot; &quot;package:grDevices&quot; [7] &quot;package:utils&quot; &quot;package:datasets&quot; [9] &quot;package:methods&quot; &quot;Autoloads&quot; [11] &quot;package:base&quot; É por isso que, ao carregar um pacote, podemos utilizar as suas funções sem a necessidade de escrever coisas do tipo ggplot2::geom_point(). Agora, veja o que acontece quando usamos a função attach() mighty &lt;- list( &quot;Jason&quot; = &quot;vermelho&quot;, &quot;Zach&quot; = &quot;Preto&quot;, &quot;Billy&quot; = &quot;Azul&quot;, &quot;Trini&quot; = &quot;Amarela&quot;, &quot;Kimberly&quot; = &quot;Rosa&quot;, &quot;Thomas&quot; = &quot;Verde&quot; ) attach(mighty) search() [1] &quot;.GlobalEnv&quot; &quot;mighty&quot; [3] &quot;package:ggplot2&quot; &quot;tools:rstudio&quot; [5] &quot;package:stats&quot; &quot;package:graphics&quot; [7] &quot;package:grDevices&quot; &quot;package:utils&quot; [9] &quot;package:datasets&quot; &quot;package:methods&quot; [11] &quot;Autoloads&quot; &quot;package:base&quot; Um novo environment mighty é criado acima do global! Isso quer dizer que se você não tiver total conhecimento dos objetos que estão sendo anexados, você estará criando uma lista de objetos “invisíveis” que podem ser avaliados mesmo dentro de funções. E veja o que acontece quando carregamos mais pacotes library(dplyr) search() Attaching package: ‘dplyr’ The following objects are masked from ‘package:stats’: filter, lag The following objects are masked from ‘package:base’: intersect, setdiff, setequal, union [1] &quot;.GlobalEnv&quot; &quot;package:dplyr&quot; [3] &quot;mighty&quot; &quot;package:ggplot2&quot; [5] &quot;tools:rstudio&quot; &quot;package:stats&quot; [7] &quot;package:graphics&quot; &quot;package:grDevices&quot; [9] &quot;package:utils&quot; &quot;package:datasets&quot; [11] &quot;package:methods&quot; &quot;Autoloads&quot; [13] &quot;package:base&quot; O environment do pacote dplyr aparece antes do mighty. Isso quer dizer que os objetos do mighty podem ser mascarados por todos os pacotes que você carregar a seguir. Veja um simples exemplo de como as coisas podem dar errado. dados &lt;- tibble::tibble( paciente = 1:30, cancer = rbinom(30, size = 1, prob = 0.5) ) attach(dados) cancer [1] 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 [20] 1 0 0 1 1 1 1 0 0 0 1 Com o código acima, criamos um banco de dados representando 30 pacientes com (1) ou sem (0) um certo tipo de câncer. As variáveis paciente e cancer foram anexadas ao rodarmos attach(dados). Agora, imagine se esse banco de dados tiver informações de tempo até a remissão do câncer e quisermos rodar modelos de sobrevivência. Um passo natural seria carregar a biblioteca survival. library(survival) search() Attaching package: ‘survival’ The following object is masked from ‘dados’: cancer [1] &quot;.GlobalEnv&quot; &quot;package:survival&quot; [3] &quot;dados&quot; &quot;package:dplyr&quot; [5] &quot;mighty&quot; &quot;package:ggplot2&quot; [7] &quot;tools:rstudio&quot; &quot;package:stats&quot; [9] &quot;package:graphics&quot; &quot;package:grDevices&quot; [11] &quot;package:utils&quot; &quot;package:datasets&quot; [13] &quot;package:methods&quot; &quot;Autoloads&quot; [15] &quot;package:base&quot; O pacote survival também tem um objeto chamado cancer. Assim, ao carregá-lo, o environment survival ficará na frente do environment dados na árvore e, se não prestarmos atenção com o warning, esse será o nosso novo objeto cancer. head(cancer) inst time status age sex ph.ecog 1 3 306 2 74 1 1 2 3 455 2 68 1 0 3 3 1010 1 56 1 0 4 5 210 2 57 1 1 5 1 883 2 60 1 0 6 12 1022 1 74 1 1 ph.karno pat.karno meal.cal wt.loss 1 90 100 1175 NA 2 90 90 1225 15 3 90 90 NA 15 4 90 60 1150 11 5 100 90 NA 0 6 50 80 513 0 Assim, se for utilizar a função attach() é preciso ter muito cuidado com o que se está fazendo. E a melhor dica é não use. Esse texto foi apenas uma introdução sobre como os environments funcionam. Ainda existe muito mais coisa por trás, como o conceito de namespaces. Se você quiser saber mais, recomendo como primeira parada esse post, do qual tirei boa parte das informações passadas aqui. Também vale a pena dar uma olhada nas definições nesse link. "],
["5-3-arrumando-banco-de-dados-o-pacote-janitor.html", "5.3 Arrumando banco de dados: o pacote janitor", " 5.3 Arrumando banco de dados: o pacote janitor No primeiro post sobre arrumação de base de dados, a gente viu como usar as funções do stringr para arrumar o nome das variáveis. Seguindo a dica do Julio, o quebrador de captchas, vamos falar do pacote janitor, que traz algumas funções para dar aquele trato nas BDs. Antes de mais nada, instale e carregue o pacote: install.packages(&quot;janitor&quot;) devtools::install_github(&quot;sfirke/janitor&quot;) # Versão de desenvolvimento library(tidyverse) library(janitor) 5.3.1 Arrumando o nome das variáveis Assim como no post passado, utilizaremos a base com informações de pacientes com arritmia cardíaca, cujas variáveis selecionadas foram: dados %&gt;% names FALSE [1] &quot;ID&quot; &quot;Sexo&quot; &quot;Nascimento&quot; FALSE [4] &quot;Idade&quot; &quot;Inclusão&quot; &quot;Cor&quot; FALSE [7] &quot;Peso&quot; &quot;Altura&quot; &quot;cintura&quot; FALSE [10] &quot;IMC&quot; &quot;Superfície corporal&quot; &quot;Tabagismo&quot; FALSE [13] &quot;cg.tabag (cig/dia)&quot; &quot;Alcool (dose/semana)&quot; &quot;Drogas ilícitas&quot; FALSE [16] &quot;Cafeína/dia&quot; &quot;Refrig/dia&quot; &quot;Sedentario&quot; FALSE [19] &quot;ativ. Fisica&quot; Os nomes têm letras maiúsculas, acentos, parênteses, pontos e barras, o que atrapalha na hora da programação. Para resolver esse problema, usamos a função clean_names(). dados %&gt;% clean_names() %&gt;% names FALSE [1] &quot;id&quot; &quot;sexo&quot; &quot;nascimento&quot; FALSE [4] &quot;idade&quot; &quot;inclusao&quot; &quot;cor&quot; FALSE [7] &quot;peso&quot; &quot;altura&quot; &quot;cintura&quot; FALSE [10] &quot;imc&quot; &quot;superficie_corporal&quot; &quot;tabagismo&quot; FALSE [13] &quot;cg_tabag_cig_dia&quot; &quot;alcool_dose_semana&quot; &quot;drogas_ilicitas&quot; FALSE [16] &quot;cafeina_dia&quot; &quot;refrig_dia&quot; &quot;sedentario&quot; FALSE [19] &quot;ativ_fisica&quot; Veja que a função removeu os parênteses, pontos e barras e substituiu os espaços por _. No entanto, ela não remove os acentos. Assim, podemos adicionar mais uma linha ao pipeline para chegar onde queremos. dados %&gt;% clean_names() %&gt;% names %&gt;% abjutils::rm_accent() FALSE [1] &quot;id&quot; &quot;sexo&quot; &quot;nascimento&quot; FALSE [4] &quot;idade&quot; &quot;inclusao&quot; &quot;cor&quot; FALSE [7] &quot;peso&quot; &quot;altura&quot; &quot;cintura&quot; FALSE [10] &quot;imc&quot; &quot;superficie_corporal&quot; &quot;tabagismo&quot; FALSE [13] &quot;cg_tabag_cig_dia&quot; &quot;alcool_dose_semana&quot; &quot;drogas_ilicitas&quot; FALSE [16] &quot;cafeina_dia&quot; &quot;refrig_dia&quot; &quot;sedentario&quot; FALSE [19] &quot;ativ_fisica&quot; E para substituir na base. nomes &lt;- dados %&gt;% clean_names() %&gt;% names %&gt;% abjutils::rm_accent() names(dados) &lt;- nomes 5.3.2 Removendo linhas e colunas vazias Esse banco de dados também tinha outro problema: linhas vazias. Na verdade, elas não eram completamente vazias, pois havia algumas informações de identificação do paciente, mas nenhuma outra variável tinha sido computada. dados[3,] FALSE # A tibble: 1 x 19 FALSE id sexo nascimento idade inclusao cor peso FALSE &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; FALSE 1 3 &lt;NA&gt; NA NA NA &lt;NA&gt; NA FALSE # … with 12 more variables: altura &lt;dbl&gt;, cintura &lt;chr&gt;, imc &lt;dbl&gt;, FALSE # superficie_corporal &lt;chr&gt;, tabagismo &lt;chr&gt;, cg_tabag_cig_dia &lt;dbl&gt;, FALSE # alcool_dose_semana &lt;dbl&gt;, drogas_ilicitas &lt;chr&gt;, cafeina_dia &lt;dbl&gt;, FALSE # refrig_dia &lt;dbl&gt;, sedentario &lt;chr&gt;, ativ_fisica &lt;chr&gt; Essa foi a solução que eu pensei para resolver o problema utilizando a função remove_empty_row(). dados %&lt;&gt;% as.data.frame %&gt;% `row.names&lt;-`(dados$id) %&gt;% dplyr::select(-id) %&gt;% remove_empty_rows() %&gt;% mutate(id = row.names(.)) %&gt;% dplyr::select(id, everything()) dados %&gt;% as_tibble() FALSE # A tibble: 4 x 19 FALSE id sexo nascimento idade inclusao cor peso FALSE &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; FALSE 1 1 F 1964-01-31 00:00:00 41 2006-02-17 00:00:00 bran… 75 FALSE 2 2 M 1959-01-28 00:00:00 45 2005-11-29 00:00:00 negra 71 FALSE 3 4 M 1957-09-13 00:00:00 50 2008-02-13 00:00:00 NT 80 FALSE 4 5 F 1938-02-06 00:00:00 71 2009-06-25 00:00:00 parda 56 FALSE # … with 12 more variables: altura &lt;dbl&gt;, cintura &lt;chr&gt;, imc &lt;dbl&gt;, FALSE # superficie_corporal &lt;chr&gt;, tabagismo &lt;chr&gt;, cg_tabag_cig_dia &lt;dbl&gt;, FALSE # alcool_dose_semana &lt;dbl&gt;, drogas_ilicitas &lt;chr&gt;, cafeina_dia &lt;dbl&gt;, FALSE # refrig_dia &lt;dbl&gt;, sedentario &lt;chr&gt;, ativ_fisica &lt;chr&gt; Eu precisei converter para data.frame primeiro porque não é possível definir os nomes das linhas de uma tibble. Se a linha estivesse completamente vazia, bastaria usar diretamente a função remove_empty_rows(). Equivalentemente para colunas, existe a função remove_empty_cols(). 5.3.3 Identificando linhas duplicadas O pacote janitor possui uma função para identificar entradas duplicadas numa base de dados: get_dupes(). Vamos criar uma base genérica para testá-la. p_nome &lt;- c(&quot;Athos&quot;, &quot;Daniel&quot;, &quot;Fernando&quot;, &quot;Julio&quot;, &quot;William&quot;) sobrenome &lt;- c(&quot;Damiani&quot;, &quot;Falbel&quot;, &quot;Corrêa&quot;, &quot;Trecenti&quot;, &quot;Amorim&quot;) base_qualquer &lt;- tibble(nome = sample(p_nome, 25, replace = T), sobrenome = sample(sobrenome, 25, replace = T), variavel_importante = rnorm(25)) get_dupes(base_qualquer, nome, sobrenome) FALSE # A tibble: 16 x 4 FALSE nome sobrenome dupe_count variavel_importante FALSE &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; FALSE 1 Athos Damiani 2 0.305 FALSE 2 Athos Damiani 2 -0.00796 FALSE 3 Athos Falbel 2 0.198 FALSE 4 Athos Falbel 2 -0.0443 FALSE 5 Athos Trecenti 2 -1.24 FALSE 6 Athos Trecenti 2 -0.729 FALSE 7 Daniel Damiani 2 0.335 FALSE 8 Daniel Damiani 2 3.07 FALSE 9 Fernando Trecenti 2 -0.190 FALSE 10 Fernando Trecenti 2 -0.868 FALSE 11 Julio Trecenti 4 -0.188 FALSE 12 Julio Trecenti 4 -1.39 FALSE 13 Julio Trecenti 4 0.528 FALSE 14 Julio Trecenti 4 1.06 FALSE 15 William Trecenti 2 -0.786 FALSE 16 William Trecenti 2 -0.505 Todas as linhas na tibble resultante representam uma combinação de nome-sobrenome repetida. 5.3.4 Outras funções Por fim, o janitor também tem funções equivalentes à table() para produzir tabelas de frequência: tabyl() - similar a table(), mas pipe-ável e com mais recursos. crosstab() - para tabelas de contingência. adorn_totals() - acrescenta o total das linhas ou colunas. adorn_crosstab() - deixa tabelas de contingência mais bonitas. mtcars %&gt;% tabyl(cyl) FALSE cyl n percent FALSE 4 11 0.34375 FALSE 6 7 0.21875 FALSE 8 14 0.43750 mtcars %&gt;% tabyl(cyl) %&gt;% adorn_totals FALSE cyl n percent FALSE 4 11 0.34375 FALSE 6 7 0.21875 FALSE 8 14 0.43750 FALSE Total 32 1.00000 mtcars %&gt;% crosstab(cyl, am) FALSE cyl 0 1 FALSE 4 3 8 FALSE 6 4 3 FALSE 8 12 2 mtcars %&gt;% crosstab(cyl, am) %&gt;% adorn_crosstab FALSE cyl 0 1 FALSE 1 4 27.3% (3) 72.7% (8) FALSE 2 6 57.1% (4) 42.9% (3) FALSE 3 8 85.7% (12) 14.3% (2) É isso! Espero que essas dicas e o pacote janitor ajudem a agilizar as suas análises :) "],
["5-4-skimr-estatisticas-basicas-com.html", "5.4 Skimr: estatísticas básicas com ❤️", " 5.4 Skimr: estatísticas básicas com ❤️ Entre os dias 25 e 27 de maio aconteceu a ROpenSci Unconf 2017. O encontro reuniu vários pop stars da comunidade R como Hadley Wickham, Joe Cheng (criador do shiny), Jeroen Ooms (criador do OpenCPU e autor de vários pacotes bacanas), Jenny Bryan (autora de vários pacotes bacanas como googlesheets), várias pessoas do #R-Ladies e muito mais. Uma coisa muito legal dessa conferência é que ela funcionou como uma hackathon. Foi criada uma nova organização no github chamada ROpenSci Labs, e os presentes simplesmente começaram a subir pacotes fantásticos lá dentro. Recomendo muito dar uma olhada. Dentre os pacotes que olhei, o que mais me chamou atenção foi o skimr e por isso estou fazendo esse post! O propósito do skimr é simples: fazer algumas estatísticas básicas univariadas de uma base de dados. O skimr ainda não está no CRAN, então para instalar recomendamos utilizar o devtools para instalar direto do GitHub, conforme código abaixo. Note que também será necessário instalar o pacote colformat do Hadley. devtools::install_github(&quot;hadley/colformat&quot;) devtools::install_github(&quot;ropenscilabs/skimr&quot;) A função skim() calcula estatísticas básicas das variáveis e imprime no seu console. Note que a função separa estatísticas para variáveis numéricas ou fatores. library(tidyverse) library(skimr) skim(iris) FALSE Skim summary statistics FALSE n obs: 150 FALSE n variables: 5 FALSE FALSE ── Variable type:factor ────────────────────────────────────────────────────────────── FALSE variable missing complete n n_unique top_counts FALSE Species 0 150 150 3 set: 50, ver: 50, vir: 50, NA: 0 FALSE ordered FALSE FALSE FALSE FALSE ── Variable type:numeric ───────────────────────────────────────────────────────────── FALSE variable missing complete n mean sd p0 p25 p50 p75 p100 FALSE Petal.Length 0 150 150 3.76 1.77 1 1.6 4.35 5.1 6.9 FALSE Petal.Width 0 150 150 1.2 0.76 0.1 0.3 1.3 1.8 2.5 FALSE Sepal.Length 0 150 150 5.84 0.83 4.3 5.1 5.8 6.4 7.9 FALSE Sepal.Width 0 150 150 3.06 0.44 2 2.8 3 3.3 4.4 FALSE hist FALSE ▇▁▁▂▅▅▃▁ FALSE ▇▁▁▅▃▃▂▂ FALSE ▂▇▅▇▆▅▂▂ FALSE ▁▂▅▇▃▂▁▁ E tem mais! O mais legal do skimr é que ele usa a função colformat::spark_bar() para desenhar histogramas direto no seu console! skim(iris) %&gt;% filter(stat == &#39;hist&#39;) %&gt;% knitr::kable(caption = &#39;HISTOGRAMA NA TABELA PORQUE SIM!&#39;) Tabela 2.3: HISTOGRAMA NA TABELA PORQUE SIM! variable type stat level value formatted Sepal.Length numeric hist .all NA ▂▇▅▇▆▅▂▂ Sepal.Width numeric hist .all NA ▁▂▅▇▃▂▁▁ Petal.Length numeric hist .all NA ▇▁▁▂▅▅▃▁ Petal.Width numeric hist .all NA ▇▁▁▅▃▃▂▂ O skimr também possui padrões de estatísticas básicas para cada tipo de variável. Você pode checar esses tipos com show_skimmers(): show_skimmers() %&gt;% map_df(enframe, .id = &#39;tipo&#39;) %&gt;% group_by(tipo) %&gt;% summarise(stats = glue::collapse(value, sep = &#39;, &#39;)) %&gt;% knitr::kable(caption = &#39;Estatísticas básicas para cada tipo de variável.&#39;) Tabela 2.4: Estatísticas básicas para cada tipo de variável. tipo stats AsIs missing, complete, n, n_unique, min_length, max_length character missing, complete, n, min, max, empty, n_unique complex missing, complete, n date missing, complete, n, min, max, median, n_unique Date missing, complete, n, min, max, median, n_unique difftime missing, complete, n, min, max, median, n_unique factor missing, complete, n, n_unique, top_counts, ordered integer missing, complete, n, mean, sd, p0, p25, p50, p75, p100, hist list missing, complete, n, n_unique, min_length, median_length, max_length logical missing, complete, n, mean, count numeric missing, complete, n, mean, sd, p0, p25, p50, p75, p100, hist POSIXct missing, complete, n, min, max, median, n_unique ts missing, complete, n, start, end, frequency, deltat, mean, sd, min, max, median, line_graph 5.4.1 Criando suas próprias funções Você também pode usar funções próprias com o skimr. Por exemplo, digamos que você queira calcular o coeficiente de variação. Primeiro, adicione sua função dentro de uma lista: funs &lt;- list(cv = function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)) e depois aplique a função skim_with(): # append adiciona as suas funcoes nas existentes skim_with(numeric = funs, append = TRUE) E pronto! Agora você pode rodar skim() novamente: skim(iris) %&gt;% filter(stat %in% c(&#39;hist&#39;, &#39;cv&#39;)) %&gt;% knitr::kable(caption = &#39;Histograma e coeficiente de variação.&#39;) Tabela 2.5: Histograma e coeficiente de variação. variable type stat level value formatted Sepal.Length numeric hist .all NA ▂▇▅▇▆▅▂▂ Sepal.Length numeric cv .all 0.1417113 0.14 Sepal.Width numeric hist .all NA ▁▂▅▇▃▂▁▁ Sepal.Width numeric cv .all 0.1425642 0.14 Petal.Length numeric hist .all NA ▇▁▁▂▅▅▃▁ Petal.Length numeric cv .all 0.4697441 0.47 Petal.Width numeric hist .all NA ▇▁▁▅▃▃▂▂ Petal.Width numeric cv .all 0.6355511 0.64 Para retornar ao skim() padrão, rode skim_with_defaults(). 5.4.2 Wrap up Instale usando devtools::install_github() Rode a função skim(). Use dplyr::filter() para filtrar as estatísticas de interesse. Você pode adicionar suas próprias estatísticas com skim_with(). Acompanhe a evolução do skimr nesta página. O pacote ainda vai evoluir muito e não duvido nada que seja um bom candidado a entrar no tidyverse. O que vocês acham? Escrevam nos comentários! É isso. Happy coding ;) "],
["5-5-pacotes-miojo-como-fazer-um-pacote-no-r-em-3-minutos.html", "5.5 Pacotes miojo: como fazer um pacote no R em 3 minutos", " 5.5 Pacotes miojo: como fazer um pacote no R em 3 minutos Autor: Julio Dificuldade média program Nesse post vou mostrar como fazer um pacote em R muito, muito rápido. Tirei várias coisas que costumo fazer nos pacotes, com dor no coração, tudo pela velocidade, mantendo só o essencial. Duas restrições que usei são O pacote precisa ficar disponível no GitHub. O pacote precisa ter pelo menos uma função. Essa é a solução que eu acho mais segura e rápida. Você também pode usar o próprio RStudio para criar pacotes ou clonar coisas do github, mas isso pode dar alguns bugs. 5.5.1 Passo 1: Crie um pacote no R Rode usethis::create_package(&quot;~/Documents/pacote&quot;) ✔ Setting active project to &#39;/home/jtrecenti/Documents/pacote&#39; ✔ Creating &#39;R/&#39; ✔ Creating &#39;man/&#39; ✔ Writing &#39;DESCRIPTION&#39; ✔ Writing &#39;NAMESPACE&#39; ✔ Writing &#39;pacote.Rproj&#39; ✔ Adding &#39;.Rproj.user&#39; to &#39;.gitignore&#39; ✔ Adding &#39;^pacote\\\\.Rproj$&#39;, &#39;^\\\\.Rproj\\\\.user$&#39; to &#39;.Rbuildignore&#39; ✔ Opening new project &#39;pacote&#39; in RStudio Pronto! pacote feito. 5.5.2 Passo 2: Adicione git e github no seu pacote usethis::use_git() FALSE ✔ Setting active project to &#39;/home/jtrecenti/Documents/livro-blog/livro&#39; ✔ Setting active project to &#39;/home/jtrecenti/Documents/pacote&#39; Criar repositório no GitHub: usethis::use_github(protocol = &quot;https&quot;) ● Check title and description Name: pacote Description: What the Package Does (One Line, Title Case) Are title and description ok? 1: No 2: No way 3: Yup 5.5.3 Parênteses: GITHUB_PAT Se você não tiver um GITHUB_PAT, tem um passo adicional: usethis::browse_github_pat() ✔ Opening URL https://github.com/settings/tokens/new?scopes=repo,gist&amp;description=R:GITHUB_PAT ● Call `edit_r_environ()` to open &#39;.Renviron&#39; and store your PAT with a line like: GITHUB_PAT=xxxyyyzzz ● Make sure &#39;.Renviron&#39; ends with a newline! e depois, rodando usethis::edit_r_environ() Adicione GITHUB_PAT=95d864ed372140f8d72f895d864ed372140f8d72f8 Salve e restarte sua sessão. 5.5.4 Passo 3: Adicione coisas de interesse README: usethis::use_readme_md() # ou usethis::use_readme_rmd() &gt; usethis::use_readme_md() ✔ Writing &#39;README.md&#39; ● Modify &#39;README.md&#39; Pipe (%&gt;%) usethis::use_roxygen_md() usethis::use_pipe() ✔ Setting Roxygen field in DESCRIPTION to &#39;list(markdown = TRUE)&#39; ✔ Setting RoxygenNote field in DESCRIPTION to &#39;6.1.1&#39; ● Run `devtools::document()` ✔ Adding &#39;magrittr&#39; to Imports field in DESCRIPTION ✔ Writing &#39;R/utils-pipe.R&#39; ● Run `devtools::document()` 5.5.5 Passo 4: Crie sua função Exemplo: #&#39; Soma 2 #&#39; #&#39; Recebe um vetor de números e retorna um vetor de números somando dois #&#39; #&#39; @param x vetor de números. #&#39; #&#39; @export soma_2 &lt;- function(x) { x + 2 } Crie a função dentro de um arquivo com extensão .R na pasta R As informações que começam com #' acima da função servem para documentar. Nesse caso, a primeira linha é o título a segunda linha é a descrição a parte que começa com @param descreve o que é o parâmetro de entrada a parte que começa com @export diz para o pacote que essa função deve estar disponível para o usuário quando ele rodar library(nomeDoPacote). 5.5.6 Passo 5: document, commit e push! Rode devtools::document(). Commite suas alterações. Dê um push! Se não saba o que é commitar e pushar, veja o artigo do Athos sobre o uso do git e do GitHub. 5.5.7 Passo 6: Instalar o pacote em outra máquina Mande o nome do seu usuário do GitHub e o nome do seu pacote para sua migue. Peça para ela rodar: devtools::install_github(&#39;usuario/pacote&#39;) Agora ela poderá usar sua função! library(pacote) soma_2(1:10) # [1] 3 4 5 6 7 8 9 10 11 12 Você também pode ver o help da função com ?soma_2: FIM! 5.5.8 Conclusões Agora você não tem desculpa para não empacotar suas soluções em R. Esse tutorial é incompleto! Para acessar mais detalhes, veja http://r-pkgs.had.co.nz, elaborado por você sabe quem. 5.5.9 Outras pequenas dicas práticas Use sempre devtools::check() para checar se seu pacote está 100% bem construído. Use usethis::use_package() para usar funções de outros pacotes. Sempre use os :: para chamar as funções e nunca rode library() ou require() dentro de um pacote. Use usethis::use_mit_license(&quot;seu nome&quot;) para adicionar um arquivo LICENSE ao seu pacote. Use usethis::use_data() para adicionar dados ao seu pacote. Use usethis::use_vignette() para escrever um tutorial sobre seu pacote, igual a esse do dplyr, por exemplo. É isso. Happy coding ;) "],
["5-6-pvec-o-laco-perfeito.html", "5.6 pvec: O laço perfeito", " 5.6 pvec: O laço perfeito Autor: Julio Dificuldade baixa model Quando usamos laços para rodar algoritmos complexos em uma lista de inputs, podemos pensar em power-ups. Tratam-se de funcionalidades que ajudam na aplicação dos laços, tanto do ponto de vista de eficiência do código quanto do ponto de vista de eficiência do trabalho do cientista de dados. Aqui na Curso-R nós já vimos três desses power-ups: Como fazer laços em paralelo. Como usar barras de progresso Como fazer tratamento de erros. Mas será que tem um jeito de juntar essas três funcionalidades em apenas uma operação? Sim, é claro que tem. E se algo é possível no R, o Caio Lente já fez. Trata-se da operação pvec(), do pacote abjutils. Para utilizá-la, você precisará instalar a versão de desenvolvimento do abjutils no GitHub: devtools::install_github(&quot;abjur/abjutils&quot;) Pode ser que o pvec() não funcione muito bem no Windows. Isso é algo que vamos trabalhar no futuro. 5.6.1 Como funciona O pvec() recebe duas informações de entrada: uma lista ou vetor de inputs e uma função a ser aplicada. O pvec() funciona exatamente como um purrr::map(), mas retorna um data.frame com os outputs. Por exemplo, digamos que nosso objetivo seja aplicar a função funcao &lt;- function(x) { # dorme 1s Sys.sleep(1) # aplica o log log(x) } em uma lista de entradas, dada por input &lt;- list(1, 2, -1, &quot;a&quot;) O resultado é dado por: resultado &lt;- abjutils::pvec(input, funcao) resultado # A tibble: 4 x 3 id return output &lt;int&gt; &lt;chr&gt; &lt;list&gt; 1 1 result &lt;dbl [1]&gt; 2 2 result &lt;dbl [1]&gt; 3 3 result &lt;dbl [1]&gt; 4 4 error &lt;S3: simpleError&gt; Ou seja, o resultado é um data.frame, que tem o número de linhas exatamente igual ao comprimento do vetor ou lista de entrada, e três colunas específicas. id, que guarda o índice de entrada. Se a lista de entrada é nomeada, id guarda esses nomes. return identifica se a aplicação retornou num resultado (result) ou erro (error) output é uma coluna-lista que contém os resultados. Quando o resultado é um erro, o erro é capturado e colocado no elemento correspondente. Ou seja, uma característica do pvec() é que ele nunca irá travar. Se essa operação travar, é porque o computador todo travou. É importante notar que alguns resultados nesse caso são NaN. Isso ocorre pois log(-1) resulta em NaN, acompanhado de um warning. O pvec() não trabalha com warnings. Outra característica importante do pvec() é que ele roda em paralelo. Você pode controlar a quantidade de núcleos de processamento com o parâmetro .cores. Por padrão, ele usará o número de núcleos da sua máquina. Finalmente, o que não poderia faltar no pvec() é a utilização de barras de progresso. Por exemplo, considerando como input input &lt;- list(a = 1, b = 2, c = -1, d = &quot;a&quot;, e = 2, f = 3, g = -2, h = &quot;b&quot;) O resultado é abjutils::pvec(input, funcao) Progress: ─────────────────────────────── 100% Progress: ──────────────────────────────────────────────────────────── 100% # A tibble: 8 x 3 id return output &lt;chr&gt; &lt;chr&gt; &lt;list&gt; 1 a result &lt;dbl [1]&gt; 2 b result &lt;dbl [1]&gt; 3 c result &lt;dbl [1]&gt; 4 d error &lt;S3: simpleError&gt; 5 e result &lt;dbl [1]&gt; 6 f result &lt;dbl [1]&gt; 7 g result &lt;dbl [1]&gt; 8 h error &lt;S3: simpleError&gt; Se você quiser desligar a barra de progresso, basta adicionar .progress = FALSE. 5.6.2 O parâmetro .flatten Esse é o parâmetro dos preguiçosos (eu que pedi para o Caio adicionar). Em muitas operações, o resultado que sai no output é uma lista de data.frames ou uma lista de vetores. A opção .flatten faz tidyr::unnest(), empilhando os resultados e colando tudo num vetor ou data.frame. O único problema é que nesse caso não é possível guardar os erros. Por isso, o pvec() retorna um warning: abjutils::pvec(input, funcao, .flatten = TRUE) Progress: ──────────────────────────────────────────────────────────── 100% # A tibble: 6 x 2 id output &lt;chr&gt; &lt;dbl&gt; 1 a 0 2 b 0.693 3 c NaN 4 e 0.693 5 f 1.10 6 g NaN Warning message: Since &#39;.flatten = TRUE&#39;, a total of 2 errors are being ignored Note que o resultado tem 6 linhas, menor que a entrada, que tem 8 elementos. Por isso, use .flatten somente quando você tem certeza do que está fazendo. 5.6.3 Por trás dos panos: o furrr O pvec() só funciona por conta de dois excelentes pacotes: o future, que é um novo paradigma de computação em paralelo no R. o furrr, que faz todo o trabalho sujo e implementa a maioria das operações do purrr usando future. Se quiser estudar esses pacotes e implementar suas próprias soluções, recomendo acessar aqui e aqui. Não incluí detalhes desses pacotes aqui para não sair do foco. Se quiser adicionar opções do future no pvec(), basta adicioná-las na opção .options. Por padrão, passamos furrr::future_options() nesse argumento. 5.6.3.1 Discussão: o future é o futuro do purrr? O purrr contém uma série de discussões no GitHub sobre a possibilidade de rodar funções em paralelo e com barras de progresso. Pode ser que a funcionalidade do pvec() passe a ser parte oficial no futuro. Veremos! 5.6.4 Wrap-up abjutils::pvec() é um map() que roda em paralelo, tem barras de progresso e trata erros automaticamente. Você pode brincar com as opções .cores, .progress e .flatten para controlar o comportamento do pvec(). Tome muito cuidado com o .flatten, pois ele pode não tratar os erros da forma que você imagina! Estude future e furrr se quiser estender as funcionalidades do pvec(). É isso pessoal. Happy coding ;) "],
["6-reflexoes.html", "Capítulo 6 Reflexões", " Capítulo 6 Reflexões "],
["referencias.html", "Referências", " Referências "]
]
