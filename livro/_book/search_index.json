[
["index.html", "Como faz no R Capítulo 1 Introdução ", " Como faz no R Curso-R 31 de January de 2019 Capítulo 1 Introdução "],
["1-1-objetivos.html", "1.1 Por quê ler esse livro", " 1.1 Por quê ler esse livro "],
["1-2-organizacao.html", "1.2 Organização", " 1.2 Organização "],
["2-analises.html", "Capítulo 2 Análises", " Capítulo 2 Análises "],
["3-tutoriais.html", "Capítulo 3 Tutoriais ", " Capítulo 3 Tutoriais "],
["3-1-por-que-usar-o.html", "3.1 Por que usar o %&gt;%", " 3.1 Por que usar o %&gt;% Provavelmente você já ouviu falar do operador pipe (%&gt;%). Muita gente acha que ele é uma sequência mágica de símbolos que muda completamente o visual do seu código, mas na verdade ele não passa de uma função como outra qualquer. Vamos explorar um pouco da história do pipe, como ele funciona e por que utilizá-lo. 3.1.1 Origem O conceito de pipe existe pelo menos desde os anos 1970. De acordo com seu criador, o operador foi concebido em “uma noite febril” e tinha o objetivo de simplificar comandos cujos resultados deveriam ser passados para outros comandos. ls | cat #&gt; Desktop #&gt; Documents #&gt; Downloads #&gt; Music #&gt; Pictures #&gt; Public #&gt; Templates #&gt; Videos Por essa descrição já conseguimos ter uma ideia de onde vem o seu nome: pipe em inglês significa “cano”, referindo-se ao transporte das saídas dos comandos. Em português o termo é traduzido como “canalização” ou “encadeamento”, mas no dia-a-dia é mais comum usar o termo em inglês. A partir daí o pipe tem aparecido nas mais diversas aplicações, desde HTML até o nosso tão querido R. Ele pode ter múltiplos disfarces, mas o seu objetivo é sempre o mesmo: transportar resultados. 3.1.2 Como funciona Em R o pipe tem uma cara meio estranha (%&gt;%), mas no fundo ele não passa de uma função infixa, ou seja, uma função que aparece entre os seus argumentos (como a + b ou a %in% b). Na verdade é por isso mesmo que ele tem porcentagens antes e depois: porque no R uma função infixa só pode ser declarada assim. Vamos começar demonstrando sua funcionalidade básica. Carregue o pacote magrittr e declare o pipe usando Ctrl + Shift + M. library(magrittr) `%&gt;%`(&quot;oi&quot;, print) #&gt; [1] &quot;oi&quot; Não ligue para os acentos graves em volta do pipe, o comando acima só serve para demonstrar que ele não é nada mais que uma função; perceba que o seu primeiro argumento (&quot;oi&quot;) virou a entrada do seu segundo argumento (print). &quot;oi&quot; %&gt;% print() #&gt; [1] &quot;oi&quot; Observe agora o comando abaixo. Queremos primeiro somar 3 a uma sequência de números e depois dividí-los por 2: mais_tres &lt;- function(x) { x + 3 } sobre_dois &lt;- function(x) { x / 2 } x &lt;- 1:3 sobre_dois(mais_tres(x)) #&gt; [1] 2.0 2.5 3.0 Perceba como fica difícil de entender o que está acontecendo primeiro? A linha relevante começa com a divisão por 2, depois vem a soma com 3 e, por fim, os valores de entrada. Nesse tipo de situação é mais legível usar a notação de composição de funções, com as funções sendo exibidas na ordem em que serão aplicadas: \\(f \\circ g\\). Isso pode ser realizado se tivermos uma função que passa o resultado do que está à sua esquerda para a função que está à sua direita… x %&gt;% mais_tres() %&gt;% sobre_dois() #&gt; [1] 2.0 2.5 3.0 No comando acima fica evidente que pegamos o objeto x, somamos 3 e dividimos por 2. Você pode já ter notado isso, mas a entrada (esquerda) de um pipe sempre é passada como o primeiro argumento agumento da sua saída (direita). Isso não impede que as funções utilizadas em uma sequência de pipes tenham outros argumentos. mais_n &lt;- function(x, n) { x + n } x %&gt;% mais_n(4) %&gt;% sobre_dois() #&gt; [1] 2.5 3.0 3.5 3.1.3 Vantagens A grande vantagem do pipe não é só enxergar quais funções são aplicadas primeiro, mas sim nos ajudar a programar pipelines (“encanamento” em inglês) de tratamentos de dados. library(dplyr) starwars %&gt;% mutate(bmi = mass/((height/100)^2)) %&gt;% select(name, bmi, species) %&gt;% group_by(species) %&gt;% summarise(bmi = mean(bmi)) #&gt; # A tibble: 38 x 2 #&gt; species bmi #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aleena 24.0 #&gt; 2 Besalisk 26.0 #&gt; 3 Cerean 20.9 #&gt; 4 Chagrian NA #&gt; 5 Clawdite 19.5 #&gt; 6 Droid NA #&gt; 7 Dug 31.9 #&gt; 8 Ewok 25.8 #&gt; 9 Geonosian 23.9 #&gt; 10 Gungan NA #&gt; # ... with 28 more rows Acima fica extremamente claro o que está acontecendo em cada passo da pipeline. Partindo da base starwars, primeiro transformamos, depois selecionamos, agrupamos e resumimos; em cada linha temos uma operação e elas são executadas em sequência. Isso não melhora só a legibilidade do código, mas também a sua debugabilidade! Se tivermos encontrado um bug na pipeline, basta executar linha a linha do encadeamento até que encontremos a linha problemática. Com o pipe podemos programar de forma mais compacta, legível e correta. Todos os exemplos acima envolvem passar a entrada do pipe como o primeiro argumento da função à direita, mas não é uma obrigatoriedade. Com um operador placeholder . podemos indicar exatamente onde deve ser colocado o valor que chega no pipe: y_menos_x &lt;- function(x, y) { y - x } x %&gt;% mais_tres() %&gt;% purrr::map2(4:6, ., y_menos_x) # [[1]] # [1] 0 # # [[2]] # [1] 0 # # [[3]] # [1] 0 3.1.4 Bônus Agora que você já sabe dos usos mais comuns do pipe, aqui está uma outra funcionalidade interessante: funções unárias. Se você estiver familiarizado com o pacote purrr, esse é um jeito bastante simples de criar funções descartáveis. m3_s2 &lt;- . %&gt;% mais_tres() %&gt;% sobre_dois() m3_s2(x) #&gt; [1] 2.0 2.5 3.0 Usando novamente o . definimos uma função que recebe apenas um argumento com uma sequência de aplicações de outras funções. 3.1.5 Conclusão O pipe não é apenas algo que deve ser usado pelos fãs do tidyverse. Ele é uma função extremamente útil que ajuda na legibilidade e programação de código, independentemente de quais pacotes utilizamos. Se quiser saber mais sobre o mundo do pipe, leia este post do Daniel sobre o Manifesto Tidy e o nosso tutorial mais aprofundado sobre o próprio pipe. "],
["3-2-o-que-e-um-grafico-estatistico.html", "3.2 O que é um gráfico estatístico?", " 3.2 O que é um gráfico estatístico? Os gráficos são técnicas de visualização de dados amplamente utilizadas em todas as áreas da pesquisa. A sua popularidade se deve à maneira como elucidam informações que estavam escondidas nas colunas do banco de dados, sendo que muitos deles podem ser compreendidos até mesmo por leigos no assunto que está sendo discutido. Mas será que podemos definir formalmente o que é um gráfico estatístico? Graças ao estatístico norte-americano Leland Wilkinson, a resposta é sim. Em 2005, Leland publicou o livro The Grammar of Graphics, uma fonte de princípios fundamentais para a construção de gráficos estatísticos. No livro, ele defende que um gráfico é o mapeamento dos dados a partir de atributos estéticos (posição, cor, forma, tamanho) e de objetos geométricos (pontos, linhas, barras, caixas). Simples assim. Além de responder a pergunta levantada nesse post, os conceitos de Leland tiveram outra grande importância para a visualização de dados. Alguns anos mais tarde, o seu trabalho inspirou Hadley Wickham a criar o pacote ggplot2, que enterrou com muitas pás de terra as funções gráficas do R base. Em A Layered Grammar of Graphics, Hadley sugeriu que os principais aspectos de um gráfico (dados, sistema de coordenadas, rótulos e anotações) podiam ser divididos em camadas, construídas uma a uma na elaboração do gráfico. Essa é a essência do ggplot2. No gráfico abaixo, temos informação de 32 carros com respeito a 4 variáveis: milhas por galão, tonelagem, transmissão e número de cilindros. O objeto geométrico escolhido para representar os dados foi o ponto. As posições dos pontos no eixo xy mapeia a associação entre a tonelagem e a quantidade de milhas por galão. A cor dos pontos mapeia o número de cilindros de cada carro, enquanto a forma dos pontos mapeia o tipo de transmissão. Observando o código, fica claro como cada linha/camada representa um aspecto diferente do gráfico. Os conceitos criados por Leland e Hadley defendem que essa estrutura pode ser utilizada para construir e entender qualquer tipo de gráfico, dando a eles, dessa maneira, a sua definição formal. ggplot(mtcars) + geom_point(aes(x = disp, y = mpg, shape = as.factor(am), color = cyl)) + labs(x = &quot;Tonelagem&quot;, y = &quot;Milhas por galão&quot;, shape = &quot;Transmissão&quot;, color = &quot;Cilindros&quot;) + scale_shape_discrete(labels = c(&quot;Automática&quot;,&quot;Manual&quot;)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) Por fim, é preciso frisar que, apesar de a gramática prover uma forte fundação para a construção de gráficos, ela não indica qual gráfico deve ser usado ou como ele deve parecer. Essas escolhas, fundamentadas na pergunta a ser respondida, nem sempre são triviais, e negligenciá-las pode gerar gráficos mal construídos e conclusões equivocadas. Cabe a nós, pesquisadores, desenvolver, aprimorar e divulgar as técnicas de visualização adequadas para cada tipo de variável, assim como apontar ou denunciar os usos incorretos e mal-intencionados. Mas, em um mundo cuja veracidade das notícias é cada vez menos importante, é papel de todos ter senso crítico para entender e julgar as informações trazidas por um gráfico. "],
["3-3-colando-textos-no-r.html", "3.3 Colando textos no R", " 3.3 Colando textos no R Uma tarefa muito comum no R é a de colar textos. As funções mais importantes para isso são paste() e sprintf(), que vêm com o pacote base. Nesse texto, vamos falar dessas duas funções e de um novo pacote do tidyverse, o glue. 3.3.1 paste() A função paste() recebe um conjunto indeterminado de objetos como argumento através do ... e vai colando os objetos passados elemento a elemento. Isso significa que se você passar dois vetores de tamanho n, a função paste() retornará um vetor de tamanho n sendo cada posição a colagem dos dois vetores nessa posição. Por padrão, a colagem é feita com um separador de espaço simples (o &quot; &quot;). Exemplo: paste(c(1, 2, 3), c(4, 5, 6)) FALSE [1] &quot;1 4&quot; &quot;2 5&quot; &quot;3 6&quot; É possível alterar o separador pelo argumento sep =. Um atalho útil para o separador vazio (&quot;&quot;) é a função paste0: paste0(c(1, 2, 3), c(4, 5, 6)) FALSE [1] &quot;14&quot; &quot;25&quot; &quot;36&quot; Algumas vezes nosso interesse não é juntar vetores elemento a elemento, mas sim passar um vetor e colar todos seus elementos. Isso é feito com o parâmetro collapse =: paste(c(1, 2, 3, 4, 5, 6), collapse = &#39;@&#39;) FALSE [1] &quot;1@2@3@4@5@6&quot; Se você passar mais de um vetor e mandar colapsar os elementos, o paste() vai primeiro colar e depois colapsar: paste(c(1, 2, 3), c(4, 5, 6), collapse = &#39;@&#39;) FALSE [1] &quot;1 4@2 5@3 6&quot; 3.3.1.1 Cuidado Tenha muito cuidado ao passar vetores com comprimentos diferentes no paste()! Assim como muitas funções do R, o paste() faz reciclagem, ou seja, ele copia os elementos do menor vetor até ele ficar com o comprimento do maior vetor1. O problema é que o paste() faz isso silenciosamente e não avisa se você inserir um vetor com comprimento que não é múltiplo dos demais. Veja que resultado bizarro: paste(5:9, 1:3, 4:5) FALSE [1] &quot;5 1 4&quot; &quot;6 2 5&quot; &quot;7 3 4&quot; &quot;8 1 5&quot; &quot;9 2 4&quot; Por essas e outras que dizemos que às vezes o R funciona bem demais… 3.3.2 sprintf() O sprintf() é similar ao printf do C. Primeiro escrevemos um texto com %s no lugar das coisas que queremos substituir. Depois colocamos esses objetos nos outros argumentos da função, na ordem em que eles aparecem no texto. sprintf(&#39;Aba%ste&#39;, &#39;ca&#39;) FALSE [1] &quot;Abacate&quot; Quando o argumento é um vetor, a função retorna um vetor com as substituições ponto a ponto. sprintf(&#39;Aba%ste&#39;, c(&#39;ca&#39;, &#39;ixas&#39;)) FALSE [1] &quot;Abacate&quot; &quot;Abaixaste&quot; Se o texto contém mais de um %s e os objetos correspondentes são vetores, o sprintf() tenta reciclar os vetores para ficarem do mesmo tamanho. Isso só funciona quando todos os objetos têm comprimentos que são múltiplos do comprimento do maior objeto. Por exemplo, isso funciona: sprintf(&#39;Aba%s%s&#39;, c(&#39;ca&#39;), c(&#39;xi&#39;, &#39;te&#39;)) # ca foi reciclado FALSE [1] &quot;Abacaxi&quot; &quot;Abacate&quot; Isso não funciona: sprintf(&#39;Aba%s%s&#39;, c(&#39;ca&#39;, &#39;ixaste&#39;), c(&#39;xi&#39;, &#39;te&#39;, &#39;.&#39;)) FALSE Error in sprintf(&quot;Aba%s%s&quot;, c(&quot;ca&quot;, &quot;ixaste&quot;), c(&quot;xi&quot;, &quot;te&quot;, &quot;.&quot;)): arguments cannot be recycled to the same length Nem sempre queremos substituir pedaços do nosso texto por outros textos. No lugar do %s, é possível colocar padrões para números, por exemplo. Eu uso bastante o %d, que recebe inteiros. Uma funcionalidade legal do %d é a possibilidade de adicionar zeros à esquerda quando um número não atinge certa quantidade de dígitos. Assim, quando ordenamos um vetor de textos que começa com números, a ordenação é a mesma da versão numérica do vetor. Exemplo: nums &lt;- 1:11 sort(as.character(nums)) # ordenado pela string: 10 vem antes de 2 FALSE [1] &quot;1&quot; &quot;10&quot; &quot;11&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; sort(sprintf(&#39;%02d&#39;, nums)) # ordenado pela string: 02 vem antes de 10 FALSE [1] &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;04&quot; &quot;05&quot; &quot;06&quot; &quot;07&quot; &quot;08&quot; &quot;09&quot; &quot;10&quot; &quot;11&quot; 3.3.3 glue O glue é um pacote recente. Sua primeira aparição no GitHub foi em 23/12/2016. Isso significa que é provável que algumas estejam em constante desenvolvimento, mas isso não nos impede de aproveitar o que a ferramenta tem de bom. A função glue() é uma generalização do sprintf() que permite chamar objetos do R diretamente ao invés de utilizar o %s. Os objetos podem estar no global environment ou descritos por meio de objetos nomeados nos argumentos do glue(). Basta inserir os objetos entre chaves {}: library(glue) planeta &lt;- &#39;mundo&#39; glue(&#39;Olá {planeta} pela {y}a vez&#39;, y = 1) FALSE Olá mundo pela 1a vez Tembém é possível adicionar expressões dentro das chaves: p &lt;- 1.123123123 glue(&#39;{p * 100}% das pessoas adoram R.&#39;) FALSE 112.3123123% das pessoas adoram R. glue(&#39;{scales::percent(p)} das pessoas adoram R.&#39;) FALSE 112% das pessoas adoram R. A função glue_collapse() é parecida com o paste() quando collapse = '', mas só aceita um objeto como entrada: x &lt;- glue_collapse(1:10) x FALSE 12345678910 x == paste(1:10, collapse = &#39;&#39;) FALSE [1] TRUE Se quiser colar os objetos elemento a elemento e depois colapsar, faça isso explicitamente em duas operações: library(magrittr) glue(&#39;{letters}/{LETTERS}&#39;) %&gt;% glue_collapse(&#39;, &#39;) FALSE a/A, b/B, c/C, d/D, e/E, f/F, g/G, h/H, i/I, j/J, k/K, l/L, m/M, n/N, o/O, p/P, q/Q, r/R, s/S, t/T, u/U, v/V, w/W, x/X, y/Y, z/Z O glue também tem uma função extra para trabalhar melhor com o %&gt;%, o glue_data(). O primeiro argumento dessa função é uma lista ou data.frame, e seus nomes são utilizados como variáveis para alimentar as chaves das strings. Use o . para fazer operações com toda a base de dados: mtcars %&gt;% head() %&gt;% glue_data(&#39;O carro {row.names(.)} rende {mpg} milhas por galão.&#39;) FALSE O carro Mazda RX4 rende 21 milhas por galão. FALSE O carro Mazda RX4 Wag rende 21 milhas por galão. FALSE O carro Datsun 710 rende 22.8 milhas por galão. FALSE O carro Hornet 4 Drive rende 21.4 milhas por galão. FALSE O carro Hornet Sportabout rende 18.7 milhas por galão. FALSE O carro Valiant rende 18.1 milhas por galão. 3.3.4 Resumo Use paste() para colar ou colapsar elementos usando um separador fixado. Use sprintf() quando quiser colocar objetos dentro de um texto complexo. Em todos os casos existe uma solução usando glue. Atualmente sempre que tenho um problema desse tipo uso o glue. Até o momento, não encontrei nenhum problema ou dificuldade. A vida do cientista de dados é mais feliz com o tidyverse! 3.3.5 Extra: O Guilherme Jardim Duarte fez uma ótima sugestão sobre este artigo. No pacote stringi existe um operador %s+% que cola textos iterativamente, com uma sintaxe similar à linguagem python, e que permite a colagem de textos usando um simples +. Exemplo: library(stringi) &#39;a&#39; %s+% &#39;ba&#39; %s+% &#39;ca&#39; %s+% &#39;xi&#39; FALSE [1] &quot;abacaxi&quot; Você pode adicionar esse operador como um atalho no RStudio, análogo ao Ctrl+Shift+M que usamos para escrever o %&gt;%. Para isso, veja esse tutorial sobre RStudio Addins. Mais sobre isso no livro R inferno↩ "],
["4-modelagem.html", "Capítulo 4 Modelagem ", " Capítulo 4 Modelagem "],
["4-1-monty-hall-e-diagramas-de-influencia.html", "4.1 Monty hall e diagramas de influência", " 4.1 Monty hall e diagramas de influência Você está num jogo na TV e o apresentador pede para escolher uma entre 3 portas. Atrás de uma dessas portas tem uma Ferrari e nas outras duas temos cabras. Você escolhe uma porta. Depois, o apresentador retira uma porta que tem uma cabra e pergunta: você quer trocar de porta? A princípio, você pode achar que sua probabilidade de ganhar é 1/2, já que uma das portas foi retirada, então não importa se você troca ou não. Mas a resposta é que sim, vale à pena trocar de porta! A probabilidade de vencer o jogo trocando a porta é de 2/3. Figura 2.1: Brincadeira do XKCD. O problema de Monty Hall é talvez o mais eloquente exemplo de como a probabilidade pode confundir a mente humana. Esse problema desafiou a comunidade científica no final do século XX e chegou até a ser considerado um paradoxo. Recomendo ler o livro O Andar do Bêbado, de Leonard Mlodinow, que conta essa e muitas outras histórias interessantes sobre a probabilidade. Existem várias formas de explicar por quê trocar a porta é a melhor estratégia. A que eu mais gosto é a do próprio Andar do Bêbado, que mostra que, quando você escolhe a primeira porta, você está apostando se acertou ou não a Ferrari. Se você apostar que acertou a Ferrari, não deve trocar a porta e, se você apostar que errou a Ferrari, deve trocar. A aposta de errar a Ferrari de primeira tem probabilidade 2/3, logo, vale à pena trocar. Nesse post, mostramos uma solução alternativa, simples e elegante para o problema usando diagramas de influência e o pacote bnlearn. 4.1.1 Redes bayesianas As redes Bayesianas são o resultado da combinação de conceitos probabilísticos e conceitos da teoria dos grafos. Segundo Pearl, tal união tem como consequências três benefícios: i) prover formas convenientes para expressar suposições do modelo; ii) facilitar a representação de funções de probabilidade conjuntas; e iii) facilitar o cálculo eficiente de inferências a partir de observações. Da teoria de probabilidades precisamos apenas de alguns resultados básicos sobre probabilidade condicional. Primeiramente, pela definição de probabilidade condicional, sabemos que \\[ p(x_1, x_2) = p(x_1)p(x_2|x_1). \\] Aplicando essa regra iterativamente para \\(n\\) variáveis, temos \\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|x_1,\\dots, x_{j-1}). \\] Agora, imagine que, no seu problema, a variável aleatória \\(X_j\\) não dependa probabilisticamente de todas as variáveis \\(X_1,\\dots, X_{j-1}\\), e sim apenas de um subconjunto \\(\\Pi_j\\) dessas variáveis. Fazendo isso, a equação pode ser escrita como \\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|\\pi_j). \\] Chamamos \\(\\Pi_j\\) de pais de \\(X_j\\). Esse conjunto pode ser pensado como as variáveis que são suficientes para determinar as probabilidades de \\(X_j\\). A parte mais legal das redes Bayesianas é que elas podem ser representadas a partir de DAGs (grafos direcionados acíclicos). No grafo, se \\(X_1\\) aponta para \\(X_2\\), então \\(X_1\\) é pai de \\(X_2\\). Por exemplo, esse grafo aqui representa a distribuição de probabilidades \\(p(x_1, \\dots, x_5)\\) com \\[ p(x_1, \\dots, x_5) = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_3,x_2)p(x_5|x_4). \\] 4.1.2 Diagrama de influências Um diagrama e influências é uma rede Bayesiana com nós de decisão e utilidade (ganhos). Ou seja, é uma junção de três conceitos: \\[ \\underbrace{\\text{prob. condicional} + \\text{grafos}}_{\\text{rede Bayesiana}} + \\text{teoria da decisão} = \\text{diagrama de influências} \\] Na teoria da decisão, usualmente estamos interessados em maximizar a utilidade esperada. No diagrama, considerando a estrutura de probabilidades dada pela rede Bayesiana e as informações disponíveis, queremos escolher a decisão que faz com que, em média, nosso retorno seja mais alto. Com diagramas de influências, é possível organizar sistemas complexos com múltiplas decisões, considerando diferentes conjuntos de informações disponíveis. É uma ferramenta realmente muito poderosa. 4.1.3 Voltando ao Monty Hall Agora que sabemos um pouquinho de diagramas de influência, podemos desenhar o do Monty Hall: O jogador tem duas decisões a tomar: \\(D_1\\) (escolha_inicial): A escolha da porta inicial (1, 2, 3). \\(D_2\\) (trocar): Trocar a porta ou não (s, n). Também temos duas fontes de incerteza: \\(X_1\\) (ferrari): Em qual porta está a Ferrari (1, 2, 3). \\(X_2\\) (porta_retirada): Qual porta foi retirada (1, 2, 3). Essa variável não é sempre aleatória: se eu escolho a porta 1 e a Ferrari está em 2, o apresentador é obrigado a retirar a porta 3. Se o apresentador tiver a opção de escolher (que acontece no caso da escolha inicial ser a Ferrari), o apresentador escolhe uma porta para retirar aleatoriamente. Finalmente, temos um nó de utilidade: \\(U_1\\) (result): Ganhei a Ferrari (ganhei, perdi). Em R, podemos construir a rede Bayesiana do problema utilizando o pacote bnlearn: FALSE [,1] [,2] FALSE [1,] &quot;escolha_inicial&quot; &quot;porta_retirada&quot; FALSE [2,] &quot;ferrari&quot; &quot;porta_retirada&quot; FALSE [3,] &quot;porta_retirada&quot; &quot;trocar&quot; FALSE [4,] &quot;trocar&quot; &quot;result&quot; FALSE [5,] &quot;ferrari&quot; &quot;result&quot; FALSE [6,] &quot;escolha_inicial&quot; &quot;result&quot; O output desse conjunto de operações é um objeto do tipo bn com várias propriedades pré calculadas pelo pacote bnlearn: Random/Generated Bayesian network model: [escolha_inicial][ferrari][porta_retirada|escolha_inicial:ferrari][trocar|porta_retirada] [result|escolha_inicial:ferrari:trocar] nodes: 5 arcs: 6 undirected arcs: 0 directed arcs: 6 average markov blanket size: 3.60 average neighbourhood size: 2.40 average branching factor: 1.20 generation algorithm: Empty Com as especificação do problema dada, se gerarmos aleatoriamente todos os cenários, chegamos à essa combinação de casos equiprováveis (ver Extra 2) Agora, vamos escrever todas as combinações possíveis de cenários e guardar num data.frame chamado dados: escolha_inicial ferrari porta_retirada trocar result 1 1 2 n ganhei 1 1 2 s perdi 1 1 3 n ganhei 1 1 3 s perdi 1 2 3 n perdi 1 2 3 s ganhei 1 3 2 n perdi 1 3 2 s ganhei 2 1 3 n perdi 2 1 3 s ganhei 2 2 1 n ganhei 2 2 1 s perdi 2 2 3 n ganhei 2 2 3 s perdi 2 3 1 n perdi 2 3 1 s ganhei 3 1 2 n perdi 3 1 2 s ganhei 3 2 1 n perdi 3 2 1 s ganhei 3 3 2 n ganhei 3 3 2 s perdi 3 3 1 n ganhei 3 3 1 s perdi Finalmente, ajustamos nossa rede Bayesiana, usando a função bnlearn::bn.fit(). A função bnlearn::cpquery() (conditional probability query) serve para realizar uma consulta de probabilidades dada a rede ajustada. No nosso caso, a partir de uma escolha inicial qualquer \\(d_1\\), queremos saber o ganho ao trocar é maior que o ganho ao não trocar. \\[ \\mathbb E(U_1\\; |\\; D_2 = \\text{s}, D_1 = d_1) &gt; \\mathbb E(U_1\\; |\\; D_2 = \\text{n}, D_1 = d_1). \\] Fazendo contas, isso equivale matematicamente a consultar se \\[ \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s}) &gt; \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{n}) \\] Agora, podemos consultar \\(\\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s})\\) com nosso modelo! [1] 0.6666704 E não é que dá 2/3 mesmo? Da mesma forma, temos [1] 0.3333187 Resolvido! 4.1.4 Wrap-up Vale à pena trocar a porta! Redes Bayesianas juntam grafos e probabilidades condicionais Diagramas de influência juntam redes Bayesianas e teoria da decisão Essas ferramentas podem ser utilizadas tanto para resolver Monty Hall quanto para ajudar em sistemas complexos. É isso pessoal. Happy coding ;) 4.1.5 Extra Se você ficou interessada em como eu fiz o diagrama, utilizei o pacote DiagrammeR. O código está aqui: 4.1.6 Extra 2 É possível simular os dados que coloquei no post com uma função simples, que adicionei abaixo. Na verdade, o fato de eu ter considerado somente as combinações únicas de cenários e não os dados simulados abaixo é um pouco roubado, e só funciona porque os cenários calham de ser, de fato, equiprováveis. Observations: 10,000 Variables: 5 $ escolha_inicial &lt;fct&gt; 3, 1, 2, 1, 1, 1, 3, 1, 2, 3, 3, 1, 3, 1, 2, 2, 2,... $ ferrari &lt;fct&gt; 1, 1, 2, 1, 1, 2, 3, 3, 1, 2, 3, 3, 2, 1, 1, 3, 1,... $ porta_retirada &lt;fct&gt; 2, 3, 1, 3, 2, 3, 2, 2, 3, 1, 1, 2, 1, 2, 3, 1, 3,... $ trocar &lt;fct&gt; n, s, s, n, s, n, n, n, n, s, s, s, s, n, n, s, n,... $ result &lt;fct&gt; perdi, perdi, perdi, ganhei, perdi, perdi, ganhei,... Os dados do post podem ser obtidos fazendo isso aqui: Agradecimentos: Rafael Stern, que me convenceu de que vale à pena mostrar os dados simulados 😉 "],
["4-2-construindo-autoencoders.html", "4.2 Construindo Autoencoders", " 4.2 Construindo Autoencoders Autoencoders são redes neurais treinadas com o objetivo de copiar o seu input para o seu output. Esse interesse pode parecer meio estranho, mas na prática o objetivo é aprender representações (encodings) dos dados, que podem ser usadas para redução de dimensionalidade ou até mesmo compressão de arquivos. Basicamente, um autoencoder é dividido em duas partes: um encoder que é uma função \\(f(x)\\) que transforma o input para uma representação \\(h\\) um decoder que é uma função \\(g(x)\\) que transforma a representação \\(h\\) em sua reconstrução \\(r\\) Imagem do blog do Keras 4.2.1 Construindo o seu primeiro autoencoder Nesse pequeno tutorial, vou usar o keras para definir e treinar os nossos autoencoders. Como base de dados vou usar algumas simulações e o banco de dados mnist (famoso para todos que já mexeram um pouco com deep learning). O mnist é um banco de dados de imagens de tamanho 28x28 de dígitos escritos à mão. Esse dataset promoveu grandes avanços na área de reconhecimento de imagens. Com esse código definimos um modelo da seguinte forma: \\[ X = (X*W_1 + b_1)*W_2 + b_2 \\] Em que: \\(X\\) é o nosso input com dimensão (?, 784) \\(W_1\\) é uma matriz de pesos com dimensões (784, 32) \\(b_1\\) é uma matriz de forma (?, 32) \\(W_2\\) é uma matriz de pesos com dimensões (32, 784) \\(b_2\\) é uma matriz de forma (?, 784) Note que ? aqui é o número de observaçãoes da base de dados. Agora vamos estimar \\(W_1\\), \\(W_2\\), \\(b_1\\) e \\(b_2\\) de modo a minimizar alguma função de perda. Inicialmente vamos usar a binary crossentropy por pixel que é definida por: \\[-\\sum_{i=1}y_i*log(\\hat{y}_i)\\] Isso é definido no keras usando: Não vou entrar em detalhes do que é o adadelta, mas é uma variação do método de otimização conhecido como gradient descent. Agora vamos carregar a base de dados e em seguida treinar o nosso autoencoder`. Estimamos os parâmetros desse modelo no keras fazendo: Depois de rodar todas as iterações, você poderá usar o seu encoder e o seu decoder para entender o que eles fazem com as imagens. Veja o exemplo a seguir em que vamos obter os encodings para as 10 primeiras imagens da base de teste e depois reconstruir a imagem usando o decoder. FALSE [1] 10 32 FALSE [1] 0.0000000 10.1513205 3.5742311 2.6635208 6.3097358 3.4840517 FALSE [7] 9.1041250 6.6329145 1.6385922 9.8017225 9.5529270 1.6670935 FALSE [13] 5.7208562 4.8035479 3.9149191 0.6408147 1.2716029 3.1215091 FALSE [19] 13.7575903 0.0000000 1.8692881 3.2142215 0.7444992 5.0728440 FALSE [25] 8.2932110 9.9866810 2.7651572 11.1291723 5.2460670 5.6875997 FALSE [31] 10.6097431 3.6338394 O encoder transforma a matriz de (10, 784) para uma matriz com dimensao (10, 2). Podemos reconstruir a imagem, a pardir da imagem que foi comprimida usando o nosso decoder. Compare as reconstruções com as imagens originais abaixo: Um ponto interessante é que esse modelo faz uma aproximação da solução por componentes principais! Na verdade, a definição do quanto são parecidos é quase-equivalente. Isso quer dizer que os pesos \\(W\\) encontrados pelo PCA e pelo autoencoder serão diferentes, mas o sub-espaço criado pelos mesmos será equivalente. Se são equivalentes, qual a vantagem de usar autoencoders ao invés de PCA? O PCA para por aqui, você define que serão apenas relações lineares, e você reduz dimensão apenas reduzindo o tamanho da matriz. Em autoencoders você tem diversas outras saídas para aprimorar o método. A primeira delas é simplesmente adicionar uma condição de esparsidade nos pesos. Isso vai reduzir o tamanho do vetor latente (como é chamada a camada do meio do autoencoder) também, pois ele terá mais zeros. Isso pode ser feito rapidamente com o keras. Basta adicionar um activity_regularizer em nossa camada de encoding. Isso vai adicionar na função de perda um termo que toma conta do valor dos outputs da camada intermediária. Outra forma de melhorar o seu autoencoder é permitir que o encoder e o decoder sejam redes neurais profundas. Com isso, ao invés de tentar encontrar transformações lineares, você permitirá que o autoencoder encontre transformações não lineares. Mais uma vez fazemos isso com o keras: Existem formas ainda mais inteligentes de construir esses autoencoders, mas o post iria ficar muito longo e não ia sobrar asssunto para o próximo. Se você quiser saber mais, recomendo fortemente a leitura deste artigo do blog do Keras e desse capítulo. Uma família bem moderna de autoencoders são os VAE (Variational Autoencoders). Esses autoencoders aprendem modelos de variáveis latentes. Isso é interessante porque permite que você gere novos dados, parecidos com os que você usou para treinar o seu autoencoder. Você pode encontrar uma implementação desse modelo aqui. "],
["4-3-modelos-beseados-em-arvores-e-a-multicolinearidade.html", "4.3 Modelos beseados em árvores e a multicolinearidade", " 4.3 Modelos beseados em árvores e a multicolinearidade Modelos baseados em árvores como árvores de decisão, random forest, ligthGBM e xgboost são conhecidos, dentre outras qualidades, pela sua robustês diante do problema de multicolinearidade. É sabido que seu poder preditivo não se abala na presença de variáveis extremamente correlacionadas. Porém, quem nunca usou um Random Forest pra fazer seleção de variáveis? Pegar, por exemplo, as top 10 mais importantes e descartar o resto? Ou até mesmo arriscou uma interpretação e concluiu sobre a ordem das variáveis mais importantes? Abaixo mostraremos o porquê não devemos ignorar a questão da multicolinearidade completamente! 4.3.1 Um modelo bonitinho Primeiro vamos ajustar um modelo bonitinho, livre de multicolinearidade. Suponha que queiramos prever Petal.Length utilizando as medidas das sépalas (Sepal.Width e Sepal.Length) da nossa boa e velha base iris. O gráfico acima mostra que as variáveis explicativas não são fortemente correlacionadas. Ajustando uma random fores, temos a seguinte ordem de importância das variáveis: Sem surpresas. Agora vamos para o problema! 4.3.2 Um modelo com feinho Vamos forjar uma situação extrema em que muitas variáveis sejam multicolineares. Vou fazer isso repetindo a coluna Sepal.Length várias vezes. Agora a coisa tá feia! Temos 20 variáveis perfeitamente colineares. Mesmo assim um random forest nessa nova base não perderia poder preditivo. Mas como ficou a importância das variáveis? Aqui o jogo já se inverteu: concluiríamos que Sepal.Width é mais importante de todas as variáveis! 4.3.3 Seleção de variáveis furado O gráfico abaixo mostra que quanto mais variáveis correlacionadas tivermos, menor a importância de TODAS ELAS SIMULTANEAMENTE! É como se as variáveis colineares repartissem a importância entre elas. Na prática, se estabelecessemos um corte no valor de importância pra descartar variáveis (como ilustrado pela linha vermelha), teríamos um problema em potencial: poderíamos estar jogando fora informação muito importante. 4.3.4 Como tratar multicolinearidade, então? Algumas maneiras de lidar com multicolinearidade são: Observar a matriz de correlação VIF Recursive feature elimination 4.3.5 Conclusão Cuidado ao jogar tudo no caldeirão! Devemos sempre nos preocupar com multicolinearidade, mesmo ajustando modelos baseados em árvores. "],
["4-4-woe-em-r-com-tidywoe.html", "4.4 WoE em R com tidywoe", " 4.4 WoE em R com tidywoe WoE (weight of evidence) é uma ferramenta bastante usada em aplicações de regressão logística, principalmente na área de score de crédito. Simploriamente falando, ele transforma categorias em números que refletem a diferença entre elas pelo critério de separação do Y = 1 e Y = 0. Se você ainda não sabe o que é ou quer ler mais sobre o assunto, um texto que eu gostei de ler: Data Exploration with Weight of Evidence and Information Value in R O autor desse texto é o Kim Larsen, criador do pacote Information que é completo e cheio de ferramentas sofisticadas em torno do WoE. Porém, no dia a dia do meu trabalho volta e meia eu tinha que construir rotinas próprias para fazer as versões em WoE das minhas variáveis, mesmo com vários pacotes completos disponíveis. A principal motivação era que eles não eram muito práticos e não se encaixavam na filosofia do tidyverse. Daí acabei juntando essas rotinas num pacote chamado tidywoe e deixando no ar. A ideia é que ela faça o analista ganhar em tempo, legibilidade e reprodutibilidade. Abaixo segue como usar. 4.4.1 Instalação e dados Para instalar, basta rodar abaixo. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;athospd/tidywoe&quot;) library(tidyverse) library(tidywoe) # install.packages(&quot;FactoMineR&quot;) data(tea, package = &quot;FactoMineR&quot;) tea_mini &lt;- tea %&gt;% dplyr::select(breakfast, how, where, price) 4.4.2 Como usar Tem duas funções que importam: - add_woe() - adiciona os woe’s num data frame. - woe_dictionary() - cria dicionário que mapeia as categorias com os woe’s. 4.4.3 add_woe() A função add_woe() serve para adicionar as versões WoE’s das variáveis em sua amostra de dados. tea_mini %&gt;% add_woe(breakfast) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 -0.2564295 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Você pode selecionar as variáveis que vc quiser selecionando-as como se fosse no dplyr::select(). tea_mini %&gt;% add_woe(breakfast, where:price) 4.4.4 woe_dictionary() A função woe_dictionary() é uma das duas partes necessárias para fazer o add_woe() funcionar (a outra parte são os dados). Ele constrói o dicionário de categorias e seus respectivos woe’s. tea_mini %&gt;% woe_dictionary(breakfast) variable explanatory n_tot n_breakfast n_Not.breakfast p_breakfast p_Not.breakfast woe how tea bag 170 80 90 0.5555556 0.5769231 -0.0377403 how tea bag+unpackaged 94 50 44 0.3472222 0.2820513 0.2078761 how unpackaged 36 14 22 0.0972222 0.1410256 -0.3719424 where chain store 192 90 102 0.6250000 0.6538462 -0.0451204 4.4.5 Usando um dicionário customizado Muitas vezes há o interesse em ajustar na mão alguns valores de woe para consertar a ordem dos efeitos de uma dada variável ordinal. Esse é o motivo de o add_woe() poder receber um dicionário passado pelo usuário. Isso se faz por meio do argumento .woe_dictionary. A maneira mais fácil de se fazer isso é montar um dicionário inicial com o woe_dictionary() e depois alterar os valores nele para alcançar os ajustes desejados. Exemplo: # Construa um dicionário inicial tea_mini_woe_dic &lt;- tea_mini %&gt;% woe_dictionary(breakfast) # Mexa um pouquinho nos woes tea_mini_woe_dic_arrumado &lt;- tea_mini_woe_dic %&gt;% mutate(woe = if_else(explanatory == &quot;p_unknown&quot;, 0, woe)) # Passe esse dicionário para o add_woe() tea_mini %&gt;% add_woe(breakfast, .woe_dictionary = tea_mini_woe_dic_arrumado) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 0.0000000 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 4.4.6 Exemplo de exploração O woe_dictionary() devolve uma tabela arrumada, bem conveniente para explorar mais. Por exemplo, a tabela está pronta para o ggplot. Aqui está o github do pacote para contribuições. Pretendo colocar bastante coisa nova no pacote ainda. "],
["5-regressao-logistica-aspectos-computacionais.html", "Capítulo 5 Regressão logística: aspectos computacionais", " Capítulo 5 Regressão logística: aspectos computacionais Neste texto vamos discutir um pouco sobre regressão logística, tensorflow e Modelos Lineares Generalizados (Generalized Linear Models, GLMs). Não vou economizar nas matemáticas nem nos códigos. Se você não conhece GLMs, recomendo dar uma lida, pelo menos na introdução, do livro do professor Gilberto A. Paula. Se você não conhece o Tensorflow, recomendo ver a página do RStudio sobre Tensorflow. Se você curte a parte computacional da estatística, esse livro do LEG-UFPR é obrigatório. Eles são os melhores. 5.0.1 Introdução: o tensorglm Um de meus interesses no momento é implementar GLMs usando Tensorflow. O Tensorflow é uma biblioteca computacional mantida pela Google que utiliza paralelização e o poder das GPUs (Graphical Processing Units) para fazer contas. O Tensorflow foi especialmente desenhado para facilitar o ajuste de redes neurais profundas e outros modelos sofisticados. GLMs são casos particulares de redes neurais. Uma rede neural com apenas uma camada e com funções de perda / verossimilhanças baseadas na Divergência de Kullback-Leibler são exatamente iguais aos GLMs. Por exemplo, essa divergência equivale ao erro quadrático médio para a distribuição gaussiana e binary-crossentropy para logística. Por isso, não é de se surpreender que já existam soluções prontas para modelos específicos, como regressão linear normal, logística, e até Poisson. No entanto, essas soluções têm duas limitações: Não são extensivas. Por exemplo, não achei códigos para as distribuições normal inversa, gama e binomial negativa. As soluções atuais utilizam o algoritmo descida de gradiente para otimização, que é muito legal, mas não se aproveita de alguns resultados que temos na área de GLMs, como o IWLS (Iterated Weighted Least Squares), que é uma derivação do algoritmo Fisher-scoring, que reduz o problema do ajuste ao cálculo iterado de inversas e multiplicações de matrizes. Meu intuito é, então, montar uma solução alternativa que funcione igual à função glm() do R, mas usando Tensorflow no backend ao invés do algoritmo atual, que é em Fortran. Com isso, espero que o ajuste seja mais eficiente quando os dados são grandes e permita trabalhar com dados que não cabem na memória. 5.0.2 A regressão logística Meu primeiro experimento com o tensorglm foi implementar a regressão logística usando tensorflow, com descida de gradiente. Considere o problema \\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta x),\\] em que \\(Y\\) é nossa variável resposta, \\(x\\) é nossa variável explicativa, \\(\\alpha\\) e \\(\\beta\\) são os parâmetros que queremos estimar e \\(\\sigma(\\cdot)\\) é a função sigmoide, cuja inversa é a função de ligação logística. \\[\\sigma(\\eta) = \\frac{1}{1 + e^{-\\eta}}\\] Considerando que temos observações \\(Y_1, \\dots, Y_n\\) condicionalmente independentes, já temos o suficiente para especificar nosso modelo de regressão logística. O próximo passo é definir, com base nisso, a função que queremos otimizar. A partir de uma amostra \\(y_1, \\dots, y_n\\) e observando que \\(\\mu_i = \\sigma(\\alpha + \\beta x_i)\\), a verossimilhança do modelo é dada por \\[ \\mathcal L((\\alpha, \\beta)|\\mathbf y) = \\prod_{i=1}^n f(y_i|(\\alpha, \\beta), x_i) = \\prod_{i=1}^n\\mu_i^{y_i}(1-\\mu_i)^{1-y_i} \\] O logaritmo da verossimilhança é dado por \\[ \\begin{aligned} l((\\alpha, \\beta)|\\mathbf y) &amp;= \\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\\\ &amp;= \\sum_{i=1}^n y_i\\log(\\sigma(\\alpha + \\beta x_i)) + (1-y_i)\\log(1 - \\sigma(\\alpha + \\beta x_i)) \\end{aligned} \\] Nosso objetivo é maximizar \\(l\\) com relação à \\(\\alpha\\) e \\(\\beta\\). Detalhe: essa soma, se multiplicada por -1, também é chamada de função de perda binary cross-entropy. Por isso que tanto faz você definir GLMs a partir de \\(P(Y|x)\\) ou a partir da função de perda! OK, problema dado! vamos implementar usando tensorflow! Feito! Agora podemos usar a magia do tensorflow, que é esperto o suficiente para otimizar essa perda sem a gente se preocupar em calcular derivadas na mão. Para quem não conhece o algoritmo de descida de gradiente, ele funciona assim: \\[ (\\alpha, \\beta)_{\\text{novo}} = (\\alpha, \\beta)_{\\text{velho}} + k \\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}}), \\] onde \\(\\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}})\\) é o gradiente da verossimilhança em relação ao vetor \\((\\alpha, \\beta)\\), ou seja, são as derivadas parciais de \\(l\\) em relação à \\(\\alpha\\) e \\(\\beta\\). Isso dá a direção e intensidade em que os valores devem ser atualizados. \\(k\\) é chamado de learning rate, é um fator usado para controlar o tamanho do passo dado pelo gradiente. Esse valor normalmente é definido à mão. No caso dos GLMs, \\(k\\) é substituído pelo inverso da segunda derivada da \\(l\\) em relação aos parâmetros, gerando assim os algoritmos de Newton-Raphson e Fisher-scoring. Detalhe: se você procurar esse algoritmo na internet, você vai encontrar um \\(-\\) e não um \\(+\\). Isso acontece porque estamos usando a verossimilhança e não a perda. FALSE Iter: 01, alpha=2.32, beta=3.593 FALSE Iter: 02, alpha=1.56, beta=3.409 FALSE Iter: 03, alpha=1.411, beta=2.989 FALSE Iter: 04, alpha=1.261, beta=2.665 FALSE Iter: 05, alpha=1.153, beta=2.422 FALSE Iter: 06, alpha=1.078, beta=2.257 FALSE Iter: 07, alpha=1.033, beta=2.154 FALSE Iter: 08, alpha=1.006, beta=2.095 FALSE Iter: 09, alpha=0.992, beta=2.062 FALSE Iter: 10, alpha=0.984, beta=2.045 Iter: 01, alpha=2.32, beta=3.593 Iter: 02, alpha=1.56, beta=3.409 Iter: 03, alpha=1.411, beta=2.989 Iter: 04, alpha=1.261, beta=2.665 Iter: 05, alpha=1.153, beta=2.422 Iter: 06, alpha=1.078, beta=2.257 Iter: 07, alpha=1.033, beta=2.154 Iter: 08, alpha=1.006, beta=2.095 Iter: 09, alpha=0.992, beta=2.062 Iter: 10, alpha=0.984, beta=2.045 Parece que funcionou! Agora sabemos ajustar uma regressão logística na mão, com o algoritmo de descida de gradiente… ou será que não? 5.0.3 O Problema Vamos considerar o mesmo problema, mas agora com duas explicativas. temos \\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta_1 x_2+ \\beta_2 x_2),\\] As contas são exatamente as mesmas e vou omitir, mostrando apenas o código novo. FALSE Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 FALSE Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN FALSE Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Oops! Explodiu! Por que será??? Uma forma de corrigir esse problema é considerando uma taxa de aprendizado k um pouco menor. Com os mesmos dados e modelo acima, ao fazer e rodar novamente, já conseguimos chegar nos resultados abaixo. Iter: 01, alpha=1.525, beta1=2.492, beta2=3.205 Iter: 02, alpha=1.183, beta1=2.32, beta2=3.36 Iter: 03, alpha=1.122, beta1=2.248, beta2=3.34 Iter: 04, alpha=1.101, beta1=2.208, beta2=3.296 Iter: 05, alpha=1.085, beta1=2.178, beta2=3.254 Iter: 06, alpha=1.073, beta1=2.152, beta2=3.216 Iter: 07, alpha=1.062, beta1=2.13, beta2=3.183 Iter: 08, alpha=1.053, beta1=2.112, beta2=3.154 Iter: 09, alpha=1.044, beta1=2.095, beta2=3.13 Iter: 10, alpha=1.037, beta1=2.082, beta2=3.109 Mais algumas iterações e o modelo converge. Mas nós não queremos ficar fazendo um ajuste tão fino no valor de k, certo? Afinal, queremos resolver problemas do mundo real, não ficar escolhendo valores de k… Outra forma de resolver isso é evitando problemas numéricos nas contas. O cálculo da função de perda, por exemplo, pode ser melhorado. Mas como? Bom, problemas numéricos não são minha especialidade, então agora é hora de seguir os mestres. Vamos olhar como o R e como o Tensorflow implementam as funções de perda para regressão logística. 5.0.3.1 Os objetos de classe family no R No R, os GLMs buscam informações de objetos da classe family() para realizar os ajustes. No caso da logística, o objeto é retornado por uma função chamada binomial(). O resultado disso é uma lista com vários métodos implementados. Por exemplo, a variância da binomial é dada por: FALSE function (mu) FALSE mu * (1 - mu) FALSE &lt;bytecode: 0x7fa83fe769e8&gt; FALSE &lt;environment: 0x7fa83fe7dbd8&gt; function (mu) mu * (1 - mu) &lt;bytecode: 0x55fc8e220a18&gt; &lt;environment: 0x55fca4eb5040&gt; A função de perda é dada pelo método fam$dev.resids() (resíduos deviance), e o código fonte é: FALSE function (y, mu, wt) FALSE .Call(C_binomial_dev_resids, y, mu, wt) FALSE &lt;bytecode: 0x7fa83fe76278&gt; FALSE &lt;environment: 0x7fa83fe7dbd8&gt; function (y, mu, wt) .Call(C_binomial_dev_resids, y, mu, wt) &lt;bytecode: 0x55fc8e2253a0&gt; &lt;environment: 0x55fca4eb5040&gt; Hmm, parece que é uma função feita em C. Como as contas da nossa perda (soma, logaritmo, multiplicação e divisão) já são todas implementadas em C, provavelmente a conta foi implementada em C para garantir estabilidade numérica. Olhando o código-fonte do pacote stats, encontramos a definição da função. A função é um pouco longa, então eu mantive apenas as partes importantes: static R_INLINE double y_log_y(double y, double mu) { return (y != 0.) ? (y * log(y/mu)) : 0; } SEXP binomial_dev_resids(SEXP y, SEXP mu, SEXP wt) { /* inicialização de variáveis e verificações */ /* rmu e ry são os valores de mu e y transformados para reais */ /* rmu e ry são os valores de mu e y transformados para reais */ for (i = 0; i &lt; n; i++) { mui = rmu[i]; yi = ry[i]; rans[i] = 2 * rwt[lwt &gt; 1 ? i : 0] * (y_log_y(yi, mui) + y_log_y(1 - yi, 1 - mui)); } /* outros códigos não muito importantes */ UNPROTECT(nprot); return ans; } Eu não programo muito em C, mas desse código dá para ver duas coisas importantes: i) a função y_log_y só faz a conta se o valor de \\(y\\) for diferente de zero, se não, ela já retorna zero; ii) a função y_log_y faz a conta \\(y\\log({y}/{\\mu})\\), ao invés de apenas \\(y\\log({\\mu})\\). Isso acontece pois no R estamos minimizando o Desvio do modelo, dado por \\[ \\begin{aligned} &amp;D(\\mathbf y, \\mu) = 2[l(\\mathbf y|\\mathbf y) - l(\\mathbf y|(\\alpha, \\beta))]\\\\ &amp;=2\\left[\\sum_{i=1}^n y_i\\log(y_i) + (1-y_i)\\log(1-y_i)\\right. - \\\\ &amp;\\left. -\\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\right] \\\\ &amp;=2\\left[\\sum_{i=1}^n y_i\\log\\left(\\frac{y_i}{\\mu_i}\\right) + (1-y_i)\\log\\left(\\frac{1-y_i}{1-\\mu_i}\\right)\\right]. \\end{aligned} \\] Essa é a formulação usual na literatura de GLMs, que apresenta uma série de propriedades estatísticas. Minimizar o desvio equivale a maximizar a verossimilhança. Será que isso ajuda nos problemas numéricos? Vamos ver: Iter: 01, alpha=NaN, beta1=NaN, beta2=NaN Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Hmm, parece que não. Se olharmos mais atentamente para a função desvio, como \\(y\\) pode assumir apenas os valores zero ou um, é possível observar que a conta é equivalente à perda calculada anteriormente. Possivelmente o problema aqui é que o tensorflow não trabalha muito bem com essas condições (tf$where) na perda, e isso dá problemas na hora de calcular o gradiente. Essa função do R simplesmente não resolve o problema inicial. Melhor olhar o que o tensorflow faz! 5.0.4 A binary cross-entropy no Tensorflow Eu escondi de vocês, mas o tensorflow já tem a função de perda implementada: tf$nn$sigmoid_cross_entropy_with_logits. Ela já assume que a função de ligação é logística, por isso o sigmoid_ no início. Traduzindo livremente o help da função, temos o seguinte (z=\\(y\\) e x=\\(\\eta = \\alpha + \\beta x\\)) z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x)) = (1 - z) * x + log(1 + exp(-x)) = x - x * z + log(1 + exp(-x)) Para \\(\\eta &lt; 0\\) para evitar problemas numéricos com \\(\\exp(-\\eta)\\), reformulamos para x - x * z + log(1 + exp(-x)) = log(exp(x)) - x * z + log(1 + exp(-x)) = - x * z + log(1 + exp(x)) Então, para garantir estabilidade e evitar problemas numéricos, a implementação usa essa formulação equivalente max(x, 0) - x * z + log(1 + exp(-abs(x))) Beleza, vamos tentar! Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=1.276, beta1=2.495, beta2=3.608 Iter: 03, alpha=1.197, beta1=2.396, beta2=3.562 Iter: 04, alpha=1.164, beta1=2.335, beta2=3.489 Iter: 05, alpha=1.14, beta1=2.287, beta2=3.42 Iter: 06, alpha=1.12, beta1=2.245, beta2=3.358 Iter: 07, alpha=1.102, beta1=2.21, beta2=3.303 Iter: 08, alpha=1.086, beta1=2.178, beta2=3.256 Iter: 09, alpha=1.073, beta1=2.152, beta2=3.215 Iter: 10, alpha=1.061, beta1=2.129, beta2=3.18 Funcionou! :) 5.0.5 Wrap-up Tensorflow é uma biblioteca interessante a ser explorada. É possível implementar uma regressão logística do zero em poucos passos. Precisamos tomar cuidado com problemas numéricos! No futuro, brincaremos também com o algoritmo IWLS. Será que ele roda mais rápido que a descida de gradiente? "],
["5-1-minimos-quadrados-com-restricoes-lineares.html", "5.1 Mínimos quadrados com restrições lineares", " 5.1 Mínimos quadrados com restrições lineares A característica mais importante de um modelo estatístico é a sua flexibilidade. Esse termo pode ser entendido de várias formas, mas neste texto vamos considerar que um modelo é flexível se ele explica de forma coerente uma ampla gama de fenômenos reais. Pensando assim, a regressão linear pode ser considerada um modelo flexível, já que muitas relações funcionais cotidianas são do tipo \\(y = \\beta x\\). É justamente por causa dessa flexibilidade que a boa e velha regressão de mínimos quadrados é tão usada, até mesmo aonde não deveria. O seu uso é tão indiscriminado que uma vez, em aula, um professor extraordinariamente admirável me disse que “90% dos problemas do mundo podem ser resolvidos com uma regressão linear”. Sendo bastante honesto, é provável que o meu professor esteja certo, mas este texto não é sobre isso. Este é um post sobre o que fazer quando a regressão linear simples não basta. No que segue, vamos discutir uma pequena (e poderosa) extensão do modelo de regressão linear simples, mas antes de prosseguir para o problema propriamente dito (e sua implementação em R), vamos discutir da teoria que existe por trás dele. 5.1.1 Regressão linear é programação quadrática Embora seja pouco enfatizado nos bacharelados de estatística, uma regressão linear pode ser formulada como um problema de programação quadrática. Entrando nos detalhes, essa afirmação deve-se a dois fatos: Existe uma teoria, que chama-se programação quadrática, que soluciona problemas da forma \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big),\\] onde \\(x \\in \\mathbb{R}^p\\) e \\(Q\\) e \\(c\\) tem dimensões que fazem a conta acima ter sentido. A teoria ocupa-se desenvolvendo algoritmos exatos e aproximados para obter soluções desses problemas, inclusive com generalizações: \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }Ax \\geq 0.\\] Uma regressão linear consiste em resolver \\[\\min_\\beta (Y - \\beta X)&#39;(Y-\\beta X),\\] que, com um pouco de álgebra, é equivalente à \\[ \\min_\\beta (-2Y&#39;X\\beta + \\beta&#39;X&#39;X\\beta).\\] Logo, tomando \\(Q = 2X&#39;X\\) e \\(c = \\frac{1}{2}X&#39;Y\\) tem-se que esse é um problema de programação quadrática, que por sua vez é um problema convexo e que, segundo a teoria, tem uma única solução no ponto \\(\\beta = (X&#39;X)^{-1}X&#39;Y\\). 5.1.2 Uma regressão linear simples mais flexível Talvez o jeito mais simples de flexibilizar uma regressão linear no sentido mencionado no começo desse texto é restringir os seus parâmetros. Em muitos contextos, esse é o único jeito de colocar conhecimentos prévios na modelagem2. Um caso bastante emblemático aparece nas curvas de crédito divulgadas pela ANBIMA3. Lá, ajusta-se um conjunto de curvas que depende de 6 parâmetros e cada curva representa uma classificação de risco (que nem aquela em que o Brasil pode tomar downgrade4). Como os níveis de risco estão ordenados, é natural exigir que também exista uma ordenação entre as curvas. Sem entrar em detalhes, a ideia pode ser expressa assim: \\[\\beta_{AAA} &lt; \\beta_{AA} &lt; \\beta_{A} &lt; \\beta_{BBB} &lt; ...\\] O que é que isso tem a ver com programação quadrática? A resposta é que a inequação acima pode ser escrita como \\(A\\beta \\geq 0\\), de tal forma já existe uma teoria para resolver uma regressão linear simples com restrições desse tipo! Basta que ela seja vista como um problema de programação quadrática. 5.1.3 O pacote quadprog Existe um pacote de R para quase tudo, então, como não poderia deixar de ser, existe um pacote em R para resolver problemas do tipo: \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }Ax \\geq 0.\\] Para ilustrar o seu uso, vamos considerar um exemplo. Vamos simular um conjunto de dados em que \\(\\beta_5 = 0.31, \\beta_4 = 0.43, \\beta_3 = 1.31, \\beta_2 = 2.19, \\beta_1 = 2.29\\) são os valores reais que precisamos estimar, considere que vale \\[Y \\approx \\beta_1X_1 + \\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5\\] e que o erro de regressão tem distribuição normal. Se soubermos antecipadamente que valem as seguintes afirmações \\[ \\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5 &gt; 0 \\text{ e } \\beta_1 &gt; \\beta_2 &gt; \\beta_3 &gt; \\beta_4 &gt; \\beta_5,\\] a minimização de \\((Y-\\beta X)&#39;(Y-\\beta X)\\) pode ser resolvida usando a função solve.QP. Tudo que precisamos fazer é escrever o conjunto de inequações na forma \\(A\\beta \\geq 0\\). Mas isso é bem fácil! Basta notar que as restrições são equivalentes à \\[ \\left(\\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\\\ \\end{array}\\right) \\times \\left(\\begin{array}{c}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\end{array}\\right) \\geq 0.\\] Dessa forma, o problema está prontinho pra passar no moedor de carne, com uma última ressalva. O problema resolvido no solve.QP é \\[\\min_x \\Big(\\frac{1}{2}x&#39; Q x + c&#39; x\\Big), \\text{ sujeito a }A&#39;x \\geq 0,\\] então vamos ter que tomar o cuidado de passar as nossas restrições através do transposto da matriz que obtivemos acima. Isso resultará na matriz \\(A\\). Para checar como valeu a pena todo esse esforço, dá uma olhada na diferença entre as estimativas! Os pontinhos vermelhos são as estimativas do modelo irrestrito, enquanto as barras são as estimativas do modelo com restrições. 5.1.4 Conclusões Regressão linear simples é um problema de programação quadrática. Algumas restrições interessantes podem ser escritas na forma \\(B\\beta \\geq 0\\). Programação quadrática resolve regressão linear simples com restrições lineares. Se em algum dia você topar com um bicho desses, o quadprog pode resolver o problema pra você. A menos que você seja uma pessoa razoável bayesiano.↩ http://www.anbima.com.br/data/files/05/43/3E/84/E12D7510E7FCF875262C16A8/metodologia-curvas_20credito_20131104_v2_1_.pdf↩ http://economia.estadao.com.br/noticias/geral,agravamento-da-crise-politica-eleva-risco-de-rebaixamento-do-brasil-diz-sep,70001824274↩ "],
["5-2-filtros-de-bloom-em-r.html", "5.2 Filtros de Bloom em R", " 5.2 Filtros de Bloom em R Filtro de Bloom é um algoritmo muito interessante para testar se um elemento pertence a um conjunto. Ele é considerado uma estrutura de dados probabilística, ou seja, o resultado pode não estar correto com alguma probabilidade. Especificamente para o filtro de bloom, existe a possibilidade de falsos positivos mas não de falsos negativos: o algoritmo pode dizer que o elemento pertence ao conjunto, mas na verdade não pertencer, mas nunca dirá que ele não pertence sendo que ele pertence. Bloom Filters são úteis em diversas situações, geralmente relacionadas ao ganho de velocidade e de espaço que o seu uso pode trazer. Muitos sistemas de bancos de dados usam bloom filters para reduzir o número de buscas no disco (ex. Cassandra). O Medium usa para evitar recomendar uma paǵina que você já leu. Recentemente, encontraram até aplicações para bloom filters em machine learning. Nesse post vamos implementar uma versão simplificada, nada otimizada dos filtros de Bloom em R. Mas antes disso, vale a pena ler o verbete da Wikipedia sobre o assunto. Essencialmente, um filtro de bloom é um vetor de TRUEs e FALSES de tamanho \\(m\\). Inicializamos esse vetor com FALSES. Em seguida para cada elemento do conjunto que você deseja representar pelo filtro, repetimos o seguinte processo: Hasheamos o elemento usando \\(k\\) funções de hash diferentes. Cada uma dessas funções indicará um elemento do vetor que deve ser marcado como TRUE. Armazenamos então esse vetor de bits. São os valores de \\(m\\) e de \\(k\\) que controlam a probabilidade de falsos positivos. Veja como podemos criar uma função em R para fazer essas operações. Essa função inicializa o vetor de bits de tamanho \\(m\\) com FALSES e em seguida, para cada uma das \\(k\\) funções de hash (no caso apenas variamos a semente do hash MurMur32) e para cada elemento de x calculamos o elemento do vetor vec que deve se tornar TRUE. No final, ela retorna o vetor vec, onde armazenamos como atributos os parâmetros usados na sua construção. library(digest) library(magrittr) criar_vetor_de_bits &lt;- function(x, m = 1000, k = 7){ vec &lt;- rep(FALSE, m) for (i in 1:k) { for (j in 1:length(x)) { hash &lt;- digest(x[j], algo = &quot;murmur32&quot;, serialize = FALSE, seed = i) %&gt;% Rmpfr::mpfr(base = 16) %% m %&gt;% as.integer() vec[hash + 1] &lt;- TRUE } } # armazenamos os parâmetros usados na construção attributes(vec) &lt;- list(m = m, k= k) return(vec) } Dado um conjunto de strings, podemos criar o vetor de bits que o representa. vect &lt;- criar_vetor_de_bits(c(&quot;eu&quot;, &quot;pertenco&quot;, &quot;ao&quot;, &quot;conjunto&quot;, &quot;de&quot;, &quot;strings&quot;), m = 1000, k = 7) Agora vamos definir uma função que verifica se uma string pertence ao conjunto, dada apenas a representação dos bits desse conjunto. Hasheamos o elemento que desejamos verificar a presença no conjunto com a primeira função de hash. Se ela indicar um elemento do vetor que já está marcado com TRUE então continuamos, se não, retorna FALSE indicando que o elemento não pertence ao conjunto. Continuamos até acabarem as funções de hash ou até 1 FALSE ter sido retornado. verificar_presenca &lt;- function(x, vetor_de_bits){ k &lt;- attr(vetor_de_bits, &quot;k&quot;) m &lt;- attr(vetor_de_bits, &quot;m&quot;) for(i in 1:k){ hash &lt;- digest(x, algo = &quot;murmur32&quot;, serialize = FALSE, seed = i) %&gt;% Rmpfr::mpfr(base = 16) %% m %&gt;% as.integer() if(!vetor_de_bits[hash + 1]) { return(FALSE) } } return(TRUE) } verificar_presenca(&quot;nao&quot;, vect) verificar_presenca(&quot;eu&quot;, vect) verificar_presenca(&quot;abc&quot;, vect) Com m = 1000 e k = 7 não consegui encontrar nenhum falso positivo, mas basta diminuir o tamanho de m e de k que encontraremos. No verbete da Wikipedia a conta está bonitinha mas de fato a probabilidade de falsos positivos pode ser estimada em função dos parâmetros \\(k\\) e \\(m\\) e \\(n\\) (tamanho do conjunto representado) é dada por \\[(1 - e^{-kn/m})^k\\] No caso apresentado, a probabilidade de colisão é de 1.991256e-10. "],
["5-3-modelando-a-variancia-da-normal.html", "5.3 Modelando a variância da normal", " 5.3 Modelando a variância da normal Verificar as suposições dos modelos é muito importante quando fazemos inferência estatística. Em particular, a suposição de homocedasticidade5 dos modelos de regressão linear é especialmente importante, pois modifica o cálculo de erros padrão, intervalos de confiança e valores-p. Neste post, vou mostrar três pacotes do R que ajustam modelos da forma \\[ Y_i = \\beta_0 + \\sum_{k=1}^p\\beta_kx_{ik} + \\epsilon_i, \\ i = 1,\\ldots,n\\] \\[ \\epsilon_{i} \\sim \\textrm{N}(0,\\sigma_i), \\ i = 1,\\ldots,n \\ \\textrm{independentes, com }\\sigma_i^2 = \\alpha x_i^2. \\] Além de mostrar como se faz, também vou ilustrar o desempenho dos pacotes em um exemplo simulado. O modelo que gerará os dados do exemplo terá a seguinte forma funcional \\[ Y_i = \\beta x_i + \\epsilon_i, \\ i = 1,...n \\] \\[ \\epsilon_i \\sim N(0, \\sigma_i)\\text{ independentes, com }\\sigma_i = \\alpha\\sqrt{|x_i|},\\] e os parâmetros do modelo serão os valores \\(\\beta = 1\\) e \\(\\alpha = 4\\). A heterocedasticidade faz com que os pontos desenhem um cone ao redor da reta de regressão. 5.3.1 Usando o pacote gamlss Quando se ajusta um GAMLSS, você pode modelar os parâmetros de locação, escala e curtose ao mesmo tempo em que escolhe a distribuição dos dados dentre uma grande gama de opções. Escolhendo a distribuição normal e modelando apenas os parâmetros de locação e escala, o GAMLSS ajusta modelos lineares normais com heterocedasticidade. No código abaixo, o parâmetro formula = Y ~ X-1 indica que a função de regressão será constituída por um preditor linear em X sem intercepto. Já o parâmetro sigma.formula = ~X2-1 indica que o desvio padrão será modelado por um preditor linear em X2 (ou raiz de X), também sem intercepto. FALSE GAMLSS-RS iteration 1: Global Deviance = 17872.29 FALSE GAMLSS-RS iteration 2: Global Deviance = 17870.67 FALSE GAMLSS-RS iteration 3: Global Deviance = 17870.67 Conforme descrito no sumário abaixo, a estimativa de alfa está muito abaixo do valor simulado. FALSE ****************************************************************** FALSE Family: c(&quot;NO&quot;, &quot;Normal&quot;) FALSE FALSE Call: gamlss::gamlss(formula = Y ~ X - 1, sigma.formula = ~X2 - FALSE 1, family = NO(), data = dataset) FALSE FALSE Fitting method: RS() FALSE FALSE ------------------------------------------------------------------ FALSE Mu link function: identity FALSE Mu Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X 0.996942 0.005131 194.3 &lt;2e-16 *** FALSE --- FALSE Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 FALSE FALSE ------------------------------------------------------------------ FALSE Sigma link function: log FALSE Sigma Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X2 0.1791449 0.0009606 186.5 &lt;2e-16 *** FALSE --- FALSE Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 FALSE FALSE ------------------------------------------------------------------ FALSE No. of observations in the fit: 1000 FALSE Degrees of Freedom for the fit: 2 FALSE Residual Deg. of Freedom: 998 FALSE at cycle: 3 FALSE FALSE Global Deviance: 17870.67 FALSE AIC: 17874.67 FALSE SBC: 17884.49 FALSE ****************************************************************** 5.3.2 Usando o pacote dglm Quando se ajusta um Modelo Linear Generalizado Duplo (MLGD em português e DGLM em inglês), você tem uma flexibilidade parecida com a de um GAMLSS. Entretanto, você não pode definir um modelo para a curtose e a classe de distribuições disponível é bem menor. O código abaixo, similar ao utilizado para ajustar o GAMLSS, ajusta um DGLM aos dados simulados. Novamente, verifica-se que o alfa estimado está muito distante do verdadeiro alfa. FALSE FALSE Call: dglm(formula = Y ~ X - 1, dformula = ~X2 - 1, family = gaussian, FALSE data = dataset, method = &quot;reml&quot;) FALSE FALSE Mean Coefficients: FALSE Estimate Std. Error t value Pr(&gt;|t|) FALSE X 0.9969432 0.008981392 111.001 0 FALSE (Dispersion Parameters for gaussian family estimated as below ) FALSE FALSE Scaled Null Deviance: 27197.48 on 1000 degrees of freedom FALSE Scaled Residual Deviance: 3090.08 on 999 degrees of freedom FALSE FALSE Dispersion Coefficients: FALSE Estimate Std. Error z value Pr(&gt;|z|) FALSE X2 0.3577322 0.001166004 306.8019 0 FALSE (Dispersion parameter for Gamma family taken to be 2 ) FALSE FALSE Scaled Null Deviance: 1628.301 on 1000 degrees of freedom FALSE Scaled Residual Deviance: 6526.59 on 999 degrees of freedom FALSE FALSE Minus Twice the Log-Likelihood: 17870.76 FALSE Number of Alternating Iterations: 18 5.3.3 Usando o pacote rstan Stan é uma linguagem de programação voltada para descrever e manipular objetos probabilísticos, como por exemplo variáveis aleatórias, processos estocásticos, distribuições de probabilidades etc. Essa linguagem foi projetada para tornar intuitivo e simples o ajuste de modelos estatísticos. Em particular, a forma de descrever modelos bayesianos é bem cômoda. O stan possui várias interfaces para R. A mais básica é o rstan, que será utilizada aqui. A principal função desse pacote é a função rstan, que possui dois parâmetros básicos: um parâmetro model_code =, que recebe um código que descreve o modelo na linguagem stan. um parâmetro data =, que recebe uma lista contendo os inputs do modelo, tais como dados coletados, parâmetros de distribuições a priori, etc. Embora esse seja o mínimo que a função precisa, também podemos passar outras componentes. O parâmetro verbose = FALSE faz com que a função não imprima nada enquanto roda e o parâmetro control = list(...) passa uma lista de opções de controle para o algoritmo de ajuste. O retorno da função stan() é um objeto do tipo stanfit, que pode ser sumarizado da mesma forma que outros modelos em R, utilizando a função summary() e a função plot(). O código abaixo ilustra a aplicação da função stan() ao nosso exemplo. A figura abaixo descreve os intervalos de credibilidade obtidos para cada parâmetro do modelo. O ponto central de cada intervalo representa as estimativas pontuais dos parâmetros. Como se nota, as estimativas do modelo utilizando stan estão bem próximas dos valores verdadeiros. Uma regressão linear é homocedástica quando a variabilidade dos erros não depende das covariáveis do modelo.↩ "],
["6-transformacao.html", "Capítulo 6 Transformação ", " Capítulo 6 Transformação "],
["6-1-arrumando-banco-de-dados-o-pacote-janitor.html", "6.1 Arrumando banco de dados: o pacote janitor", " 6.1 Arrumando banco de dados: o pacote janitor No primeiro post sobre arrumação de base de dados, a gente viu como usar as funções do stringr para arrumar o nome das variáveis. Seguindo a dica do Julio, o quebrador de captchas, vamos falar do pacote janitor, que traz algumas funções para dar aquele trato nas BDs. Antes de mais nada, instale e carregue o pacote: 6.1.1 Arrumando o nome das variáveis Assim como no post passado, utilizaremos a base com informações de pacientes com arritmia cardíaca, cujas variáveis selecionadas foram: FALSE [1] &quot;ID&quot; &quot;Sexo&quot; &quot;Nascimento&quot; FALSE [4] &quot;Idade&quot; &quot;Inclusão&quot; &quot;Cor&quot; FALSE [7] &quot;Peso&quot; &quot;Altura&quot; &quot;cintura&quot; FALSE [10] &quot;IMC&quot; &quot;Superfície corporal&quot; &quot;Tabagismo&quot; FALSE [13] &quot;cg.tabag (cig/dia)&quot; &quot;Alcool (dose/semana)&quot; &quot;Drogas ilícitas&quot; FALSE [16] &quot;Cafeína/dia&quot; &quot;Refrig/dia&quot; &quot;Sedentario&quot; FALSE [19] &quot;ativ. Fisica&quot; Os nomes têm letras maiúsculas, acentos, parênteses, pontos e barras, o que atrapalha na hora da programação. Para resolver esse problema, usamos a função clean_names(). FALSE [1] &quot;id&quot; &quot;sexo&quot; &quot;nascimento&quot; FALSE [4] &quot;idade&quot; &quot;inclusao&quot; &quot;cor&quot; FALSE [7] &quot;peso&quot; &quot;altura&quot; &quot;cintura&quot; FALSE [10] &quot;imc&quot; &quot;superficie_corporal&quot; &quot;tabagismo&quot; FALSE [13] &quot;cg_tabag_cig_dia&quot; &quot;alcool_dose_semana&quot; &quot;drogas_ilicitas&quot; FALSE [16] &quot;cafeina_dia&quot; &quot;refrig_dia&quot; &quot;sedentario&quot; FALSE [19] &quot;ativ_fisica&quot; Veja que a função removeu os parênteses, pontos e barras e substituiu os espaços por _. No entanto, ela não remove os acentos. Assim, podemos adicionar mais uma linha ao pipeline para chegar onde queremos. FALSE [1] &quot;id&quot; &quot;sexo&quot; &quot;nascimento&quot; FALSE [4] &quot;idade&quot; &quot;inclusao&quot; &quot;cor&quot; FALSE [7] &quot;peso&quot; &quot;altura&quot; &quot;cintura&quot; FALSE [10] &quot;imc&quot; &quot;superficie_corporal&quot; &quot;tabagismo&quot; FALSE [13] &quot;cg_tabag_cig_dia&quot; &quot;alcool_dose_semana&quot; &quot;drogas_ilicitas&quot; FALSE [16] &quot;cafeina_dia&quot; &quot;refrig_dia&quot; &quot;sedentario&quot; FALSE [19] &quot;ativ_fisica&quot; E para substituir na base. 6.1.2 Removendo linhas e colunas vazias Esse banco de dados também tinha outro problema: linhas vazias. Na verdade, elas não eram completamente vazias, pois havia algumas informações de identificação do paciente, mas nenhuma outra variável tinha sido computada. FALSE # A tibble: 1 x 19 FALSE id sexo nascimento idade inclusao cor peso FALSE &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; FALSE 1 3 &lt;NA&gt; NA NA NA &lt;NA&gt; NA FALSE # … with 12 more variables: altura &lt;dbl&gt;, cintura &lt;chr&gt;, imc &lt;dbl&gt;, FALSE # superficie_corporal &lt;chr&gt;, tabagismo &lt;chr&gt;, cg_tabag_cig_dia &lt;dbl&gt;, FALSE # alcool_dose_semana &lt;dbl&gt;, drogas_ilicitas &lt;chr&gt;, cafeina_dia &lt;dbl&gt;, FALSE # refrig_dia &lt;dbl&gt;, sedentario &lt;chr&gt;, ativ_fisica &lt;chr&gt; Essa foi a solução que eu pensei para resolver o problema utilizando a função remove_empty_row(). FALSE # A tibble: 4 x 19 FALSE id sexo nascimento idade inclusao cor peso FALSE &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; FALSE 1 1 F 1964-01-31 00:00:00 41 2006-02-17 00:00:00 bran… 75 FALSE 2 2 M 1959-01-28 00:00:00 45 2005-11-29 00:00:00 negra 71 FALSE 3 4 M 1957-09-13 00:00:00 50 2008-02-13 00:00:00 NT 80 FALSE 4 5 F 1938-02-06 00:00:00 71 2009-06-25 00:00:00 parda 56 FALSE # … with 12 more variables: altura &lt;dbl&gt;, cintura &lt;chr&gt;, imc &lt;dbl&gt;, FALSE # superficie_corporal &lt;chr&gt;, tabagismo &lt;chr&gt;, cg_tabag_cig_dia &lt;dbl&gt;, FALSE # alcool_dose_semana &lt;dbl&gt;, drogas_ilicitas &lt;chr&gt;, cafeina_dia &lt;dbl&gt;, FALSE # refrig_dia &lt;dbl&gt;, sedentario &lt;chr&gt;, ativ_fisica &lt;chr&gt; Eu precisei converter para data.frame primeiro porque não é possível definir os nomes das linhas de uma tibble. Se a linha estivesse completamente vazia, bastaria usar diretamente a função remove_empty_rows(). Equivalentemente para colunas, existe a função remove_empty_cols(). 6.1.3 Identificando linhas duplicadas O pacote janitor possui uma função para identificar entradas duplicadas numa base de dados: get_dupes(). Vamos criar uma base genérica para testá-la. FALSE # A tibble: 16 x 4 FALSE nome sobrenome dupe_count variavel_importante FALSE &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; FALSE 1 Athos Damiani 2 0.305 FALSE 2 Athos Damiani 2 -0.00796 FALSE 3 Athos Falbel 2 0.198 FALSE 4 Athos Falbel 2 -0.0443 FALSE 5 Athos Trecenti 2 -1.24 FALSE 6 Athos Trecenti 2 -0.729 FALSE 7 Daniel Damiani 2 0.335 FALSE 8 Daniel Damiani 2 3.07 FALSE 9 Fernando Trecenti 2 -0.190 FALSE 10 Fernando Trecenti 2 -0.868 FALSE 11 Julio Trecenti 4 -0.188 FALSE 12 Julio Trecenti 4 -1.39 FALSE 13 Julio Trecenti 4 0.528 FALSE 14 Julio Trecenti 4 1.06 FALSE 15 William Trecenti 2 -0.786 FALSE 16 William Trecenti 2 -0.505 Todas as linhas na tibble resultante representam uma combinação de nome-sobrenome repetida. 6.1.4 Outras funções Por fim, o janitor também tem funções equivalentes à table() para produzir tabelas de frequência: tabyl() - similar a table(), mas pipe-ável e com mais recursos. crosstab() - para tabelas de contingência. adorn_totals() - acrescenta o total das linhas ou colunas. adorn_crosstab() - deixa tabelas de contingência mais bonitas. FALSE cyl n percent FALSE 4 11 0.34375 FALSE 6 7 0.21875 FALSE 8 14 0.43750 FALSE cyl n percent FALSE 4 11 0.34375 FALSE 6 7 0.21875 FALSE 8 14 0.43750 FALSE Total 32 1.00000 FALSE cyl 0 1 FALSE 4 3 8 FALSE 6 4 3 FALSE 8 12 2 FALSE cyl 0 1 FALSE 1 4 27.3% (3) 72.7% (8) FALSE 2 6 57.1% (4) 42.9% (3) FALSE 3 8 85.7% (12) 14.3% (2) É isso! Espero que essas dicas e o pacote janitor ajudem a agilizar as suas análises :) "],
["6-2-skimr-estatisticas-basicas-com.html", "6.2 Skimr: estatísticas básicas com ❤️", " 6.2 Skimr: estatísticas básicas com ❤️ Entre os dias 25 e 27 de maio aconteceu a ROpenSci Unconf 2017. O encontro reuniu vários pop stars da comunidade R como Hadley Wickham, Joe Cheng (criador do shiny), Jeroen Ooms (criador do OpenCPU e autor de vários pacotes bacanas), Jenny Bryan (autora de vários pacotes bacanas como googlesheets), várias pessoas do #R-Ladies e muito mais. Uma coisa muito legal dessa conferência é que ela funcionou como uma hackathon. Foi criada uma nova organização no github chamada ROpenSci Labs, e os presentes simplesmente começaram a subir pacotes fantásticos lá dentro. Recomendo muito dar uma olhada. Dentre os pacotes que olhei, o que mais me chamou atenção foi o skimr e por isso estou fazendo esse post! O propósito do skimr é simples: fazer algumas estatísticas básicas univariadas de uma base de dados. O skimr ainda não está no CRAN, então para instalar recomendamos utilizar o devtools para instalar direto do GitHub, conforme código abaixo. Note que também será necessário instalar o pacote colformat do Hadley. A função skim() calcula estatísticas básicas das variáveis e imprime no seu console. Note que a função separa estatísticas para variáveis numéricas ou fatores. FALSE Skim summary statistics FALSE n obs: 150 FALSE n variables: 5 FALSE FALSE ── Variable type:factor ─────────────────────────────────────────────────────────── FALSE variable missing complete n n_unique top_counts FALSE Species 0 150 150 3 set: 50, ver: 50, vir: 50, NA: 0 FALSE ordered FALSE FALSE FALSE FALSE ── Variable type:numeric ────────────────────────────────────────────────────────── FALSE variable missing complete n mean sd p0 p25 p50 p75 p100 FALSE Petal.Length 0 150 150 3.76 1.77 1 1.6 4.35 5.1 6.9 FALSE Petal.Width 0 150 150 1.2 0.76 0.1 0.3 1.3 1.8 2.5 FALSE Sepal.Length 0 150 150 5.84 0.83 4.3 5.1 5.8 6.4 7.9 FALSE Sepal.Width 0 150 150 3.06 0.44 2 2.8 3 3.3 4.4 FALSE hist FALSE ▇▁▁▂▅▅▃▁ FALSE ▇▁▁▅▃▃▂▂ FALSE ▂▇▅▇▆▅▂▂ FALSE ▁▂▅▇▃▂▁▁ E tem mais! O mais legal do skimr é que ele usa a função colformat::spark_bar() para desenhar histogramas direto no seu console! Tabela 5.1: HISTOGRAMA NA TABELA PORQUE SIM! variable type stat level value formatted Sepal.Length numeric hist .all NA ▂▇▅▇▆▅▂▂ Sepal.Width numeric hist .all NA ▁▂▅▇▃▂▁▁ Petal.Length numeric hist .all NA ▇▁▁▂▅▅▃▁ Petal.Width numeric hist .all NA ▇▁▁▅▃▃▂▂ O skimr também possui padrões de estatísticas básicas para cada tipo de variável. Você pode checar esses tipos com show_skimmers(): Tabela 5.2: Estatísticas básicas para cada tipo de variável. tipo stats AsIs missing, complete, n, n_unique, min_length, max_length character missing, complete, n, min, max, empty, n_unique complex missing, complete, n date missing, complete, n, min, max, median, n_unique Date missing, complete, n, min, max, median, n_unique difftime missing, complete, n, min, max, median, n_unique factor missing, complete, n, n_unique, top_counts, ordered integer missing, complete, n, mean, sd, p0, p25, p50, p75, p100, hist list missing, complete, n, n_unique, min_length, median_length, max_length logical missing, complete, n, mean, count numeric missing, complete, n, mean, sd, p0, p25, p50, p75, p100, hist POSIXct missing, complete, n, min, max, median, n_unique ts missing, complete, n, start, end, frequency, deltat, mean, sd, min, max, median, line_graph 6.2.1 Criando suas próprias funções Você também pode usar funções próprias com o skimr. Por exemplo, digamos que você queira calcular o coeficiente de variação. Primeiro, adicione sua função dentro de uma lista: e depois aplique a função skim_with(): E pronto! Agora você pode rodar skim() novamente: Tabela 5.3: Histograma e coeficiente de variação. variable type stat level value formatted Sepal.Length numeric hist .all NA ▂▇▅▇▆▅▂▂ Sepal.Length numeric cv .all 0.1417113 0.14 Sepal.Width numeric hist .all NA ▁▂▅▇▃▂▁▁ Sepal.Width numeric cv .all 0.1425642 0.14 Petal.Length numeric hist .all NA ▇▁▁▂▅▅▃▁ Petal.Length numeric cv .all 0.4697441 0.47 Petal.Width numeric hist .all NA ▇▁▁▅▃▃▂▂ Petal.Width numeric cv .all 0.6355511 0.64 Para retornar ao skim() padrão, rode skim_with_defaults(). 6.2.2 Wrap up Instale usando devtools::install_github() Rode a função skim(). Use dplyr::filter() para filtrar as estatísticas de interesse. Você pode adicionar suas próprias estatísticas com skim_with(). Acompanhe a evolução do skimr nesta página. O pacote ainda vai evoluir muito e não duvido nada que seja um bom candidado a entrar no tidyverse. O que vocês acham? Escrevam nos comentários! É isso. Happy coding ;) "],
["7-reflexoes.html", "Capítulo 7 Reflexões ", " Capítulo 7 Reflexões "],
["7-1-manifesto-tidy.html", "7.1 Manifesto tidy", " 7.1 Manifesto tidy Autor: Daniel Dificuldade baixa program O manifesto das ferramentas tidy do Hadley Wickham é um dos documentos mais importantes sobre R dos últimos tempos. Esse documento formaliza uma série de princípios que norteiam o desenvolvimento dotidyverse. O tidyverse é um conjunto de pacotes que, por compartilharem esses princípios do manifesto tidy, podem ser utilizados naturalmente em conjunto. Pode-se dizer que existe o R antes do tidyverse e o R depois do tidyverse. A linguagem mudou muito, a comunidade abraçou fortemente o uso desses princípios e tem muita gente criando pacotes para conversar uns com os outros dessa forma. No entanto, usar a filosofia tidy não é a única forma de fazer pacotes do R, existem muitos pacotes excelentes que não utilizam essa filosofia. Como o próprio texto diz “O contrário de tidyverse não é o messyverse, e sim muitos outros universos de pacotes interconectados.”. Os princípios fundamentais do tidyverse são: Reutilizar estruturas de dados existentes. Organizar funções simples usando o pipe. Aderir à programação funcional. Projetado para ser usado por seres humanos. No texto do manifesto tidy cada um dos lemas é descrito de forma detalhada. Aqui, selecionei os aspectos que achei mais importante de cada um deles. 7.1.1 Reutilizar estruturas de dados existentes Quando possível, é melhor utilizar estruturas de dados comuns do que criar uma estrutura específica para o seu pacote. Geralmente, é melhor reutilizar uma estrutura existente mesmo que ela não se encaixe perfeitamente. 7.1.2 Organizar funções simples usando o pipe Faça com que suas funções sejam o mais simples possíveis. Uma função deve poder ser descrita com apenas uma sentença. A sua função deve fazer uma transformação no estilo copy-on-modify ou ter um efeito colateral. Nunca os dois. O nome das funções devem ser verbos. Exceto quando as funções do pacote usam sempre o mesmo verbo. Ex: adicionar ou modificar. 7.1.3 Aderir à programação funcional O R é uma linguagem de programação funcional, não lute contra isso. 7.1.4 Projetado para ser usado por seres humanos Desenvolva o seu pacote para ser usado por humanos. Foque em ter uma API clara para que você escreva o código de maneira intuitiva e rápida. Eficiência dos algoritmos é uma preocupação secundária, pois gastamos mais tempo escrevendo o código do que executando. Esses princípios são bem gerais, mas ajudam bastante a tomar decisões quando estamos escrevendo o nosso código. Para finalizar, clique aqui e veja uma busca no Github por “tidy” em repositórios de R. São mais de 3000 resultados, quase todos seguindo essa filosofia e estendendo o universo arrumado. "],
["7-2-eu-a-estatistica-e-a-programacao.html", "7.2 Eu, a Estatística e a programação", " 7.2 Eu, a Estatística e a programação Autor: William Dificuldade baixa program — Não sabia que nessa cidade a cada 20 minutos atropelam um homem? — Nossa! E como está o coitado? O episódio “Estatísticas” do Chaves foi o meu primeiro contato com o conceito de Estatística (pelo menos que eu possa me lembrar). Claro que naquela época, com 5 ou 6 anos, eu nunca imaginaria que seria essa a minha profissão. Assim como o Quico e o Chaves, eu não fazia muita ideia do que as “senhoras estatísticas”&quot; eram e continuei sem saber de fato até entrar na graduação, em 2007. Reassistindo o episódio, depois de mais de dez anos estudando a disciplina, me identifiquei bastante com a dificuldade que a Dona Florinda e o Professor Girafales têm para explicar o que são estatísticas, o que antes via apenas como uma escada para as piadas que constroem a cena. Quando saio da minha bolha de colegas de faculdade e trabalho, percebo o quanto conceitos básicos de probabilidade e estatística são desconhecidos pela população, mesmo aqueles presentes no dia a dia. Recentemente, lendo um comentário de um radialista sobre a derrota do São Paulo para o Coritiba, na rodada 18 do Campeonato Brasileiro, uma frase me chamou atenção. “Ao iniciar o jogo dessa quinta à noite, o Tricolor, de acordo com as estatísticas, tinha 1,78% de chances de vencer o Coritiba.” A tese do radialista é que a probabilidade dos quatro grandes times de São Paulo vencerem na mesma rodada do campeonato é de 1,78%, a frequência relativa desse evento na era de pontos corridos do Campeonato Brasileiro. Como o São Paulo foi o último grande a jogar e os outros três já haviam vencido, o pobre tricolor paulista teve suas chances reduzidas pelas estatísticas e acabou perdendo o jogo. Essa interpretação com certeza pode ser refutada por vários motivos, mas o que mais me incomodou foi o desconhecimento de probabilidade condicional, ou simplesmente como novas informações modificam as probabilidades dos eventos. Encucado, eu deixei uma resposta, cuja parte central é essa: Mesmo se considerássemos que a probabilidade dos 4 grandes de SP ganharem numa rodada não dependesse de fatores como a fase dos times, os adversários, o momento do campeonato etc., esse número, 1,78%, seria a probabilidade dos quatro ganharem antes da rodada começar. Dado que já sabemos que os outros três ganharam, e considerando que o resultado desses jogos não influenciam o jogo do SP, a probabilidade do evento em questão ocorrer passa a ser apenas a probabilidade do SP ganhar o jogo dele. Em seguida, recebi alguns comentários de outros torcedores dizendo (jocosamente) que não tinham entendido nada do que escrevi. Comecei então a refletir sobre o assunto, pensando no quanto a minha explicação poderia estar confusa e de que forma poderia ter explicado melhor, no quanto as pessoas não costumam se esforçar para entender temas que elas não dominam e no quanto a falta de uma base matemática adequada atrapalha nessas horas. Eu acredito que a Probabilidade e a Estatística são vítimas da onda do “é legal odiar Matemática”, que muitas pessoas se orgulham de surfar. Crianças saem da escola com um conhecimento superficial dessas disciplinas (quando muito!), achando que é tudo uma questão de jogar dados, calcular médias e fazer gráficos. Comunicadores sofrem para interpretar os números de uma pesquisa e pesquisadores encaram a análise estatística como o grande vilão que os separa da publicação. Felizmente, esse comportamento vem mudando, mesmo que a passos lentos. Profissionais estão buscando cursos de data science e programação, empresas estão promovendo cursos para qualificarem seus funcionários e o mercado para estatísticos continua um céu estrelado, tanto para analistas e programadores quanto para educadores. Eu vejo essa mudança, e as pessoas ao meu redor também a veem. Mas o exemplo que citei acima me faz acreditar que preciso espiar fora da minha bolha. Por isso, vou começar uma pequena série de posts, dando a minha opinião sobre algumas coisas que orbitam a educação estatística e a programação, com o objetivo de gerar reflexão e discussão sobre o assunto. A Estatística vem crescendo como carreira, o estatístico vem se tornando cada vez mais protagonista, e vejo esse momento como o ideal para melhorarmos a educação da nossa disciplina. Dividirei o texto nos seguintes tópicos: Por que amar a Estatística? Preconceitos no aprendizado Estatística e programação Espero que esses posts possam contribuir para mostrarmos para mais gente a importância da Estatística e da Computação e por que amamos tanto trabalhar com essas ciências. 7.2.1 Por que amar a estatística? Escolher uma profissão, para quem tem esse privilégio, é uma das decisões mais importantes das nossas vidas. Aos 17, 18 anos, a imaturidade, o pouco auto-conhecimento e a falta de informação sobre as alternativas podem nos desviar da opção que nos vestiria melhor, um erro que muitas vezes nunca será reparado. Às vezes, eu me pergunto o que levou amigos e conhecido a escolherem suas profissões na hora do vestibular. No meu caso, eu quase segui um caminho da “profissões da moda”. O que me impediu de prestar Administração foi descobrir, na hora da inscrição, que era uma carreira da área de Humanas, não Exatas. Sim, eu era bem perdido. Na época, a segunda fase da FUVEST era diferente para cada área, e eu não tinha perspectiva nenhuma de ir bem se tivesse que fazer uma prova dissertativa de História e Geografia em vez de Matemática e Física, disciplinas que eu dominava muito mais. Por isso, após uma (muito breve) pesquisa na internet, fui convencido a prestar Estatística, e o que me convenceu foi a frase “[…] envolve bastante matemática e o mercado de trabalho é muito bom”. Foi baseado nisso que eu tomei uma das decisões mais importantes da minha vida e era basicamente tudo o que eu sabia sobre a carreira quando comecei a graduação. Prestar vestibular para Estatística foi um tiro no escuro tão certeiro que às vezes me pego pensando em destino e esoterismos desse tipo. Durante a graduação, conheci pessoas que não tiveram a mesma sorte e acabaram desistindo nos primeiros semestres, que são bem pesados na matemática. A primeira parte da informação que eu tinha sobre realmente estava certa, e o curso de Estatística pode assustar quem não estiver na pegada de provar vários teoremas. Mas, neste post, não quero falar sobre as dificuldades da escalada, mas sim sobre a vista ao se chegar ao topo. Conforme fui conhecendo a Estatística, eu descobri que ela é a profissão mais nerd que existe6. Eu sustento essa opinião porque a melhor definição de nerd que já escutei é “pessoa ama aprender” e, graças à Estatística, tenho a oportunidade de estudar muita coisa diferente. Nesses dez anos como estatístico, já fiz análises na área de engenharia, finanças, educação, jornalismo, zoologia, farmácia, fisioterapia, medicina, psicologia, odontologia, educação física… e essas são apenas as que eu lembrei de cabeça. Estatística é parte essencial do método científico e está presente em todas as ciências. Pegar trabalhos novos para um estatístico nerd é extremamente motivante, porque não é apenas uma troca de tempo por dinheiro, é uma ótima chance para aprender coisas novas. A Estatística te estimula a ser curioso e criativo, e isso é o que eu mais amo nela. Outra coisa para se amar é o mercado de trabalho. A segunda parte da informação que eu tinha também estava correta: o mercado de trabalho para o estatístico é excelente! Não só pelo número de oportunidades, mas pela gama de lugares diferentes onde somos necessários. Não vou listar aqui porque é praticamente qualquer área. E sobre salários, como diria um professor do IME, dá para alimentar famílias. Apesar de ter sido um dos poucos dos meus colegas a não mergulhar de cabeça no mercado, já tive duas experiências. A primeira foi como estagiário em um banco, onde aprendi bastante sobre o que eu não queria fazer na vida. Tudo o que eu fazia era rodar modelos pré-estabelecidos para gerar relatórios pré-formatados. Tinha aprendido tanta coisa legal na graduação e não podia usar nada, o que me fazia sentir como um pássaro engaiolado. A segunda foi no Instituto Butantan, onde eu era o único estatístico ao lado de vários biólogos, farmacêuticos e veterinários. Foi uma ótima experiência, na qual conheci muita gente bacana e aprendi muita coisa de biologia, farmâcia e controle de qualidade. Trabalhar com pessoas diferentes de você, com outras formas de pensar, é outra parte legal de ser estatístico. O pessoal do Butantan me ensinou bastante, principalmente sobre como a ciência e a pesquisa funcionam na prática. Além disso, foi lá que nasceu o meu interesse em ensinar Estatística. Bom, essa foi uma parte da história de como eu me apaixonei pela Estatística. Talvez eu não tenha acrescentado nada se você já compartilha desse sentimento, mas espero que esse texto chegue a pessoas que ainda estejam escolhendo sua profissão e jogue luz sobre essa alternativa. Essa é a hora de mudarmos gráficos como esse. Resumindo: Estatística é a profissão para quem gosta de aprender. Um bom estatístico no mercado é uma criança com cartão de crédito numa loja de brinquedos. No próximo post desta série, vou levantar um pouco de polêmica desabafando sobre alguns preconceitos de aprendizagem. Até breve! 7.2.2 Preconceitos no aprendizado Volta e meia eu escuto as famosas frases FALSE [1] &quot;Eu sou de Humanas&quot; &quot;Eu sou de Exatas&quot; &quot;Eu sou de Biológicas&quot; de alguém tentando justificar por que não vai fazer alguma coisa. Muitas vezes, não passa de uma brincadeira na hora de dividir a conta do bar. Muitas outras, me soa como uma desculpa pronta para não encarar problemas complicados. Para mim, todo aprendizado é difícil, não acho que existe conhecimento de graça, então realmente importa se ele é de Humanas, Exatas ou Biológicas? A divisão do conhecimento nessas três grandes áreas tem a sua importância organizacional, mas acaba motivando muita gente a criar limitações que não existem de verdade. Por que alguém de Exatas não conseguiria assimilar as ideias de um texto filosófico? Ou por que alguém de Biológicas não conseguiria aprender Cálculo? Acredito que cada um de nós tem afinidade por uma das áreas e maior facilidade em estudar um tópico ou outro. Normal. Mas fico triste quando vejo pessoas inteligentes se diminuindo ao se declararem incapazes de aprender outras competências que não a delas. Sei que essa incapacidade não existe e enxergo apenas como uma forma sofisticada de dizer “Estou com preguiça”. Uma das belezas da Estatística é nos fazer perder esse preconceito. Por mais que tenhamos nossos gostos, descobrimos que não estamos presos ao domínio de apenas uma área. Nós trabalhamos com pessoas que pensam e aprendem de formas diferentes de nossa e construímos juntos pontes para trocarmos conhecimento. Ser estatístico é não ter medo de estudar, seja lá o que for. Trazendo a reflexão aqui para o nosso mundinho, já ouvi muitas vezes colegas dizendo, principalmente na Graduação, que não usam o R porque ele é difícil ou porque não gostam de programar. A minha opinião sobre a primeira desculpa está nos parágrafos acima. Sobre a segunda, vou discutir no próximo e último post desta série: a relação entre Estatística e programação. Resumindo a ópera: sempre vamos apanhar aprendendo, e vamos apanhar mais ainda quando não gostamos do que estamos estudando, mas cedo ou tarde, com a quantidade certa de esforço, o conhecimento dá as caras. E no bar, na hora de dividir a conta, o problema não é você ser de Humanas. O problema é a sua preguiça. :D 7.2.3 Estatística e programação Não importa a área de atuação, a maior parte do dia do estatístico é atrás do computador. E desse tempo, a maior parte é atrás de um (geralmente único) programa estatístico. Os principais programas hoje em dia permitem a execução das etapas essenciais de uma análise: interação com banco de dados, transformação, criação de visualizações e modelagem. Alguns vão além e auxiliam na comunicação dos resultados. Também é comum a existência de ambientes de programação, mesmo quando o programa é bem estruturado no point and click. Eu considero a programação primordial para um estatístico. Ela nos dá a liberdade para sermos criativos, para não nos limitarmos em técnicas que alguém criou e todo mundo usa. Para mim, um estatístico que não sabe/gosta de programar é igual a um piloto que só dirige carro automático. É por isso que o R é uma ferramenta tão incrível para se trabalhar. Ele pega a sua mão no momento em que você recebe a base de dados, estando ela arrumada ou não, e só solta depois da sua análise estar devidamente divulgada. Para cada problema, o R te fornece todas as peças e te deixa montar do jeito que quiser. E mesmo quando uma peça não existe, você mesmo pode criá-la ou pedir socorro para a comunidade mais que fantástica de erreiros pelo mundo. Claro que aprender a programar é bem custoso. Para quem nunca foi familiar com a computação, vai ser um caminho bem tortuoso no início. Mas como discutimos no último post, não existe aprendizado de graça, e por mais que você não goste de estudar programação, é um investimento com retorno mais do que garantido. 7.2.4 Wrap-up Fazendo um resumão do que falamos até aqui, podemos enumerar os seguintes itens: A Estatística é uma disciplina fantástica, principalmente para quem gosta de aprender, e o mercado está bombando. Aprender Estatística é difícil, assim como todo conhecimento. O que vai limitar a sua capacidade de aprender é o quanto você vai conseguir dominar a sua preguiça de estudar. A Estatística e a programação andam lado a lado. O Estatístico que sabe programar tem muito mais poder para resolver problemas complicados. O R é o ambiente mais legal para trabalhar com Estatística. :D É isso! Espero que possamos continuar discutindo o quanto é legal trabalhar com Estatística e que cada vez mais pessoas se interessem por esse caminho difícil, mas recompensador. Se você ainda vê alguma conotação negativa na palavra nerd, mande as minha lembranças aos anos 90. 😉↩ "],
["7-3-o-fluxo-do-web-scraping.html", "7.3 O Fluxo do Web Scraping", " 7.3 O Fluxo do Web Scraping Autor: Caio Dificuldade média program Web scraping (ou raspagem web) não é nada mais que o ato de coletar dados da internet. Hoje em dia é muito comum termos acesso rápido e fácil a qualquer conjunto de informações pela web, mas raramente esses dados estão estruturados e em uma forma de fácil obtenção pelo usuário. Isso faz com que precisemos aprender a coletar esses dados por conta própria. Neste post vou descrever o fluxo do web scraping, um passo a passo para explicar aos iniciantes como funciona a criação de um raspador. 7.3.1 O fluxo Caso você já tenha visto o fluxo da ciência de dados descrito por Hadley Wickham, o fluxo do web scraping vai ser bastante simples de entender. Todos os itens a seguir vão se basear neste diagrama: Cada verbo indica um fase do processo de raspar dados da internet. A caixa azulada no meio do diagrama denominada reprodução indica um procedimento iterativo que devemos repetir até que a coleta funcione, mas, de resto, o fluxo é um processo linear. Nas próximas seções, vamos explorar um exemplo bem simples para entender como esses passos se dariam no mundo real: extrair os títulos de artigos da Wikipédia. 7.3.1.1 Identificar O primeiro passo do fluxo se chama identificar porque nele identificamos a informação que vamos coletar. Aqui precisamos entender bem qual é a estrutura das páginas que queremos raspar e traçar um plano para extrair tudo que precisamos. No nosso exemplo, precisaríamos entrar em algumas páginas da Wikipédia para entender se os títulos se comportam da mesma forma em todas. Como a Wikipédia é um site organizado, todos os títulos são criados da mesma forma em absolutamente todos os artigos. 7.3.1.2 Navegar Agora precisamos entender de onde vem o dado que queremos extrair. Esse passo pode ser extremamente simples, mas de vez em quando ele se tornara algo bastante complexo. Usando as ferramentas de desenvolvedor do nosso navegador, vamos navegar para encontrar a fonte dos dados. Sem entrar em muitos detalhes, poderíamos analisar o networking do navegador para entender as chamadas HTTP que são feitas, poderíamos estudar os resultados das funções JavaScript invocadas pela página e assim por diante. No nosso caso, como escolhi um exemplo simples, precisamos apenas inspecionar o elemento do título e ver qual é o seu XPath (basicamente o endereço do elemento no HTML da página): //*[@id=&quot;firstHeading&quot;]. 7.3.1.3 Replicar Se tivéssemos que fazer várias requests HTTP para chegar até a informação que queremos, seria aqui em que tentaríamos replicar essas chamadas. Neste passo é importante compreender absolutamente tudo que a página está fazendo para trazer o conteúdo até você, então é necessário analisar o seu networking a fim de entender tais requests e seus respectivos queries. No nosso caso, basta fazer uma chamada GET para obter a página do artigo desejado. Também se faz necessário salvar a página localmente para que possamos dar continuidade ao fluxo. url &lt;- &quot;https://en.wikipedia.org/wiki/R_(programming_language)&quot; httr::GET(url, httr::write_disk(&quot;~/Desktop/wiki.html&quot;)) 7.3.1.4 Parsear O anglicismo parsear vem do verbo to parse, que quer dizer algo como analisar ou estudar, mas que, no contexto do web scraping, significa extrair os dados desejados de um arquivo HTML. Aqui vamos usar a informação obtida no passo 2 para retirar do arquivo que chamei de wiki.html o título do artigo. &quot;~/Desktop/wiki.html&quot; %&gt;% xml2::read_html() %&gt;% rvest::html_node(xpath = &quot;//*[@id=&#39;firstHeading&#39;]&quot;) %&gt;% rvest::html_text() #&gt; [1] &quot;R (programming language)&quot; 7.3.1.5 Validar Se tivermos feito tudo certo até agora, validar os resultados será uma tarefa simples. Precisamos apenas reproduzir o procedimento descrito até agora para algumas outras páginas de modo verificar se estamos de fato extraindo corretamente tudo o que queremos. Caso encontremos algo de errado precisamos voltar ao passo 3, tentar replicar corretamente o comportamento do site e parsear os dados certos nas páginas. 7.3.1.6 Iterar O último passo consiste em colocar o nosso scraper em produção. Aqui, ele já deve estar funcionando corretamente para todos os casos desejados e estar pronto para raspar todos os dados dos quais precisamos. Na maior parte dos casos isso consiste em encapsular o scraper em uma função que recebe uma série de links e aplica o mesmo procedimento em cada um. Se quisermos aumentar a eficiência desse processo, podemos paralelizar ou distribuir o nosso raspador. scraper &lt;- function(url, path) { httr::GET(url, httr::write_disk(path)) path %&gt;% xml2::read_html() %&gt;% rvest::html_node(xpath = &quot;//*[@id=&#39;firstHeading&#39;]&quot;) %&gt;% rvest::html_text() } purrr::map2_chr(links, paths, scraper) 7.3.2 Conclusão Fazer um scraper não é uma tarefa fácil, mas, se toda vez seguirmos um método consistente e robusto, podemos melhorar um pouco o nosso trabalho. O fluxo do web scraping tenta ser este método, englobando em passos simples e razoavelmente bem definidos essa arte que é fazer raspadores web. "],
["7-4-tidy-data-teste-t-pareado-e-modelos-mistos.html", "7.4 Tidy Data, Teste t Pareado e Modelos Mistos", " 7.4 Tidy Data, Teste t Pareado e Modelos Mistos Autor: Daniel Dificuldade alta model O que teste \\(t\\)-pareado, modelos mistos e tidy data podem ter em comum? 7.4.1 Tidy Data Para começar, vamos relemebrar o que é tidy data para depois seguir ao ponto do post. Tidy data é um conceito introduzido pelo Hadley Wickham neste paper. Esse paper é, para mim, o melhor artigo do Hadley. A primeira frase da definição cita Tolstoi e diz: Like families, tidy datasets are all alike but every messy dataset is messy in its own way. – Leo Tolstoi Essa frase resume a vida de qualquer um que trabalha ou já trabalhou com análise de dados. O ponto mais importante do que significa tidy data também está neste primeiro parágrafo: são datasets em que a estrutura dos dados está ligada com o seu significado. A forma padronizada é: Cada variável é uma coluna de uma tabela Cada observação é uma linha de uma tabela Cada tipo de unidade observacional forma uma tabela O exemplo cássico é o seguinte. Primeiro vamos ver um banco de dados desarrumado. Pais Idh2015 Idh2014 Brasil 0.754 0.755 Argentina 0.827 0.836 Chile 0.847 0.832 Esse dataset está desarrumado pois existem duas colunas Idh2015 e Idh2014 que representam a mesma variável: IDH e uma variável implícita ANO, que também aparece nesta duas colunas. A forma tidy de representar este dataset seria: Pais ano idh Brasil 2015 0.754 Argentina 2015 0.827 Chile 2015 0.847 Brasil 2014 0.755 Argentina 2014 0.836 Chile 2014 0.832 7.4.2 O que isso tem a ver com teste \\(t\\)-pareado e modelos mistos? Suponha que queremos inferir se houve alguma mudança na média do IDH de um ano para o outro. Ou seja, testar se a média do IDH de 2015 é diferente da média do IDH de 2014. Vamos considerar um banco de dados simulado: Uma forma de fazer isso é usar o teste \\(t\\)-pareado, ensinado nos cursos introdutórios de estatística. Basicamente o que ele faz é testar se a média da diferença entre o IDH2015 e o IDH 2014 é diferente de zero. Isso é diferente de um teste \\(t\\) usual, pois o teste \\(t\\)-pareado ajusta o seu cálculo da variância para considerar que existem duas fontes de incerteza, eventualmente correlacionadas. No R a forma mais natural de fazer isso é: Note que o nosso banco de dados está desarrumado e mesmo assim foi muito simples fazer esse teste no R. Agora vamos arrumar o banco de dados. Agora para fazer o mesmo teste, poderíamos filtrar o banco de dados duas vezes, por exemplo: Paired t-test data: df$idh[df$ano == 2015] and df$idh[df$ano == 2014] t = 27.355, df = 49, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 0.09000554 0.10427822 sample estimates: mean of the differences 0.09714188 Mas aí estamos voltando para a forma desarrumada para fazer o teste. Outra forma de fazer é considerar essa comparação de médias como um problema de regressão em que a suposição independência das observações não é válida, uma vez que para cada país, os IDHs de 2014 e de 2015 são correlacionados. Vamos ajustar um modelo com efeitos aleatórios para esse problema e comparar os resultados. Linear mixed-effects model fit by REML Data: df AIC BIC logLik -184.7518 -174.4119 96.37588 Random effects: Formula: ~1 | Pais (Intercept) Residual StdDev: 0.3009017 0.01775584 Fixed effects: idh ~ as.factor(ano) Value Std.Error DF t-value p-value (Intercept) 0.4840132 0.04262795 49 11.35436 0 as.factor(ano)2015 0.0971419 0.00355117 49 27.35491 0 Correlation: (Intr) as.factor(ano)2015 -0.042 Standardized Within-Group Residuals: Min Q1 Med Q3 Max -1.88877770 -0.44544521 -0.01239249 0.39934207 1.84475543 Number of Observations: 100 Number of Groups: 50 Estamos interessados em comparar a significância do efeito fixo da variável ano nesse modelo com a do teste \\(t\\)-pareado. Veja que no caso a estatística \\(t\\) do testes é idêntica: 27.35. Vimos que a forma como os dados estão estruturados no seu banco de dados pode influenciar a operação utilizada para realizar a análise. Se ele estivesse na forma desarrumada o mais natural seria aplicar um teste \\(t\\)-pareado, se ele estivesse em formado tidy o natural seria usar um modelo misto. Em seu paper, Hadley argumenta que a maioria dos softwares esperam que o seu banco de dados esteja arrumado no sentido de que cada variável é uma coluna e cada observação é uma linha. "],
["referencias.html", "Referências", " Referências "]
]
